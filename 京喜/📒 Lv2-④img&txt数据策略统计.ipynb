{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1 - 对于图片主体的box数量统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\1047\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\12010\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\12811\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\1349\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\1355\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\13661\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\1476\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\15908\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\1656\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\1657\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\16777\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\34919\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\35404\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\6191\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\6221\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\739\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\753\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\760\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\9435\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选\\9775\\grounding_output\\grounding_results_processed.xlsx\n",
      "所有文件夹处理完成\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_excel(input_file, output_file):\n",
    "    # 读取Excel文件\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # 定义一个函数来移除后缀\n",
    "    def remove_suffix(name):\n",
    "        return re.sub(r'_\\d+$', '', name)\n",
    "\n",
    "    # 应用函数到'Image Name'列\n",
    "    df['Image Name'] = df['Image Name'].apply(remove_suffix)\n",
    "\n",
    "    # 计算每个Image Name的出现次数\n",
    "    name_counts = df['Image Name'].value_counts()\n",
    "\n",
    "    # 创建一个新的'box_no'列，并填充对应的计数\n",
    "    df['box_no'] = df['Image Name'].map(name_counts)\n",
    "\n",
    "    # 保存修改后的DataFrame到新的Excel文件\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"处理完成，结果已保存到 {output_file}\")\n",
    "\n",
    "def process_all_folders(base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'grounding_output' in dirs:\n",
    "            grounding_output_path = os.path.join(root, 'grounding_output')\n",
    "            input_file = os.path.join(grounding_output_path, 'grounding_results.xlsx')\n",
    "            if os.path.exists(input_file):\n",
    "                output_file = os.path.join(grounding_output_path, 'grounding_results_processed.xlsx')\n",
    "                process_excel(input_file, output_file)\n",
    "\n",
    "# 设置基础路径\n",
    "base_path = 'D://code//data//Lv2期结论//京喜_from_0501//筛选'\n",
    "\n",
    "# 处理所有文件夹\n",
    "process_all_folders(base_path)\n",
    "\n",
    "print(\"所有文件夹处理完成\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2 - 文本框识别, 并合并相邻的文本框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 1047: 100%|██████████| 34/34 [00:07<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\12010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 12010: 100%|██████████| 81/81 [00:23<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\12811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 12811: 100%|██████████| 30/30 [00:07<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\1349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 1349: 100%|██████████| 140/140 [00:27<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\1355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 1355: 100%|██████████| 132/132 [00:17<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\13661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 13661: 100%|██████████| 51/51 [00:16<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 1476: 100%|██████████| 16/16 [00:04<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\15908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 15908: 100%|██████████| 100/100 [00:51<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 1656: 100%|██████████| 55/55 [00:19<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\1657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 1657: 100%|██████████| 108/108 [00:43<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\16777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 16777: 100%|██████████| 119/119 [00:43<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\34919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 34919: 100%|██████████| 63/63 [00:33<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\35404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 35404: 100%|██████████| 108/108 [00:45<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\6191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 6191: 100%|██████████| 20/20 [00:08<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\6221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 6221: 100%|██████████| 50/50 [00:23<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 739: 100%|██████████| 30/30 [00:18<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 753: 100%|██████████| 9/9 [00:06<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 760: 100%|██████████| 10/10 [00:07<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\9435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 9435: 100%|██████████| 45/45 [00:30<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//京喜_from_0501//筛选\\9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 9775: 100%|██████████| 98/98 [00:46<00:00,  2.12it/s]\n",
      "Analyzing text: 100%|██████████| 12730/12730 [00:02<00:00, 5381.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成\n",
      "完成时间: 2024-10-18 10:54:04\n"
     ]
    }
   ],
   "source": [
    "# 修改后的代码, 先从图片中识别出文本, 然后分两步\n",
    "# ① 对文本框进行阈值下的合并; 同时也保留原文本框\n",
    "# ② 对文本进行高度和关键词的分类\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from paddleocr import PaddleOCR\n",
    "from PIL import Image\n",
    "import math\n",
    "import re\n",
    "\n",
    "# 设置输入和输出路径\n",
    "input_folder_path = 'D://code//data//Lv2期结论//京喜_from_0501//筛选'\n",
    "output_file_path = 'D://code//data//Lv2期结论//京喜_from_0501//筛选//txt_info.xlsx'\n",
    "\n",
    "# 加载 OCR 模型\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", show_log=False)\n",
    "\n",
    "def calculate_shortest_distance(point_a, points_bcd):\n",
    "    shortest_distance = float('inf')\n",
    "    for point_bcd in points_bcd:\n",
    "        distance = ((point_bcd[0] - point_a[0]) ** 2 + (point_bcd[1] - point_a[1]) ** 2) ** 0.5\n",
    "        if distance < shortest_distance:\n",
    "            shortest_distance = distance\n",
    "    return shortest_distance\n",
    "\n",
    "def merge_text_boxes(img_path, style):\n",
    "    result = ocr.ocr(img_path, cls=True)\n",
    "    img = Image.open(img_path)\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    if not result or not result[0]:\n",
    "        print(f\"No text detected in the image: {img_path}\")\n",
    "        return None, None\n",
    "\n",
    "    rectangles_with_text = result[0]\n",
    "\n",
    "    original_text_box_info = []\n",
    "    for rectangle in rectangles_with_text:\n",
    "        points = rectangle[0]\n",
    "        original_text_box_info.append({\n",
    "            'File Name': os.path.basename(img_path),\n",
    "            'Style': style,\n",
    "            'x1': points[0][0],\n",
    "            'y1': points[0][1],\n",
    "            'x2': points[2][0],\n",
    "            'y2': points[2][1],\n",
    "            'text': rectangle[1][0]\n",
    "        })\n",
    "\n",
    "    merged_text_boxes = []\n",
    "\n",
    "    for index, row in pd.DataFrame(original_text_box_info).iterrows():\n",
    "        if not merged_text_boxes:\n",
    "            merged_text_boxes.append(row.to_dict())\n",
    "        else:\n",
    "            last_merged_box = merged_text_boxes[-1]\n",
    "\n",
    "            if calculate_shortest_distance((row['x1'], row['y1']), [(last_merged_box['x1'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y2']), (last_merged_box['x1'], last_merged_box['y2'])]) < 100:\n",
    "                last_merged_box['text'] += ' ' + row['text']\n",
    "                last_merged_box['x1'] = min(last_merged_box['x1'], row['x1'])\n",
    "                last_merged_box['y1'] = min(last_merged_box['y1'], row['y1'])\n",
    "                last_merged_box['x2'] = max(last_merged_box['x2'], row['x2'])\n",
    "                last_merged_box['y2'] = max(last_merged_box['y2'], row['y2'])\n",
    "            else:\n",
    "                merged_text_boxes.append(row.to_dict())\n",
    "\n",
    "    original_text_box_df = pd.DataFrame(original_text_box_info)\n",
    "    merged_text_box_df = pd.DataFrame(merged_text_boxes)\n",
    "\n",
    "    for i, box in original_text_box_df.iterrows():\n",
    "        if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "            region = '上半'\n",
    "        elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "            region = '下半'\n",
    "        elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "            region = '左半'\n",
    "        else:\n",
    "            region = '右半'\n",
    "        original_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "        box_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "        box_per = box_area / (img_width * img_height)\n",
    "        original_text_box_df.at[i, 'txt_Area'] = box_area\n",
    "        original_text_box_df.at[i, 'txt_Per'] = box_per\n",
    "\n",
    "    for i, box in merged_text_box_df.iterrows():\n",
    "        if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "            region = '上半'\n",
    "        elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "            region = '下半'\n",
    "        elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "            region = '左半'\n",
    "        else:\n",
    "            region = '右半'\n",
    "        merged_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "        merge_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "        merge_per = merge_area / (img_width * img_height)\n",
    "        merged_text_box_df.at[i, 'Area'] = merge_area\n",
    "        merged_text_box_df.at[i, 'Per'] = merge_per\n",
    "\n",
    "    return merged_text_box_df, original_text_box_df\n",
    "\n",
    "keyword_groups = {\n",
    "    '通用': ['以旧换新', '只换不修', '包邮', '无理由退', '先用后付', '京东白条', '期免息', '送货上门', '保修'],\n",
    "    '价保': ['价保', '保价'],\n",
    "    '纯价格': ['¥', '夫', '￥', r'\\b价\\b', '到手价', '活动价'],\n",
    "    '直降': ['立减', '直降', '降', '立省', r'^(?!.*升降).*$', r'^(?!.*降温).*$', r'^(?!.*降噪).*$', r'^(?!.*降低).*$'],\n",
    "    '折扣': ['折', r'^(?!.*折叠).*$', r'^(?!.*翻折).*$'],\n",
    "    '满减': [r'.*满.*减.*', r'.*满.*-.*', r'.*满.*免.*'],\n",
    "    '用券': ['用券', '领券', '券'],\n",
    "    '返券': ['返券', '京豆', '返现', r'.*返.*E卡.*', r'.*返.*红包.*'],\n",
    "    '限时': ['.*小时$', '.*天$', '时间', 'time', 'TIME', '限时', r'.*月.*日.*', r'.*日.*点.*', r'.*:.*', r'.*:.*', r'.*：.*', r'\\b\\d{1,2}\\.\\d{1,2}-\\d{1,2}\\b'],\n",
    "    'xx元任选': [r'.*元.*件.*'],\n",
    "    '赠品': [r'.*满.*赠.*', r'.*满.*送.*', '送', '抽', '奖励', '赠', r'^(?!.*送货).*$', r'^(?!.*送礼).*$', r'^(?!.*送装).*$', r'^(?!.*配送).*$', r'^(?!.*送达).*$'],\n",
    "    '节日名称': ['节', '出游季', '购物季', '毕业季', '开学季', '黑五', '周年庆', '儿童节', '父亲节', '端午节', '七夕', '中秋节', '国庆', '万圣节', '感恩节', '元旦', '圣诞', '情人节', '春节', '元宵节', '38节', '3.8节', '清明节', '母亲节', '618', '购物季', '开学季', '11.11', '黑五', '12.12', '女神节', '出游季', '放价季', '吃货节', '家装节'],\n",
    "    '是否限购': ['限购', '限量']\n",
    "}\n",
    "\n",
    "def keyword_analysis(text):\n",
    "    results = {}\n",
    "    for key, words in keyword_groups.items():\n",
    "        results[key] = any(re.search(word, text) for word in words)\n",
    "    return results\n",
    "\n",
    "def height_analysis(x1, y1, x2, y2):\n",
    "    height = abs(y2 - y1)\n",
    "    return height\n",
    "\n",
    "def process_images(folder_path, subfolder_name):\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        if 'grounding_output' in root and ('price' in root or 'txt' in root):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                    image_files.append(os.path.join(root, file))\n",
    "    \n",
    "    combined_results = []\n",
    "    for img_path in tqdm(image_files, desc=f'Processing images in {subfolder_name}'):\n",
    "        style = 'price' if 'price' in img_path else 'txt'\n",
    "        merged_df, original_df = merge_text_boxes(img_path, style)\n",
    "        if merged_df is not None and original_df is not None:\n",
    "            merged_df['Subfolder'] = subfolder_name\n",
    "            original_df['Subfolder'] = subfolder_name\n",
    "            combined_results.append({\n",
    "                'original': original_df,\n",
    "                'merged': merged_df\n",
    "            })\n",
    "    \n",
    "    return combined_results\n",
    "\n",
    "# 主程序\n",
    "all_results = []\n",
    "for folder in os.listdir(input_folder_path):\n",
    "    if folder.isdigit():\n",
    "        folder_path = os.path.join(input_folder_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"正在处理文件夹: {folder_path}\")\n",
    "            results = process_images(folder_path, folder)\n",
    "            all_results.extend(results)\n",
    "\n",
    "final_combined_data = []\n",
    "for result in all_results:\n",
    "    result['original']['Type'] = 'Original'\n",
    "    result['merged']['Type'] = 'Merged'\n",
    "    combined = pd.concat([result['original'], result['merged']], ignore_index=True)\n",
    "    final_combined_data.append(combined)\n",
    "\n",
    "final_combined_df = pd.concat(final_combined_data, ignore_index=True)\n",
    "final_combined_df.sort_values(by=['Subfolder', 'File Name', 'Type'], inplace=True)\n",
    "\n",
    "for index, row in tqdm(final_combined_df.iterrows(), total=final_combined_df.shape[0], desc=\"Analyzing text\"):\n",
    "    keyword_results = keyword_analysis(row['text'])\n",
    "    for key, value in keyword_results.items():\n",
    "        final_combined_df.at[index, key] = value\n",
    "    \n",
    "    height = height_analysis(row['x1'], row['y1'], row['x2'], row['y2'])\n",
    "    final_combined_df.at[index, 'Height'] = height\n",
    "    final_combined_df.at[index, 'Height_Category'] = (\n",
    "        'Height_<18' if height < 18 else\n",
    "        'Height_18-29' if 18 <= height < 29 else\n",
    "        'Height_29-38' if 29 <= height < 38 else\n",
    "        'Height_>38'\n",
    "    )\n",
    "\n",
    "final_combined_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print('处理完成')\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 自动文件名进行处理的方法, 但是还没有验证\n",
    "\n",
    "\n",
    "# import os\n",
    "# import glob\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# from paddleocr import PaddleOCR\n",
    "# from PIL import Image\n",
    "# import math\n",
    "# import re\n",
    "\n",
    "# # 设置输入和输出路径\n",
    "# input_folder_path = 'D://code//data//Lv2期结论//京喜_from_0501//筛选'\n",
    "# output_file_path = 'D://code//data//Lv2期结论//京喜_from_0501//筛选//txt_info.xlsx'\n",
    "\n",
    "# # 加载 OCR 模型\n",
    "# ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", show_log=False)\n",
    "\n",
    "# def calculate_shortest_distance(point_a, points_bcd):\n",
    "#     shortest_distance = float('inf')\n",
    "#     for point_bcd in points_bcd:\n",
    "#         distance = ((point_bcd[0] - point_a[0]) ** 2 + (point_bcd[1] - point_a[1]) ** 2) ** 0.5\n",
    "#         if distance < shortest_distance:\n",
    "#             shortest_distance = distance\n",
    "#     return shortest_distance\n",
    "\n",
    "# def merge_text_boxes(img_path, style):\n",
    "#     result = ocr.ocr(img_path, cls=True)\n",
    "#     img = Image.open(img_path)\n",
    "#     img_width, img_height = img.size\n",
    "\n",
    "#     if not result or not result[0]:\n",
    "#         print(f\"No text detected in the image: {img_path}\")\n",
    "#         return None, None\n",
    "\n",
    "#     rectangles_with_text = result[0]\n",
    "\n",
    "#     # 提取文件名并处理\n",
    "#     file_name = os.path.basename(img_path)\n",
    "#     # 移除扩展名\n",
    "#     file_name = os.path.splitext(file_name)[0]\n",
    "#     # 移除可能的 'txt_' 或 'price_' 前缀\n",
    "#     file_name = re.sub(r'^(txt_|price_)', '', file_name)\n",
    "#     # 确保文件名格式为 XXXXXXXXXXXXXXXX_XXXXXXXXXXXXXXXX\n",
    "#     if '_' in file_name:\n",
    "#         parts = file_name.split('_')\n",
    "#         if len(parts) >= 2:\n",
    "#             file_name = f\"{parts[-2]}_{parts[-1]}\"\n",
    "\n",
    "#     original_text_box_info = []\n",
    "#     for rectangle in rectangles_with_text:\n",
    "#         points = rectangle[0]\n",
    "#         original_text_box_info.append({\n",
    "#             'File Name': file_name,  # 使用处理后的文件名\n",
    "#             'Style': style,\n",
    "#             'x1': points[0][0],\n",
    "#             'y1': points[0][1],\n",
    "#             'x2': points[2][0],\n",
    "#             'y2': points[2][1],\n",
    "#             'text': rectangle[1][0]\n",
    "#         })\n",
    "\n",
    "#     merged_text_boxes = []\n",
    "\n",
    "#     for index, row in pd.DataFrame(original_text_box_info).iterrows():\n",
    "#         if not merged_text_boxes:\n",
    "#             merged_text_boxes.append(row.to_dict())\n",
    "#         else:\n",
    "#             last_merged_box = merged_text_boxes[-1]\n",
    "\n",
    "#             if calculate_shortest_distance((row['x1'], row['y1']), [(last_merged_box['x1'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y2']), (last_merged_box['x1'], last_merged_box['y2'])]) < 100:\n",
    "#                 last_merged_box['text'] += ' ' + row['text']\n",
    "#                 last_merged_box['x1'] = min(last_merged_box['x1'], row['x1'])\n",
    "#                 last_merged_box['y1'] = min(last_merged_box['y1'], row['y1'])\n",
    "#                 last_merged_box['x2'] = max(last_merged_box['x2'], row['x2'])\n",
    "#                 last_merged_box['y2'] = max(last_merged_box['y2'], row['y2'])\n",
    "#             else:\n",
    "#                 merged_text_boxes.append(row.to_dict())\n",
    "\n",
    "#     original_text_box_df = pd.DataFrame(original_text_box_info)\n",
    "#     merged_text_box_df = pd.DataFrame(merged_text_boxes)\n",
    "\n",
    "#     for i, box in original_text_box_df.iterrows():\n",
    "#         if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "#             region = '上半'\n",
    "#         elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "#             region = '下半'\n",
    "#         elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "#             region = '左半'\n",
    "#         else:\n",
    "#             region = '右半'\n",
    "#         original_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "#         box_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "#         box_per = box_area / (img_width * img_height)\n",
    "#         original_text_box_df.at[i, 'txt_Area'] = box_area\n",
    "#         original_text_box_df.at[i, 'txt_Per'] = box_per\n",
    "\n",
    "#     for i, box in merged_text_box_df.iterrows():\n",
    "#         if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "#             region = '上半'\n",
    "#         elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "#             region = '下半'\n",
    "#         elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "#             region = '左半'\n",
    "#         else:\n",
    "#             region = '右半'\n",
    "#         merged_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "#         merge_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "#         merge_per = merge_area / (img_width * img_height)\n",
    "#         merged_text_box_df.at[i, 'Area'] = merge_area\n",
    "#         merged_text_box_df.at[i, 'Per'] = merge_per\n",
    "\n",
    "#     return merged_text_box_df, original_text_box_df\n",
    "\n",
    "# keyword_groups = {\n",
    "#     '通用': ['以旧换新', '只换不修', '包邮', '无理由退', '先用后付', '京东白条', '期免息', '送货上门', '保修'],\n",
    "#     '价保': ['价保', '保价'],\n",
    "#     '纯价格': ['¥', '夫', '￥', r'\\b价\\b', '到手价', '活动价'],\n",
    "#     '直降': ['立减', '直降', '降', '立省', r'^(?!.*升降).*$', r'^(?!.*降温).*$', r'^(?!.*降噪).*$', r'^(?!.*降低).*$'],\n",
    "#     '折扣': ['折', r'^(?!.*折叠).*$', r'^(?!.*翻折).*$'],\n",
    "#     '满减': [r'.*满.*减.*', r'.*满.*-.*', r'.*满.*免.*'],\n",
    "#     '用券': ['用券', '领券', '券'],\n",
    "#     '返券': ['返券', '京豆', '返现', r'.*返.*E卡.*', r'.*返.*红包.*'],\n",
    "#     '限时': ['.*小时$', '.*天$', '时间', 'time', 'TIME', '限时', r'.*月.*日.*', r'.*日.*点.*', r'.*:.*', r'.*:.*', r'.*：.*', r'\\b\\d{1,2}\\.\\d{1,2}-\\d{1,2}\\b'],\n",
    "#     'xx元任选': [r'.*元.*件.*'],\n",
    "#     '赠品': [r'.*满.*赠.*', r'.*满.*送.*', '送', '抽', '奖励', '赠', r'^(?!.*送货).*$', r'^(?!.*送礼).*$', r'^(?!.*送装).*$', r'^(?!.*配送).*$', r'^(?!.*送达).*$'],\n",
    "#     '节日名称': ['节', '出游季', '购物季', '毕业季', '开学季', '黑五', '周年庆', '儿童节', '父亲节', '端午节', '七夕', '中秋节', '国庆', '万圣节', '感恩节', '元旦', '圣诞', '情人节', '春节', '元宵节', '38节', '3.8节', '清明节', '母亲节', '618', '购物季', '开学季', '11.11', '黑五', '12.12', '女神节', '出游季', '放价季', '吃货节', '家装节'],\n",
    "#     '是否限购': ['限购', '限量']\n",
    "# }\n",
    "\n",
    "# def keyword_analysis(text):\n",
    "#     results = {}\n",
    "#     for key, words in keyword_groups.items():\n",
    "#         results[key] = any(re.search(word, text) for word in words)\n",
    "#     return results\n",
    "\n",
    "# def height_analysis(x1, y1, x2, y2):\n",
    "#     height = abs(y2 - y1)\n",
    "#     return height\n",
    "\n",
    "# def process_images(folder_path, subfolder_name):\n",
    "#     image_files = []\n",
    "#     for root, dirs, files in os.walk(folder_path):\n",
    "#         if 'grounding_output' in root and ('price' in root or 'txt' in root):\n",
    "#             for file in files:\n",
    "#                 if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "#                     image_files.append(os.path.join(root, file))\n",
    "    \n",
    "#     combined_results = []\n",
    "#     for img_path in tqdm(image_files, desc=f'Processing images in {subfolder_name}'):\n",
    "#         style = 'price' if 'price' in img_path else 'txt'\n",
    "#         merged_df, original_df = merge_text_boxes(img_path, style)\n",
    "#         if merged_df is not None and original_df is not None:\n",
    "#             merged_df['Subfolder'] = subfolder_name\n",
    "#             original_df['Subfolder'] = subfolder_name\n",
    "#             combined_results.append({\n",
    "#                 'original': original_df,\n",
    "#                 'merged': merged_df\n",
    "#             })\n",
    "    \n",
    "#     return combined_results\n",
    "\n",
    "# # 主程序\n",
    "# all_results = []\n",
    "# for folder in os.listdir(input_folder_path):\n",
    "#     if folder.isdigit():\n",
    "#         folder_path = os.path.join(input_folder_path, folder)\n",
    "#         if os.path.isdir(folder_path):\n",
    "#             print(f\"正在处理文件夹: {folder_path}\")\n",
    "#             results = process_images(folder_path, folder)\n",
    "#             all_results.extend(results)\n",
    "\n",
    "# final_combined_data = []\n",
    "# for result in all_results:\n",
    "#     result['original']['Type'] = 'Original'\n",
    "#     result['merged']['Type'] = 'Merged'\n",
    "#     combined = pd.concat([result['original'], result['merged']], ignore_index=True)\n",
    "#     final_combined_data.append(combined)\n",
    "\n",
    "# final_combined_df = pd.concat(final_combined_data, ignore_index=True)\n",
    "# final_combined_df.sort_values(by=['Subfolder', 'File Name', 'Type'], inplace=True)\n",
    "\n",
    "# for index, row in tqdm(final_combined_df.iterrows(), total=final_combined_df.shape[0], desc=\"Analyzing text\"):\n",
    "#     keyword_results = keyword_analysis(row['text'])\n",
    "#     for key, value in keyword_results.items():\n",
    "#         final_combined_df.at[index, key] = value\n",
    "    \n",
    "#     height = height_analysis(row['x1'], row['y1'], row['x2'], row['y2'])\n",
    "#     final_combined_df.at[index, 'Height'] = height\n",
    "#     final_combined_df.at[index, 'Height_Category'] = (\n",
    "#         'Height_<18' if height < 18 else\n",
    "#         'Height_18-29' if 18 <= height < 29 else\n",
    "#         'Height_29-38' if 29 <= height < 38 else\n",
    "#         'Height_>38'\n",
    "#     )\n",
    "\n",
    "# final_combined_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "# print('处理完成')\n",
    "\n",
    "# import datetime\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3 - 将分散在各个grounding_output文件夹中的grounding_results_processed.xlsx合并起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并完成，结果保存在: D://code//data//Lv2期结论//京喜_from_0501//筛选\\img_info.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "def merge_excel_files(base_path):\n",
    "    # 用于存储所有数据框的列表\n",
    "    all_dataframes = []\n",
    "\n",
    "    # 遍历基础路径下的所有文件夹\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'grounding_output' in dirs:\n",
    "            grounding_output_path = os.path.join(root, 'grounding_output')\n",
    "            excel_file = os.path.join(grounding_output_path, 'grounding_results_processed.xlsx')\n",
    "            \n",
    "            if os.path.exists(excel_file):\n",
    "                # 读取Excel文件\n",
    "                df = pd.read_excel(excel_file)\n",
    "                \n",
    "                # 添加新列，值为当前子文件夹的名称\n",
    "                subfolder_name = os.path.basename(os.path.dirname(grounding_output_path))\n",
    "                df['Subfolder'] = subfolder_name\n",
    "                \n",
    "                # 将数据框添加到列表中\n",
    "                all_dataframes.append(df)\n",
    "\n",
    "    # 合并所有数据框\n",
    "    if all_dataframes:\n",
    "        merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        \n",
    "        # 保存合并后的数据框到新的Excel文件\n",
    "        output_file = os.path.join(base_path, 'img_info.xlsx')\n",
    "        merged_df.to_excel(output_file, index=False)\n",
    "        print(f\"合并完成，结果保存在: {output_file}\")\n",
    "    else:\n",
    "        print(\"没有找到符合条件的Excel文件\")\n",
    "\n",
    "# 使用示例\n",
    "base_path = \"D://code//data//Lv2期结论//京喜_from_0501//筛选\"\n",
    "merge_excel_files(base_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step-4 将布局和ctr参数进行合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to D://code//data//Lv2期结论//京喜_from_0501//筛选//merged_info_ctr.xlsx\n",
      "2024-10-18 11:20:19\n",
      "2024-10-18 11:20:19\n",
      "2024-10-18 11:20:19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# def merge_excel_files(txt_box_info_file, img_box_info_file, output_file):\n",
    "#     # 读取 txt_box_info 文件\n",
    "#     txt_df = pd.read_excel(txt_box_info_file)\n",
    "#     txt_df = txt_df.rename(columns={'File Name':'Image Name', 'x1': 'txt_x1', 'y1': 'txt_y1', 'x2': 'txt_x2', 'y2': 'txt_y2'})\n",
    "\n",
    "#     # 读取 img_box_info 文件\n",
    "#     img_df = pd.read_excel(img_box_info_file)\n",
    "\n",
    "#     # 重命名列\n",
    "#     img_df = img_df.rename(columns={'x1': 'img_x1', 'y1': 'img_y1', 'x3': 'img_x2', 'y3':'img_y2', 'Subfolder':'Style'})\n",
    "#     img_df = img_df.loc[:, ['Image Name', 'Style','img_x1', 'img_y1', 'img_x2', 'img_y2', 'box_no', ]]\n",
    "    \n",
    "#     # 合并两个 DataFrame，使用 txt_box_info 的表头作为准\n",
    "#     merged_df = pd.concat([txt_df, img_df], ignore_index=True)\n",
    "    \n",
    "#     # 将 img_box_info 中缺少的数据设置为空\n",
    "#     merged_df = merged_df.fillna(\"\")\n",
    "    \n",
    "#     # 将合并后的 DataFrame 写入新的 Excel 文件\n",
    "#     with pd.ExcelWriter(output_file) as writer:\n",
    "#         merged_df.to_excel(writer, index=False)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     txt_box_info_file = f\"D://code//data//Lv2期结论//京喜_from_0501//筛选//txt_info.xlsx\"\n",
    "#     img_box_info_file = f\"D://code//data//Lv2期结论//京喜_from_0501//筛选//img_info.xlsx\"\n",
    "#     output_file = f\"D://code//data//Lv2期结论//京喜_from_0501//筛选//merged_info.xlsx\"\n",
    "    \n",
    "#     merge_excel_files(txt_box_info_file, img_box_info_file, output_file)\n",
    "    \n",
    "#     print(\"Excel files merged successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 定义路径\n",
    "data_1 = f\"D://code//data//Lv2期结论//京喜_from_0501//筛选//merged_info.xlsx\"\n",
    "data_2 = \"D://code//data//Lv2期结论//京喜_from_0501//京喜数据_from_0501_筛选.csv\"\n",
    "# data_3 = 'D://code//data//howtodo_from_0401//服饰鞋靴箱包//品类聚类-服饰鞋靴箱包.csv'\n",
    "output_path = f\"D://code//data//Lv2期结论//京喜_from_0501//筛选//merged_info_ctr.xlsx\"\n",
    "\n",
    "# 读取df1\n",
    "df1 = pd.read_excel(data_1)\n",
    "\n",
    "# 读取df2\n",
    "df2 = pd.read_csv(data_2)\n",
    "\n",
    "aggregated_data = df2.groupby('img_url').agg({\n",
    "    'cid2': 'first',  # 使用 'first' 函数来选择分组中的第一个值\n",
    "    'cid3': 'first',\n",
    "    'uv': 'sum',\n",
    "    'click_uv': 'sum',\n",
    "    'gmv_cj':'sum',\n",
    "    'sale_qtty_cj':'sum'\n",
    "#     'folder_path': 'first'  # 同样使用 'first' 函数选择第一个值\n",
    "}).reset_index()  # 重置索引\n",
    "\n",
    "df2 = aggregated_data\n",
    "\n",
    "# 计算ctr字段\n",
    "df2['ctr'] = df2['click_uv'] / df2['uv']\n",
    "\n",
    "\n",
    "def extract_filename(x):\n",
    "    # 分割路径，取倒数第二部分和最后一部分（文件名部分）\n",
    "    parts = x.split('/')\n",
    "    return f\"{parts[-2]}_{os.path.splitext(parts[-1])[0]}\"  # 保留原文件扩展名\n",
    "    # return f\"{os.path.splitext(parts[-1])[0]}.jpg\"  # 保留原文件扩展名\n",
    "\n",
    "# 应用函数\n",
    "df2['only_2'] = df2['img_url'].apply(extract_filename)\n",
    "\n",
    "# 初始化结果列表\n",
    "results = []\n",
    "\n",
    "# 遍历df1的每一行\n",
    "for index, row1 in df1.iterrows():\n",
    "    # 查找df2中匹配的行\n",
    "    matching_rows_df2 = df2[df2['only_2'] == row1['Image Name']]\n",
    "    \n",
    "    # 如果没有找到匹配的行，则只添加df1的当前行\n",
    "    if matching_rows_df2.empty:\n",
    "        results.append(row1.to_dict())\n",
    "    else:\n",
    "        # 对于找到的每个匹配行，先添加df1的当前行，然后添加匹配的df2行\n",
    "        results.append(row1.to_dict())\n",
    "        for _, row2 in matching_rows_df2.iterrows():\n",
    "            # 可能需要添加额外的逻辑来处理多个匹配的情况\n",
    "            # 这里假设每个df1的行在df2中最多只有一个匹配\n",
    "            merged_row = {**row1.to_dict(), **row2.to_dict()}\n",
    "            results.append(merged_row)\n",
    "\n",
    "# 将结果列表转换为DataFrame\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "result_df_drop = result_df.dropna(subset=['uv'])\n",
    "\n",
    "# 保存到指定路径\n",
    "result_df_drop.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Merged file saved to {output_path}\")\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step4 - 将图片按照比例进行分类\n",
    "### x<0.77 / 0.77<x<1.3 / x>1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到 D://code//data//Lv2期结论//京喜_from_0501//筛选//0.77-1.3.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//img_info.xlsx')\n",
    "\n",
    "# # 图片路径前缀\n",
    "# path_to_your_images = f'D://code//data//background_color//服饰鞋靴箱包//{x}//grounding_output//{y}'\n",
    "\n",
    "# # 去掉File Name列中的后缀\n",
    "# df['File Name'] = df['File Name'].str.split('_').str[0]\n",
    "\n",
    "# # 定义一个函数来计算新的矩形框坐标\n",
    "# def calculate_new_coordinates(group):\n",
    "#     x_coords = group['main_box_x'] + group['main_box_width']\n",
    "#     y_coords = group['main_box_y'] + group['main_box_height']\n",
    "    \n",
    "#     min_x = group['main_box_x'].min()\n",
    "#     min_y = group['main_box_y'].min()\n",
    "#     max_x = x_coords.max()\n",
    "#     max_y = y_coords.max()\n",
    "    \n",
    "#     return pd.Series({\n",
    "#         'merge_x1': min_x,\n",
    "#         'merge_y1': min_y,\n",
    "#         'merge_x2': max_x,\n",
    "#         'merge_y2': max_y\n",
    "#     })\n",
    "\n",
    "\n",
    "# 定义一个函数来计算新的矩形框坐标\n",
    "def calculate_new_coordinates(group):\n",
    "\n",
    "    min_x = group['x1'].min()\n",
    "    min_y = group['y1'].min()\n",
    "    max_x = group['x3'].max()\n",
    "    max_y = group['y3'].max()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'merge_x1': min_x,\n",
    "        'merge_y1': min_y,\n",
    "        'merge_x2': max_x,\n",
    "        'merge_y2': max_y\n",
    "    })\n",
    "\n",
    "\n",
    "# 按File Name分组并计算新坐标\n",
    "df = df.groupby('Image Name').apply(calculate_new_coordinates).reset_index()\n",
    "\n",
    "# 计算矩形框的横纵比\n",
    "df['aspect_ratio'] = (df['merge_x2'] - df['merge_x1']) / (df['merge_y2'] - df['merge_y1'])\n",
    "\n",
    "# # 在图片路径前缀下创建新文件夹\n",
    "# os.makedirs(os.path.join(path_to_your_images, '小于0.77'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(path_to_your_images, '0.77到1.3'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(path_to_your_images, '大于1.3'), exist_ok=True)\n",
    "\n",
    "# 定义一个函数来分类图片并复制到相应文件夹\n",
    "def classify_and_copy_image(row):\n",
    "    # image_path = os.path.join(path_to_your_images, f\"{row['File Name']}\")\n",
    "    if row['aspect_ratio'] < 0.77:\n",
    "        # shutil.copy(image_path, os.path.join(path_to_your_images, '小于0.77', f\"{row['File Name']}.jpg\"))\n",
    "        return '小于0.77'\n",
    "    elif 0.77 <= row['aspect_ratio'] <= 1.3:\n",
    "        # shutil.copy(image_path, os.path.join(path_to_your_images, '0.77到1.3', f\"{row['File Name']}.jpg\"))\n",
    "        return '0.77到1.3'\n",
    "    else:\n",
    "        # shutil.copy(image_path, os.path.join(path_to_your_images, '大于1.3', f\"{row['File Name']}.jpg\"))\n",
    "        return '大于1.3'\n",
    "\n",
    "# 应用分类函数并添加结果列\n",
    "df['classification'] = df.apply(classify_and_copy_image, axis=1)\n",
    "\n",
    "# 保存结果到Excel\n",
    "output_file = os.path.join('D://code//data//Lv2期结论//京喜_from_0501//筛选//0.77-1.3.xlsx')\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"处理完成，结果已保存到 {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step5 - 文本:布局分类&热力图生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取并预处理数据...\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:01<00:00, 43.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！\n",
      "2024-10-18 13:05:12\n",
      "2024-10-18 13:05:12\n",
      "2024-10-18 13:05:12\n",
      "1355 txt\n"
     ]
    }
   ],
   "source": [
    "# 识别文本框是在3x3网格中，并将图片复制到相应的分类目录中，并保存可视化结果\n",
    "# 这段代码是包含左上角的,即还是对可能的logo进行了统计\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "x = '1355'\n",
    "y = 'txt'\n",
    "\n",
    "# 1. 读取和预处理数据\n",
    "def normalize_coordinates(row):\n",
    "    width = 616\n",
    "    height = 616\n",
    "    row['left_norm'] = max(0, min(row['txt_x1'] / width, 1))\n",
    "    row['top_norm'] = max(0, min(row['txt_y1'] / height, 1))\n",
    "    row['right_norm'] = max(0, min(row['txt_x2'] / width, 1))\n",
    "    row['bottom_norm'] = max(0, min(row['txt_y2'] / height, 1))\n",
    "    return row\n",
    "\n",
    "# 2. 绘制矩形和网格\n",
    "def draw_rectangles(group):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        rect = Rectangle((row['left_norm'], 1 - row['bottom_norm']), \n",
    "                         row['right_norm'] - row['left_norm'], \n",
    "                         row['bottom_norm'] - row['top_norm'],\n",
    "                         fill=False, edgecolor='r')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            rect = Rectangle((j/3, 1 - (i+1)/3), 1/3, 1/3, fill=False, edgecolor='b')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    return fig, ax\n",
    "\n",
    "# 3. 判断重叠和分类\n",
    "def check_overlap(rect, grid_cell):\n",
    "    return not (rect[2] < grid_cell[0] or rect[0] > grid_cell[2] or\n",
    "                rect[3] < grid_cell[1] or rect[1] > grid_cell[3])\n",
    "\n",
    "def classify_image(group):\n",
    "    overlaps = [0] * 9\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        rect = (row['left_norm'], row['top_norm'], row['right_norm'], row['bottom_norm'])\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                grid_cell = (j/3, i/3, (j+1)/3, (i+1)/3)\n",
    "                if check_overlap(rect, grid_cell):\n",
    "                    overlaps[i*3 + j] = 1\n",
    "    return ''.join(map(str, overlaps))  # 直接使用join方法生成字符串\n",
    "\n",
    "# 4. 处理单个图像\n",
    "def process_image(name, group, x, y):\n",
    "    classification = classify_image(group)\n",
    "    \n",
    "    # 复制图片\n",
    "    source = os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}', name)\n",
    "    destination = os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}//50%_txt_classified_images', classification.zfill(9), name)  # 使用zfill方法填充前导零\n",
    "    os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "    shutil.copy2(source, destination)\n",
    "    \n",
    "    # 保存可视化结果\n",
    "    fig, ax = draw_rectangles(group)\n",
    "    visualization_name = f\"{name.split('.')[0]}_visualization.png\"  # 保留原文件名的前导零\n",
    "    fig.savefig(os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}//50%_txt_visualizations', visualization_name), bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return name, classification\n",
    "\n",
    "\n",
    "# 主处理函数\n",
    "def main(x, y):\n",
    "    # 读取CSV文件\n",
    "    print(\"读取并预处理数据...\")\n",
    "\n",
    "    # 在读取df时添加筛选条件\n",
    "    def filter_by_rectangle(row):\n",
    "        right, bottom = 616 * 0.3, 616 * 0.2\n",
    "        if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    df = pd.read_excel(os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//merged_info_ctr.xlsx'))\n",
    "    # df = df[df['img_x1'].isna()]\n",
    "    # df = df[df['Subfolder'] == 9775]\n",
    "    # df = df[df['Style'] == 'txt']\n",
    "    # df = df[df['Type'] == 'Original']\n",
    "    # df = df.sort_values('ctr', ascending=False)\n",
    "    # df['File Name1'] = df['File Name1'] + '.jpg'\n",
    "\n",
    "    # df = df[(df['img_x1'].isna()) & (df['Subfolder'] == 9775) & (df['Style'] == 'txt') & (df['Type'] == 'Original')]\n",
    "    # df = df.sort_values('ctr', ascending=False)\n",
    "    # df['File Name1'] = df['File Name1'] + '.jpg'\n",
    "\n",
    "    df = df[(df['img_x1'].isna()) & (df['Subfolder'] == 1355) & (df['Style'] == 'txt') & (df['Type'] == 'Original')]\n",
    "    df = df.sort_values('ctr', ascending=False)\n",
    "    df['Image Name'] = df['Image Name'] + '.jpg'\n",
    "\n",
    "\n",
    "    # 应用左上角的筛选条件\n",
    "    df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "    rows_to_keep = int(len(df) * 0.5)\n",
    "    df = df.head(rows_to_keep)\n",
    "\n",
    "    # 应用normalize_coordinates函数\n",
    "    df = df.apply(normalize_coordinates, axis=1)\n",
    "\n",
    "    # 创建输出目录\n",
    "    classified_images_dir = os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}', '50%_txt_classified_images')\n",
    "    visualizations_dir = os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}', '50%_txt_visualizations')\n",
    "    os.makedirs(classified_images_dir, exist_ok=True)\n",
    "    os.makedirs(visualizations_dir, exist_ok=True)\n",
    "\n",
    "    # 按File name分组并处理\n",
    "    grouped = df.groupby('Image Name')\n",
    "    print(\"处理图像...\")\n",
    "\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_image, name, group, x, y) for name, group in grouped]\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            name, classification = future.result()\n",
    "            results.append((name, classification))\n",
    "    \n",
    "    # 在原Excel文件中新增一列，保存分类结果\n",
    "    classification_df = pd.DataFrame(results, columns=['Image Name', 'Classification'])\n",
    "    df = pd.merge(df, classification_df, on='Image Name', how='left')\n",
    "    df.to_excel(os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'), index=False)\n",
    "    \n",
    "    print(\"处理完成！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main(x, y)\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(x, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 100 的热力图\n",
      "已保存 111 的热力图\n",
      "已保存 1000100 的热力图\n",
      "已保存 1000111 的热力图\n",
      "已保存 1001101 的热力图\n",
      "已保存 1001111 的热力图\n",
      "已保存 100000100 的热力图\n",
      "已保存 100100111 的热力图\n",
      "已保存 100110111 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "2024-10-18 13:06:38\n",
      "2024-10-18 13:06:38\n",
      "2024-10-18 13:06:38\n",
      "1355 txt\n"
     ]
    }
   ],
   "source": [
    "# 针对分类结果, 绘制每个类别的文本框热力图\n",
    "#\n",
    "\n",
    "# 创建保存结果的文件夹\n",
    "output_folder = os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}', '50%_txt_output_heatmaps')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'))\n",
    "\n",
    "# 按Classification列进行分组\n",
    "grouped = df.groupby('Classification')\n",
    "\n",
    "# 遍历每个分组\n",
    "for name, group in grouped:\n",
    "    # 创建一个新的图形\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    # 绘制矩形框\n",
    "    ax1.set_xlim(0, 616)\n",
    "    ax1.set_ylim(616, 0)\n",
    "    for _, row in group.iterrows():\n",
    "        x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "        # 排除左上角的文本框\n",
    "        if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "            continue\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        rect = plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='r')\n",
    "        ax1.add_patch(rect)\n",
    "    ax1.set_title(f'Bounding Boxes for {name}')\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "\n",
    "    # 创建热力图\n",
    "    heatmap = np.zeros((616, 616))\n",
    "    for _, row in group.iterrows():\n",
    "        x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "        # 排除左上角的文本框\n",
    "        if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "            continue\n",
    "        x1, y1 = max(0, min(x1, 615)), max(0, min(y1, 615))\n",
    "        x2, y2 = max(0, min(x2, 615)), max(0, min(y2, 615))\n",
    "        heatmap[int(y1):int(y2)+1, int(x1):int(x2)+1] += 1\n",
    "\n",
    "    # 绘制热力图\n",
    "    sns.heatmap(heatmap, ax=ax2, cmap='YlOrRd', cbar=True)\n",
    "    ax2.set_title(f'Heatmap for {name}')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "\n",
    "    # 调整子图之间的间距\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存图像\n",
    "    plt.savefig(os.path.join(output_folder, f'{name}_heatmap.png'), dpi=100, bbox_inches='tight')\n",
    "    plt.close()  # 关闭图形，释放内存\n",
    "\n",
    "    print(f\"已保存 {name} 的热力图\")\n",
    "\n",
    "print(\"所有热力图已保存在 output_heatmaps 文件夹中\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(x, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本大小的总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap saved to: D://code//data//Lv2期结论//京喜_from_0501//筛选//1355//grounding_output//txt//1355_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//京喜_from_0501//筛选//1355//grounding_output//txt//1355_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//京喜_from_0501//筛选//1355//grounding_output//txt//1355_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//京喜_from_0501//筛选//1355//grounding_output//txt//1355_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "All heatmaps have been generated.\n"
     ]
    }
   ],
   "source": [
    "# 新流程, 可以通过list方式, 来合并读取\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# x = ['女士春夏上装', '女士春夏下装', '女士休闲鞋']\n",
    "# y = ['price', 'txt']\n",
    "\n",
    "x = ['1355']\n",
    "y = ['txt']\n",
    "\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "for item_x in x:\n",
    "    for item_y in y:\n",
    "        # 读取Excel文件\n",
    "        df = pd.read_excel(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{item_x}//grounding_output//{item_y}//50%_txt_info_with_classification-{item_x}_{item_y}.xlsx')\n",
    "\n",
    "        # 应用 filter_by_rectangle 函数来过滤数据\n",
    "        df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "        # 替换 Height_Category 列的值\n",
    "        df['Height_Category'] = df['Height_Category'].replace({\n",
    "            'Height_>38': 'Height大于38',\n",
    "            'Height_18-29': 'Height18到29',\n",
    "            'Height_29-38': 'Height29到38',\n",
    "            'Height_<18': 'Height小于18'\n",
    "        })\n",
    "\n",
    "        # 删除 structure 为空值的行\n",
    "        # df = df.dropna(subset=['structure'])\n",
    "        df = df.dropna(subset=['Height_Category'])\n",
    "\n",
    "        # 确保必要的列存在\n",
    "        required_columns = ['Height_Category', 'txt_x1', 'txt_y1', 'txt_x2', 'txt_y2']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"DataFrame must contain all of these columns: {required_columns}\")\n",
    "\n",
    "        # 创建输出目录\n",
    "        output_dir = f\"D://code//data//Lv2期结论//京喜_from_0501//筛选//{item_x}//grounding_output//{item_y}//{item_x}_{item_y}_wordsize_heatmaps\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 按 structure 和 box_no 分组\n",
    "        grouped = df.groupby(['Height_Category'])\n",
    "\n",
    "        # 遍历每个分组\n",
    "        for (box_no), group in grouped:\n",
    "            # 创建一个空的 2D numpy 数组来存储热力图数据，大小为 616x616\n",
    "            heatmap_data = np.zeros((616, 616))\n",
    "\n",
    "            # 对每个矩形框增加热度值\n",
    "            for _, row in group.iterrows():\n",
    "                x1, y1, x2, y2 = map(int, [row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']])\n",
    "                # 确保坐标不超出边界\n",
    "                x1, x2 = max(0, min(x1, 616)), max(0, min(x2, 616))\n",
    "                y1, y2 = max(0, min(y1, 616)), max(0, min(y2, 616))\n",
    "                heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "            # 创建图形，设置大小为正方形\n",
    "            plt.figure(figsize=(10, 10))\n",
    "\n",
    "            # 使用 seaborn 绘制热力图\n",
    "            sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Frequency'}, square=True)\n",
    "\n",
    "            # 设置标题和轴标签\n",
    "            # plt.title(f'Bounding Box Heatmap - Structure: {structure}, word size: {Height_Category}')\n",
    "            plt.xlabel('X coordinate')\n",
    "            plt.ylabel('Y coordinate')\n",
    "\n",
    "            # 调整图形以保持正方形比例\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "            # 保存图形\n",
    "            output_path = os.path.join(output_dir, f\"heatmap_wordsize_{box_no}.png\")\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"Heatmap saved to: {output_path}\")\n",
    "\n",
    "print(\"All heatmaps have been generated.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1355 - txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height Category: ('Height_18-29',)\n",
      "Text: CHUNYUFENG WaUmnao SANDER Wailnlao CHUNYUFENG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:19<00:57, 19.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: ## 商品描述信息的数据分析总结\n",
      "\n",
      "根据提供的数据集 \"CHUNYUFENG WaUmnao SANDER Wailnlao CHUNYUFENG\"，我们可以得出以下分析结果：\n",
      "\n",
      "### 维度 1: ...\n",
      "Height Category: ('Height_29-38',)\n",
      "Text: 休闲百搭时尚，亲肤舒适 京喜自营 京喜自营 休闲百搭时尚，亲肤舒适 京喜自营 夏季新款热卖 京喜自营 京喜自营 夏季新款热卖 京喜自营 京喜自营 夏季新款热卖 京喜自营 夏季新款热卖 京喜自营 时尚优...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:27<00:25, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: ### 电商商品描述信息维度总结报告\n",
      "\n",
      "#### 1. 商品类别\n",
      "- **细分维度**: T恤类型\n",
      "  - 具体内容: 休闲百搭时尚, 时尚优质百搭女士T恤\n",
      "  - 出现频次: 4\n",
      "\n",
      "#### 2. ...\n",
      "Height Category: ('Height_<18',)\n",
      "Text: CHUNYUFENG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:59<00:21, 21.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: ## 商品描述维度分析报告\n",
      "\n",
      "经过对所提供的商品描述信息数据集的详细分析与总结，以下归纳出商品描述的主要方向维度、细分维度及其具体内容，并统计了各个维度出现的频次。\n",
      "\n",
      "### 1. 产品特性\n",
      "\n",
      "###...\n",
      "Height Category: ('Height_>38',)\n",
      "Text: 纯欲风 包邮 包邮 包邮 包邮 包邮 包邮 包邮 包邮 包邮 包邮 四季百搭 包邮 包邮 精梳纯棉 包邮 纯欲风 包邮 4EM 包邮 包邮 包邮 包邮 包邮 包邮 包邮 包邮 包邮 包邮 包邮 包邮 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:07<00:00, 16.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: ## 商品描述信息的数据分析报告\n",
      "\n",
      "### 1. 物流方式\n",
      "#### 细分维度:\n",
      "- 包邮\n",
      "\n",
      "#### 频次统计:\n",
      "- 包邮: 61 次\n",
      "\n",
      "### 2. 商品风格\n",
      "#### 细分维度:\n",
      "- 纯欲风\n",
      "-...\n",
      "Results saved to: D://code//data//Lv2期结论//京喜_from_0501//筛选//1355//grounding_output//txt//1355_txt_文本分类总结.xlsx\n",
      "All processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "这里是通过读取list形式, 来简化输入的\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# 定义 x 和 y 列表\n",
    "x_list = ['1355']  # 示例值，请根据实际需求修改\n",
    "y_list = ['txt']  # 示例值，请根据实际需求修改\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "def summarize_with_gpt4(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "                # Role \n",
    "                    角色: 电商数据分析师。\n",
    "                # Profile \n",
    "                    简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "                ## Background \n",
    "                    背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "                ## Goals \n",
    "                    目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "                ## Constrains \n",
    "                    约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "                ## Tone \n",
    "                    语气风格: 正式的，客观的，科学的。\n",
    "                ## Skills \n",
    "                    技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "                ## OutputFormat \n",
    "                    输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# 遍历 x 和 y 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        file_path = f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_excel(file_path)\n",
    "        # df = df.dropna(subset=['structure'])\n",
    "\n",
    "        # 确保 'text' 列中的所有值都是字符串\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        \n",
    "        df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "        df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "        # 遍历每个分组，合并文本并进行总结\n",
    "        summaries = []\n",
    "        \n",
    "        # 遍历每个分组\n",
    "        for (height_category), group in tqdm(df_grouped):\n",
    "            # 合并该组的所有文本\n",
    "            all_text = \" \".join(group['text'].dropna())\n",
    "            # print(f\"Structure: {structure}\")\n",
    "            print(f\"Height Category: {height_category}\")\n",
    "            print(f\"Text: {all_text[:100]}...\")  # 只打印前100个字符\n",
    "            \n",
    "            # 使用 GPT-4 进行总结\n",
    "            try:\n",
    "                summary = summarize_with_gpt4(all_text)\n",
    "                print(f\"Summary: {summary[:100]}...\")  # 只打印前100个字符\n",
    "            except Exception as e:\n",
    "                print(f\"Error in summarization: {str(e)}\")\n",
    "                summary = \"Error in summarization\"\n",
    "            \n",
    "            # 将结果添加到列表中\n",
    "            summaries.append({\n",
    "                # 'structure': structure,\n",
    "                'Height_Category': height_category,\n",
    "                'text': all_text,\n",
    "                'summary': summary\n",
    "            })\n",
    "        \n",
    "        # 创建一个新的DataFrame来存储结果\n",
    "        result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "        # 保存结果到Excel文件\n",
    "        output_file = f\"D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}//{x}_{y}_文本分类总结.xlsx\"\n",
    "        result_df.to_excel(output_file, index=False)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "print(\"All processing completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1657 - txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height Category: ('Height_18-29',)\n",
      "Text: 卡扣设计拆装方便 不易扯下 20条装 20*20cm 柔软亲肤·粘合力强·不易缩水 铲刷两件套 BRUSHDUSTPANSUIT 贴心好用每天安心相伴 WORLDLIFE洁牙擦 特殊纳米密胺绵深层清洁...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:16<00:50, 16.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: 好的，让我们开始根据提供的商品描述信息进行归纳总结和频次统计。\n",
      "\n",
      "### 1. 价格信息\n",
      "总频次：0\n",
      "\n",
      "- 直接展示价格\n",
      "    - 频次：0\n",
      "    - 举例：无\n",
      "\n",
      "### 2. 价促活动\n",
      "总频次：...\n",
      "Height Category: ('Height_29-38',)\n",
      "Text: 通用型拉链头/简单更耐用 京喜自营 直接安装 无需缝纫 京喜自营 舒适按摩 京喜自营 弹性梳齿 发型师级 镂空按摩梳 镂空设计 京喜自营 全包防护/加厚涤纶 京喜自营 双层 双层 20*20cm20条...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:51<00:55, 27.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: 基于提供的数据集和定义的维度，以下是对描述信息的归纳总结以及频次统计：\n",
      "\n",
      "1. **价格信息** (总频次: 0)\n",
      "    - 直接展示价格 (频次: 0)\n",
      "\n",
      "2. **价促活动** (总频次: 1)...\n",
      "Height Category: ('Height_<18',)\n",
      "Text: 更方便更轻松 着色污 CoolSumr 秒出美照! 自带反光板 PC CLASSICFLOSSRODS 50 柔精恒滑洁牙护边 DENTALFLOSS强韧不伤牙齿 DENTALFLOSS强韧不伤牙齿 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [01:01<00:19, 19.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: 以下是基于提供的商品描述信息数据集归纳总结出的各维度方向及其出现频次的统计结果：\n",
      "\n",
      "### 1. 价格信息\n",
      "总频次：0  \n",
      "- 直接展示价格：0次\n",
      "  - 举例：无  \n",
      "\n",
      "### 2. 价促活动\n",
      "总频...\n",
      "Height Category: ('Height_>38',)\n",
      "Text: 包邮 拉链头 可拆卸式 包邮 牙膏の开挂收纳 清爽一整夜） 要你爱不释口 包邮 按摩头皮 全包裹 包邮 背面卡扣 包邮 强力去渍 不伤表面 包邮 免缝·裤脚修改 包邮 刷子簸箕套装 安睡整晚 包邮 创...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:14<00:00, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: 根据你提供的数据集和定义，我归纳总结了描述信息的方向维度并统计了每个维度的频次。以下是具体的分析结果：\n",
      "\n",
      "1. **价格信息** 总频次：0\n",
      "   - 直接展示价格：0\n",
      "     - 举例：无\n",
      "\n",
      "2....\n",
      "Results saved to: D://code//data//Lv2期结论//京喜//筛选//1657//grounding_output//txt//1657_txt_文本分类总结.xlsx\n",
      "All processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "这里是通过读取list形式, 来简化输入的\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# 定义 x 和 y 列表\n",
    "x_list = ['1657']  # 示例值，请根据实际需求修改\n",
    "y_list = ['txt']  # 示例值，请根据实际需求修改\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "def summarize_with_gpt4(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "                # Role \n",
    "                角色: 电商数据分析师。\n",
    "                # Profile \n",
    "                简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "                ## Background \n",
    "                背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，并基于一些前置的定义，找出这些描述信息都是从哪些维度切入的。\n",
    "                ## Goals \n",
    "                目标: 基于我给到的商品描述信息数据集和前置的维度定义，归纳总结出描述的方向维度，需要特别关注与细化商品本身的卖点特性，并统计这些维度出现的频率。\n",
    "                ## Definitions\n",
    "                定义：\n",
    "                1. 直接展示价格：直接展示价格信息，到手价，预估到手价，会员价等，通常包含上述前缀，¥+具体的价格数字或者具体的价格数字+元。\n",
    "                2. 折扣信息：描述商品的折扣，通常包含具体的折扣数字+折。\n",
    "                3. 直降信息：描述商品相较原价进行了大幅降价，通常包含直降、立减。\n",
    "                4. 满减信息：描述若购买到一定金额，可以在此基础上进行金额优惠，通常包含满+具体的金额+减+具体的金额\n",
    "                5. 赠品信息：描述若购买商品则会赠送服务或商品，通常包含赠、送\n",
    "                6. 限时：描述商品促销的时间，通常包含活动时间段、活动开始时间、活动结束时间\n",
    "                7. 品牌名称：描述商品的品牌名称\n",
    "                8. 代言人信息：描述商品的代言人信息\n",
    "                9. 价保：价格保护，通常包含价保\n",
    "                10. 店铺背书：描述店铺的信息，通常包含旗舰店、自营\n",
    "                11. 物流服务：描述商品所包含的物流服务，通常包含物流时效、运费险、物流名称、仓库名称、包邮\n",
    "                12. 直接展示价格属于价格信息一级维度，折扣信息、直降信息、满减信息、赠品信息、限时属于价促活动一级维度，品牌名称、代言人信息属于品牌信息一级维度，价保、店铺背书、物流服务属于服务保障一级维度\n",
    "                ## Constrains \n",
    "                约束条件: 1、时刻保持自己是电商数据分析师的角色，2、可以进行适当的联想和猜测，3、举例的时候禁止出现\"\"，4、统计频率的时候请仔细仔细再仔细，5、若识别到的内容不在上述定义的维度中，可自行命名并统计，请不要忽视未被定义的维度，特别是关于商品本身的卖点信息描述\n",
    "                ## Tone \n",
    "                语气风格: 正式的，客观的，科学的。\n",
    "                ## Skills \n",
    "                技能: 1、你有出色的文本理解能力，能够理解输入数据的含义 2、你有出色的归纳总结能力，能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力，能够精确的统计出各个维度出现的频次。\n",
    "                ## OutputFormat \n",
    "                输出格式:以文字方式输出，一级维度，一级维度下具体内容和举例和频次，输出顺序按照价格信息、价促活动、品牌信息、服务保障、商品卖点进行输出，商品卖点为未定义维度，请你依照自己的知识库信息进行汇总输出，需要特别注意，是关于商品本身的描述，输出格式为1.价格信息 总频次 直接展示价格 频次 举例 以此类推,注意输出要精简，减少不必要的换行\n",
    "                    \"\"\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 遍历 x 和 y 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        file_path = f'D://code//data//Lv2期结论//京喜//筛选//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_excel(file_path)\n",
    "        # df = df.dropna(subset=['structure'])\n",
    "\n",
    "        # 确保 'text' 列中的所有值都是字符串\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        \n",
    "        df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "        df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "        # 遍历每个分组，合并文本并进行总结\n",
    "        summaries = []\n",
    "        \n",
    "        # 遍历每个分组\n",
    "        for (height_category), group in tqdm(df_grouped):\n",
    "            # 合并该组的所有文本\n",
    "            all_text = \" \".join(group['text'].dropna())\n",
    "            # print(f\"Structure: {structure}\")\n",
    "            print(f\"Height Category: {height_category}\")\n",
    "            print(f\"Text: {all_text[:100]}...\")  # 只打印前100个字符\n",
    "            \n",
    "            # 使用 GPT-4 进行总结\n",
    "            try:\n",
    "                summary = summarize_with_gpt4(all_text)\n",
    "                print(f\"Summary: {summary[:100]}...\")  # 只打印前100个字符\n",
    "            except Exception as e:\n",
    "                print(f\"Error in summarization: {str(e)}\")\n",
    "                summary = \"Error in summarization\"\n",
    "            \n",
    "            # 将结果添加到列表中\n",
    "            summaries.append({\n",
    "                # 'structure': structure,\n",
    "                'Height_Category': height_category,\n",
    "                'text': all_text,\n",
    "                'summary': summary\n",
    "            })\n",
    "        \n",
    "        # 创建一个新的DataFrame来存储结果\n",
    "        result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "        # 保存结果到Excel文件\n",
    "        output_file = f\"D://code//data//Lv2期结论//京喜//筛选//{x}//grounding_output//{y}//{x}_{y}_文本分类总结.xlsx\"\n",
    "        result_df.to_excel(output_file, index=False)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "print(\"All processing completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
