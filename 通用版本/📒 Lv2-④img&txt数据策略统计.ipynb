{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有的通用变量都放在这里,方便管理\n",
    "\n",
    "# x_list = ['6908','6909','6910','6911','6912','6913','9783','12066']\n",
    "# x_list = ['6914', '6916', '6917', '6918', '9775', '9776', '9777']\n",
    "x_list = ['1','2','3']\n",
    "y_list = ['txt', 'price']\n",
    "path = 'Lv2期结论'\n",
    "\n",
    "# filter_layer_cases = [[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]]\n",
    "filter_layer_cases = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n",
    "\n",
    "\n",
    "# filter_suffix_list = ['filter_1.0_2.0_3.0', 'filter_4.0_5.0_6.0']\n",
    "# filter_suffix_list = ['filter_1.0','filter_3.0','filter_3.0','filter_4.0''filter_5.0' 'filter_6.0']\n",
    "filter_suffix_list = ['filter_1.0_2.0', 'filter_3.0_4.0', 'filter_5.0_6.0']\n",
    "\n",
    "z = '男鞋_新分类_from_0501'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1 - 对于图片主体的box数量统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def process_excel(input_file, output_file):\n",
    "    # 读取Excel文件\n",
    "    df = pd.read_excel(input_file)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # 定义一个函数来移除后缀\n",
    "    def remove_suffix(name):\n",
    "        return re.sub(r'_\\d+$', '', name)\n",
    "\n",
    "    # 应用函数到'Image Name'列\n",
    "    df['Image Name'] = df['Image Name'].apply(remove_suffix)\n",
    "\n",
    "    # 计算每个Image Name的出现次数\n",
    "    name_counts = df['Image Name'].value_counts()\n",
    "\n",
    "    # 创建一个新的'box_no'列，并填充对应的计数\n",
    "    df['box_no'] = df['Image Name'].map(name_counts)\n",
    "\n",
    "    # 保存修改后的DataFrame到新的Excel文件\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"处理完成，结果已保存到 {output_file}\")\n",
    "\n",
    "def process_all_folders(base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'grounding_output' in dirs:\n",
    "            grounding_output_path = os.path.join(root, 'grounding_output')\n",
    "            input_file = os.path.join(grounding_output_path, 'grounding_results.xlsx')\n",
    "            if os.path.exists(input_file):\n",
    "                output_file = os.path.join(grounding_output_path, 'grounding_results_processed.xlsx')\n",
    "                process_excel(input_file, output_file)\n",
    "\n",
    "# 设置基础路径\n",
    "base_path = f'D://code//data//{path}//{z}'\n",
    "\n",
    "# 处理所有文件夹\n",
    "process_all_folders(base_path)\n",
    "\n",
    "print(\"所有文件夹处理完成\")\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1.1 - 在图片信息统计的基础上拼接品牌分成信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加品牌分层信息\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "csv_file_path = f'D://code//data//{path}//{z}//{z}.csv'\n",
    "brand_path = f'D://code//data//{path}//{z}//女鞋品牌分层.xlsx'\n",
    "\n",
    "# 读取CSV文件=\n",
    "df = pd.read_csv(csv_file_path)\n",
    "df_brand = pd.read_excel(brand_path)\n",
    "\n",
    "# 合并df和df_brand数据\n",
    "df = pd.merge(df, df_brand, on='main_brand_code', how='left')\n",
    "\n",
    "# # 定义筛选条件\n",
    "filter_layers = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
    "\n",
    "\n",
    "# 筛选数据\n",
    "filtered_df = df[df['最终分层'].isin(filter_layers)]\n",
    "\n",
    "def extract_matching_part(img_url):\n",
    "    if pd.isna(img_url):\n",
    "        return None\n",
    "    img_url = img_url.split('?')[0]\n",
    "    img_url = os.path.splitext(img_url)[0]\n",
    "    parts = img_url.split('/')\n",
    "    if len(parts) >= 2:\n",
    "        return f\"{parts[-2]}_{parts[-1]}\"\n",
    "    return None\n",
    "\n",
    "filtered_df['matching_part'] = filtered_df['img_url'].apply(extract_matching_part)\n",
    "filtered_df = filtered_df[['matching_part', 'main_brand_code', '最终分层']]\n",
    "\n",
    "def process_excel(input_file, filtered_df):\n",
    "    # 读取Excel文件\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # 先对DataFrame进行去重\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # 定义一个函数来移除后缀\n",
    "    def remove_suffix(name):\n",
    "        return re.sub(r'_\\d+$', '', name)\n",
    "\n",
    "    # 应用函数到'Image Name'列\n",
    "    df['Image Name'] = df['Image Name'].apply(remove_suffix)\n",
    "\n",
    "    # 计算每个Image Name的出现次数\n",
    "    name_counts = df['Image Name'].value_counts()\n",
    "\n",
    "    # 创建一个新的'box_no'列，并填充对应的计数\n",
    "    df['box_no'] = df['Image Name'].map(name_counts)\n",
    "\n",
    "    # 将filtered_df中的数据添加到df中\n",
    "    df = pd.merge(df, filtered_df, left_on='Image Name', right_on='matching_part', how='left')\n",
    "\n",
    "    # 删除matching_part列，因为它与Image Name重复\n",
    "    df = df.drop(columns=['matching_part'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_all_folders(base_path, filtered_df):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'grounding_output' in dirs:\n",
    "            grounding_output_path = os.path.join(root, 'grounding_output')\n",
    "            input_file = os.path.join(grounding_output_path, 'grounding_results.xlsx')\n",
    "            if os.path.exists(input_file):\n",
    "                processed_df = process_excel(input_file, filtered_df)\n",
    "                output_file = os.path.join(grounding_output_path, 'grounding_results_processed.xlsx')\n",
    "                processed_df = processed_df.drop_duplicates()\n",
    "                processed_df.to_excel(output_file, index=False)\n",
    "                print(f\"处理完成，结果已保存到 {output_file}\")\n",
    "\n",
    "# 设置基础路径\n",
    "base_path = f'D://code//data//{path}//{z}'\n",
    "\n",
    "# 处理所有文件夹\n",
    "process_all_folders(base_path, filtered_df)\n",
    "\n",
    "print(\"所有文件夹处理完成\")\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2 - 文本框识别, 合并相邻的文本框, 生成txt_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//男鞋_新分类_from_0501\\1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 1:   2%|▏         | 21/1243 [00:05<05:09,  3.95it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 修改后的代码, 先从图片中识别出文本, 然后分两步\n",
    "# ① 对文本框进行阈值下的合并; 同时也保留原文本框\n",
    "# ② 对文本进行高度和关键词的分类\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from paddleocr import PaddleOCR\n",
    "from PIL import Image\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# 设置输入和输出路径\n",
    "input_folder_path = f'D://code//data//{path}//{z}'\n",
    "output_file_path = f'D://code//data//{path}//{z}//txt_info.xlsx'\n",
    "\n",
    "# 加载 OCR 模型\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", show_log=False)\n",
    "\n",
    "def calculate_shortest_distance(point_a, points_bcd):\n",
    "    shortest_distance = float('inf')\n",
    "    for point_bcd in points_bcd:\n",
    "        distance = ((point_bcd[0] - point_a[0]) ** 2 + (point_bcd[1] - point_a[1]) ** 2) ** 0.5\n",
    "        if distance < shortest_distance:\n",
    "            shortest_distance = distance\n",
    "    return shortest_distance\n",
    "\n",
    "def merge_text_boxes(img_path, style):\n",
    "    result = ocr.ocr(img_path, cls=True)\n",
    "    img = Image.open(img_path)\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    if not result or not result[0]:\n",
    "        # print(f\"No text detected in the image: {img_path}\")\n",
    "        return None, None\n",
    "\n",
    "    rectangles_with_text = result[0]\n",
    "\n",
    "    original_text_box_info = []\n",
    "    for rectangle in rectangles_with_text:\n",
    "        points = rectangle[0]\n",
    "        original_text_box_info.append({\n",
    "            'File Name': os.path.basename(img_path),\n",
    "            'Style': style,\n",
    "            'x1': points[0][0],\n",
    "            'y1': points[0][1],\n",
    "            'x2': points[2][0],\n",
    "            'y2': points[2][1],\n",
    "            'text': rectangle[1][0]\n",
    "        })\n",
    "\n",
    "    merged_text_boxes = []\n",
    "\n",
    "    for index, row in pd.DataFrame(original_text_box_info).iterrows():\n",
    "        if not merged_text_boxes:\n",
    "            merged_text_boxes.append(row.to_dict())\n",
    "        else:\n",
    "            last_merged_box = merged_text_boxes[-1]\n",
    "\n",
    "            if calculate_shortest_distance((row['x1'], row['y1']), [(last_merged_box['x1'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y2']), (last_merged_box['x1'], last_merged_box['y2'])]) < 100:  # 设定文本框合并的阈值\n",
    "                last_merged_box['text'] += ' ' + row['text']\n",
    "                last_merged_box['x1'] = min(last_merged_box['x1'], row['x1'])\n",
    "                last_merged_box['y1'] = min(last_merged_box['y1'], row['y1'])\n",
    "                last_merged_box['x2'] = max(last_merged_box['x2'], row['x2'])\n",
    "                last_merged_box['y2'] = max(last_merged_box['y2'], row['y2'])\n",
    "            else:\n",
    "                merged_text_boxes.append(row.to_dict())\n",
    "\n",
    "    original_text_box_df = pd.DataFrame(original_text_box_info)\n",
    "    merged_text_box_df = pd.DataFrame(merged_text_boxes)\n",
    "\n",
    "    for i, box in original_text_box_df.iterrows():\n",
    "        if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "            region = '上半'\n",
    "        elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "            region = '下半'\n",
    "        elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "            region = '左半'\n",
    "        else:\n",
    "            region = '右半'\n",
    "        original_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "        box_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "        box_per = box_area / (img_width * img_height)\n",
    "        original_text_box_df.at[i, 'txt_Area'] = box_area\n",
    "        original_text_box_df.at[i, 'txt_Per'] = box_per\n",
    "\n",
    "    for i, box in merged_text_box_df.iterrows():\n",
    "        if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "            region = '上半'\n",
    "        elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "            region = '下半'\n",
    "        elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "            region = '左半'\n",
    "        else:\n",
    "            region = '右半'\n",
    "        merged_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "        merge_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "        merge_per = merge_area / (img_width * img_height)\n",
    "        merged_text_box_df.at[i, 'Area'] = merge_area\n",
    "        merged_text_box_df.at[i, 'Per'] = merge_per\n",
    "\n",
    "    return merged_text_box_df, original_text_box_df\n",
    "\n",
    "keyword_groups = {\n",
    "    '通用': ['以旧换新', '只换不修', '包邮', '无理由退', '先用后付', '京东白条', '期免息', '送货上门', '保修'],\n",
    "    '价保': ['价保', '保价'],\n",
    "    '纯价格': ['¥', '夫', '￥', r'\\b价\\b', '到手价', '活动价'],\n",
    "    '直降': ['立减', '直降', '降', '立省', r'^(?!.*升降).*$', r'^(?!.*降温).*$', r'^(?!.*降噪).*$', r'^(?!.*降低).*$'],\n",
    "    '折扣': ['折', r'^(?!.*折叠).*$', r'^(?!.*翻折).*$'],\n",
    "    '满减': [r'.*满.*减.*', r'.*满.*-.*', r'.*满.*免.*'],\n",
    "    '用券': ['用券', '领券', '券'],\n",
    "    '返券': ['返券', '京豆', '返现', r'.*返.*E卡.*', r'.*返.*红包.*'],\n",
    "    '限时': ['.*小时$', '.*天$', '时间', 'time', 'TIME', '限时', r'.*月.*日.*', r'.*日.*点.*', r'.*:.*', r'.*:.*', r'.*：.*', r'\\b\\d{1,2}\\.\\d{1,2}-\\d{1,2}\\b'],\n",
    "    'xx元任选': [r'.*元.*件.*'],\n",
    "    '赠品': [r'.*满.*赠.*', r'.*满.*送.*', '送', '抽', '奖励', '赠', r'^(?!.*送货).*$', r'^(?!.*送礼).*$', r'^(?!.*送装).*$', r'^(?!.*配送).*$', r'^(?!.*送达).*$'],\n",
    "    '节日名称': ['节', '出游季', '购物季', '毕业季', '开学季', '黑五', '周年庆', '儿童节', '父亲节', '端午节', '七夕', '中秋节', '国庆', '万圣节', '感恩节', '元旦', '圣诞', '情人节', '春节', '元宵节', '38节', '3.8节', '清明节', '母亲节', '618', '购物季', '开学季', '11.11', '黑五', '12.12', '女神节', '出游季', '放价季', '吃货节', '家装节'],\n",
    "    '是否限购': ['限购', '限量']\n",
    "}\n",
    "\n",
    "def keyword_analysis(text):\n",
    "    results = {}\n",
    "    for key, words in keyword_groups.items():\n",
    "        results[key] = any(re.search(word, text) for word in words)\n",
    "    return results\n",
    "\n",
    "def height_analysis(x1, y1, x2, y2):\n",
    "    height = abs(y2 - y1)\n",
    "    return height\n",
    "\n",
    "def process_images(folder_path, subfolder_name):\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        if 'grounding_output' in root and ('price' in root or 'txt' in root):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                    image_files.append(os.path.join(root, file))\n",
    "    \n",
    "    combined_results = []\n",
    "    for img_path in tqdm(image_files, desc=f'Processing images in {subfolder_name}'):\n",
    "        style = 'price' if 'price' in img_path else 'txt'\n",
    "        merged_df, original_df = merge_text_boxes(img_path, style)\n",
    "        if merged_df is not None and original_df is not None:\n",
    "            merged_df['Subfolder'] = subfolder_name\n",
    "            original_df['Subfolder'] = subfolder_name\n",
    "            combined_results.append({\n",
    "                'original': original_df,\n",
    "                'merged': merged_df\n",
    "            })\n",
    "    \n",
    "    return combined_results\n",
    "\n",
    "# 主程序\n",
    "all_results = []\n",
    "for folder in os.listdir(input_folder_path):\n",
    "    if folder.isdigit():\n",
    "        folder_path = os.path.join(input_folder_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"正在处理文件夹: {folder_path}\")\n",
    "            results = process_images(folder_path, folder)\n",
    "            all_results.extend(results)\n",
    "\n",
    "final_combined_data = []\n",
    "for result in all_results:\n",
    "    result['original']['Type'] = 'Original'\n",
    "    result['merged']['Type'] = 'Merged'\n",
    "    combined = pd.concat([result['original'], result['merged']], ignore_index=True)\n",
    "    final_combined_data.append(combined)\n",
    "\n",
    "final_combined_df = pd.concat(final_combined_data, ignore_index=True)\n",
    "final_combined_df.sort_values(by=['Subfolder', 'File Name', 'Type'], inplace=True)\n",
    "\n",
    "for index, row in tqdm(final_combined_df.iterrows(), total=final_combined_df.shape[0], desc=\"Analyzing text\"):\n",
    "    keyword_results = keyword_analysis(row['text'])\n",
    "    for key, value in keyword_results.items():\n",
    "        final_combined_df.at[index, key] = value\n",
    "    \n",
    "    height = height_analysis(row['x1'], row['y1'], row['x2'], row['y2'])\n",
    "    final_combined_df.at[index, 'Height'] = height\n",
    "    final_combined_df.at[index, 'Height_Category'] = (\n",
    "        'Height_<18' if height < 18 else\n",
    "        'Height_18-29' if 18 <= height < 29 else\n",
    "        'Height_29-38' if 29 <= height < 38 else\n",
    "        'Height_>38'\n",
    "    )\n",
    "\n",
    "final_combined_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print('处理完成')\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 - 在现有txt_info的基础上, 拼接品牌分类, 生成txt_info-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在现有txt_info的基础上, 拼接品牌分类\n",
    "# 在现有txt_info的基础上, 拼接品牌分类\n",
    "# 在现有txt_info的基础上, 拼接品牌分类\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from paddleocr import PaddleOCR\n",
    "from PIL import Image\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "in_file_path = f'D://code//data//{path}//{z}//txt_info.xlsx'\n",
    "output_file_path = f'D://code//data//{path}//{z}//txt_info-1.xlsx'\n",
    "\n",
    "df = pd.read_excel(in_file_path)\n",
    "\n",
    "# 删除df的File Name列中的.jpg扩展名\n",
    "df['File Name'] = df['File Name'].str.replace('.jpg', '')\n",
    "\n",
    "filtered_df = filtered_df.drop_duplicates()\n",
    "\n",
    "# 删除filtered_df的matching_part列中的.jpg扩展名\n",
    "filtered_df['matching_part'] = filtered_df['matching_part'].str.replace('.jpg', '')\n",
    "\n",
    "df1 = pd.merge(df, filtered_df, left_on='File Name', right_on='matching_part', how='left')\n",
    "\n",
    "df1.to_excel(output_file_path)\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3 - 将分散在各个grounding_output文件夹中的grounding_results_processed合并为img_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "def merge_excel_files(base_path):\n",
    "    # 用于存储所有数据框的列表\n",
    "    all_dataframes = []\n",
    "\n",
    "    # 遍历基础路径下的所有文件夹\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'grounding_output' in dirs:\n",
    "            grounding_output_path = os.path.join(root, 'grounding_output')\n",
    "            excel_file = os.path.join(grounding_output_path, 'grounding_results_processed.xlsx')\n",
    "            \n",
    "            if os.path.exists(excel_file):\n",
    "                # 读取Excel文件\n",
    "                df = pd.read_excel(excel_file)\n",
    "                \n",
    "                # 添加新列，值为当前子文件夹的名称\n",
    "                subfolder_name = os.path.basename(os.path.dirname(grounding_output_path))\n",
    "                df['Subfolder'] = subfolder_name\n",
    "                \n",
    "                # 将数据框添加到列表中\n",
    "                all_dataframes.append(df)\n",
    "\n",
    "    # 合并所有数据框\n",
    "    if all_dataframes:\n",
    "        merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        \n",
    "        # 保存合并后的数据框到新的Excel文件\n",
    "        output_file = os.path.join(base_path, 'img_info.xlsx')\n",
    "        merged_df.to_excel(output_file, index=False)\n",
    "        print(f\"合并完成，结果保存在: {output_file}\")\n",
    "    else:\n",
    "        print(\"没有找到符合条件的Excel文件\")\n",
    "\n",
    "# 使用示例\n",
    "base_path = f\"D://code//data//{path}//{z}\"\n",
    "merge_excel_files(base_path)\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3.1 - 在img_info后面拼接品牌信息, 生成img_info-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在img_info后面拼接品牌分类信息\n",
    "\n",
    "\n",
    "# z = '女士春夏上装_from_0501'\n",
    "\n",
    "in_file_path = f'D://code//data//{path}//{z}//img_info.xlsx'\n",
    "output_file_path = f'D://code//data//{path}//{z}//img_info-1.xlsx'\n",
    "\n",
    "df = pd.read_excel(in_file_path)\n",
    "\n",
    "# 删除df的File Name列中的.jpg扩展名\n",
    "df['Image Name'] = df['Image Name'].str.replace('.jpg', '')\n",
    "\n",
    "filtered_df = filtered_df.drop_duplicates()\n",
    "\n",
    "# 删除filtered_df的matching_part列中的.jpg扩展名\n",
    "filtered_df['matching_part'] = filtered_df['matching_part'].str.replace('.jpg', '')\n",
    "\n",
    "df1 = pd.merge(df, filtered_df, left_on='Image Name', right_on='matching_part', how='left')\n",
    "df1 = df1.drop_duplicates()\n",
    "\n",
    "df1.to_excel(output_file_path)\n",
    "print(output_file_path)\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step4 - 将布局和ctr参数进行合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def merge_excel_files(txt_box_info_file, img_box_info_file, output_file):\n",
    "    # 读取 txt_box_info 文件\n",
    "    txt_df = pd.read_excel(txt_box_info_file)\n",
    "    txt_df = txt_df.rename(columns={'File Name': 'Image Name', 'x1': 'txt_x1', 'y1': 'txt_y1', 'x2': 'txt_x2', 'y2': 'txt_y2'})\n",
    "\n",
    "    # 读取 img_box_info 文件\n",
    "    img_df = pd.read_excel(img_box_info_file)\n",
    "\n",
    "    # 重命名列\n",
    "    img_df = img_df.rename(columns={'x1': 'img_x1', 'y1': 'img_y1', 'x3': 'img_x2', 'y3': 'img_y2', 'Subfolder': 'Style'})\n",
    "    img_df = img_df.loc[:, ['Image Name', 'Style', 'img_x1', 'img_y1', 'img_x2', 'img_y2', 'box_no']]\n",
    "    img_df = img_df.drop_duplicates()\n",
    "\n",
    "    # 合并两个DataFrame，以'Image Name'为键进行合并\n",
    "    merged_df = pd.merge(txt_df, img_df, on='Image Name', how='outer')\n",
    "\n",
    "    # 将 img_box_info 中缺少的数据设置为空\n",
    "    merged_df = merged_df.fillna(\"\")\n",
    "\n",
    "    # 使用正则表达式删除.jpg 或.png 后缀\n",
    "    merged_df['Image Name'] = merged_df['Image Name'].str.replace(r'\\.(?:jpg|png)$', '', regex=True)\n",
    "\n",
    "    # 将合并后的DataFrame写入新的Excel文件\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        merged_df.to_excel(writer, index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    txt_box_info_file = f\"D://code//data//{path}//{z}//txt_info-1.xlsx\"\n",
    "    img_box_info_file = f\"D://code//data//{path}//{z}//img_info.xlsx\"\n",
    "    output_file = f\"D://code//data//{path}//{z}//merged_info-1.xlsx\"\n",
    "\n",
    "    merge_excel_files(txt_box_info_file, img_box_info_file, output_file)\n",
    "\n",
    "    print(\"Excel files merged successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 定义路径\n",
    "data_1 = f\"D://code//data//{path}//{z}//merged_info-1.xlsx\"\n",
    "data_2 = f\"D://code//data//{path}//{z}//{z}.csv\"\n",
    "# data_3 = 'D://code//data//howtodo_from_0401//服饰鞋靴箱包//品类聚类-服饰鞋靴箱包.csv'\n",
    "output_path = f\"D://code//data//{path}//{z}//merged_info_ctr-1.xlsx\"\n",
    "\n",
    "# 读取df1\n",
    "df1 = pd.read_excel(data_1)\n",
    "\n",
    "# 读取df2\n",
    "df2 = pd.read_csv(data_2)\n",
    "\n",
    "aggregated_data = df2.groupby('img_url').agg({\n",
    "    'cid2': 'first',  # 使用 'first' 函数来选择分组中的第一个值\n",
    "    'cid3': 'first',\n",
    "    'uv': 'sum',\n",
    "    'click_uv': 'sum',\n",
    "    'gmv_cj':'sum',\n",
    "    'sale_qtty_cj':'sum',\n",
    "    'sku':'first'\n",
    "#     'folder_path': 'first'  # 同样使用 'first' 函数选择第一个值\n",
    "}).reset_index()  # 重置索引\n",
    "\n",
    "df2 = aggregated_data\n",
    "\n",
    "# 计算ctr字段\n",
    "df2['ctr'] = df2['click_uv'] / df2['uv']\n",
    "\n",
    "\n",
    "def extract_filename(x):\n",
    "    # 分割路径，取倒数第二部分和最后一部分（文件名部分）\n",
    "    parts = x.split('/')\n",
    "    return f\"{parts[-2]}_{os.path.splitext(parts[-1])[0]}\"  # 保留原文件扩展名\n",
    "    # return f\"{os.path.splitext(parts[-1])[0]}.jpg\"  # 保留原文件扩展名\n",
    "\n",
    "# 应用函数\n",
    "df2['only_2'] = df2['img_url'].apply(extract_filename)\n",
    "\n",
    "# 初始化结果列表\n",
    "results = []\n",
    "\n",
    "# 遍历df1的每一行\n",
    "for index, row1 in df1.iterrows():\n",
    "    # 查找df2中匹配的行\n",
    "    matching_rows_df2 = df2[df2['only_2'] == row1['Image Name']]\n",
    "    \n",
    "    # 如果没有找到匹配的行，则只添加df1的当前行\n",
    "    if matching_rows_df2.empty:\n",
    "        results.append(row1.to_dict())\n",
    "    else:\n",
    "        # 对于找到的每个匹配行，先添加df1的当前行，然后添加匹配的df2行\n",
    "        results.append(row1.to_dict())\n",
    "        for _, row2 in matching_rows_df2.iterrows():\n",
    "            # 可能需要添加额外的逻辑来处理多个匹配的情况\n",
    "            # 这里假设每个df1的行在df2中最多只有一个匹配\n",
    "            merged_row = {**row1.to_dict(), **row2.to_dict()}\n",
    "            results.append(merged_row)\n",
    "\n",
    "# 将结果列表转换为DataFrame\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "result_df_drop = result_df.dropna(subset=['uv'])\n",
    "\n",
    "# 保存到指定路径\n",
    "result_df_drop.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Merged file saved to {output_path}\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step5 - 将图片按照比例进行分类\n",
    "### x<0.77 / 0.77<x<1.3 / x>1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 这是用来对图片尺寸进行分类的,但是感觉在实际项目中并没有什么用,暂时注销了\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import shutil\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# # 读取Excel文件\n",
    "# df = pd.read_excel(f'D://code//data//{path}//{z}//img_info-1.xlsx')\n",
    "\n",
    "\n",
    "# # 定义一个函数来计算新的矩形框坐标\n",
    "# def calculate_new_coordinates(group):\n",
    "\n",
    "#     min_x = group['x1'].min()\n",
    "#     min_y = group['y1'].min()\n",
    "#     max_x = group['x3'].max()\n",
    "#     max_y = group['y3'].max()\n",
    "    \n",
    "#     return pd.Series({\n",
    "#         'merge_x1': min_x,\n",
    "#         'merge_y1': min_y,\n",
    "#         'merge_x2': max_x,\n",
    "#         'merge_y2': max_y\n",
    "#     })\n",
    "\n",
    "\n",
    "# # 按File Name分组并计算新坐标\n",
    "# df = df.groupby('Image Name').apply(calculate_new_coordinates).reset_index()\n",
    "\n",
    "# # 计算矩形框的横纵比\n",
    "# df['aspect_ratio'] = (df['merge_x2'] - df['merge_x1']) / (df['merge_y2'] - df['merge_y1'])\n",
    "\n",
    "\n",
    "# # 定义一个函数来分类图片并复制到相应文件夹\n",
    "# def classify_and_copy_image(row):\n",
    "#     # image_path = os.path.join(path_to_your_images, f\"{row['File Name']}\")\n",
    "#     if row['aspect_ratio'] < 0.77:\n",
    "#         # shutil.copy(image_path, os.path.join(path_to_your_images, '小于0.77', f\"{row['File Name']}.jpg\"))\n",
    "#         return '小于0.77'\n",
    "#     elif 0.77 <= row['aspect_ratio'] <= 1.3:\n",
    "#         # shutil.copy(image_path, os.path.join(path_to_your_images, '0.77到1.3', f\"{row['File Name']}.jpg\"))\n",
    "#         return '0.77到1.3'\n",
    "#     else:\n",
    "#         # shutil.copy(image_path, os.path.join(path_to_your_images, '大于1.3', f\"{row['File Name']}.jpg\"))\n",
    "#         return '大于1.3'\n",
    "\n",
    "# # 应用分类函数并添加结果列\n",
    "# df['classification'] = df.apply(classify_and_copy_image, axis=1)\n",
    "\n",
    "# # 保存结果到Excel\n",
    "# output_file = os.path.join(f'D://code//data//{path}//{z}//0.77-1.3-1.xlsx')\n",
    "# df.to_excel(output_file, index=False)\n",
    "\n",
    "# print(f\"处理完成，结果已保存到 {output_file}\")\n",
    "\n",
    "# import datetime\n",
    "\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step6 - 针对商品主体的坐标和分类统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没有考虑brand\n",
    "# 这里的热力图不是基于25宫格分类,而是基于商品数量\n",
    "# 还是保留了5x5网格分类,生成了线框图和分类图片\n",
    "# 将结果统计为了一个新的excel表,包含'x', 'y', 'box_no', 'x1', 'y1', 'x2', 'y2'\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "import datetime\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 用于收集每个 x 和 y 分类的结果\n",
    "all_results = []\n",
    "\n",
    "# 1. 归一化坐标\n",
    "def normalize_coordinates(row):\n",
    "    width, height = 800, 800\n",
    "    row['left_norm'] = max(0, min(row['img_x1'] / width, 1))\n",
    "    row['top_norm'] = max(0, min(row['img_y1'] / height, 1))\n",
    "    row['right_norm'] = max(0, min(row['img_x2'] / width, 1))\n",
    "    row['bottom_norm'] = max(0, min(row['img_y2'] / height, 1))\n",
    "    return row\n",
    "\n",
    "# 2. 判断重叠和分类\n",
    "def check_overlap(rect, grid_cell):\n",
    "    return not (rect[2] < grid_cell[0] or rect[0] > grid_cell[2] or\n",
    "                rect[3] < grid_cell[1] or rect[1] > grid_cell[3])\n",
    "\n",
    "def classify_image(group):\n",
    "    overlaps = [0] * 25\n",
    "    for _, row in group.iterrows():\n",
    "        rect = (row['left_norm'], row['top_norm'], row['right_norm'], row['bottom_norm'])\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                grid_cell = (i / 5, j / 5, (i + 1) / 5, (j + 1) / 5)\n",
    "                if check_overlap(rect, grid_cell):\n",
    "                    overlaps[i * 5 + j] = 1\n",
    "    return ''.join(map(str, overlaps))\n",
    "\n",
    "# 3. 绘制线框图\n",
    "def draw_rectangles(group, save_path):\n",
    "    fig, ax = plt.subplots(figsize=(6.16, 6.16), dpi=100)\n",
    "\n",
    "    # 绘制矩形框\n",
    "    for _, row in group.iterrows():\n",
    "        rect = Rectangle((row['left_norm'], 1 - row['bottom_norm']),\n",
    "                         row['right_norm'] - row['left_norm'],\n",
    "                         row['bottom_norm'] - row['top_norm'],\n",
    "                         fill=False, edgecolor='r')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # 绘制5x5网格\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            grid_rect = Rectangle((j / 5, 1 - (i + 1) / 5), 1 / 5, 1 / 5, fill=False, edgecolor='b')\n",
    "            ax.add_patch(grid_rect)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # 保存线框图\n",
    "    fig.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "# 4. 处理单个图像并分类\n",
    "def process_image(name, group, x, y):\n",
    "    # 检查源图片是否存在，避免处理不存在的文件\n",
    "    source = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', name)\n",
    "    if not os.path.exists(source):\n",
    "        # print(f\"File not found: {source}\")\n",
    "        return None\n",
    "\n",
    "    # 分类图像\n",
    "    classification = classify_image(group)\n",
    "    \n",
    "    # 处理这个图片的所有框\n",
    "    results = []\n",
    "    for _, row in group.iterrows():\n",
    "        results.append((\n",
    "            x,                  # x\n",
    "            y,                  # y\n",
    "            row['box_no'],      # box_no\n",
    "            row['x1'],          # x1\n",
    "            row['y1'],          # y1\n",
    "            row['x2'],          # x2\n",
    "            row['y2'],          # y2\n",
    "            name,               # Image Name\n",
    "            classification,     # Classification\n",
    "            row['uv'],         # uv\n",
    "            row['click_uv'],   # click_uv\n",
    "            row['最终分层']     # 添加最终分层\n",
    "        ))\n",
    "\n",
    "    # 复制图片到分类文件夹\n",
    "    destination_dir = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_img_classified_images', classification)\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    destination = os.path.join(destination_dir, name)\n",
    "    shutil.copy(source, destination)\n",
    "\n",
    "    # 绘制并保存线框图\n",
    "    visualization_dir = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_img_visualizations'\n",
    "    os.makedirs(visualization_dir, exist_ok=True)\n",
    "    draw_rectangles(group, os.path.join(visualization_dir, f\"{name}_visualization.png\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "# 主处理函数\n",
    "def main(x, y):\n",
    "    print(f\"Processing for x: {x}, y: {y}\")\n",
    "    df = pd.read_excel(f'D://code//data//{path}//{z}//merged_info_ctr-1.xlsx')\n",
    "    \n",
    "    # 添加验证代码\n",
    "    required_columns = ['box_no', 'uv', 'click_uv']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"'{col}' 列在Excel文件中不存在\")\n",
    "    \n",
    "    df['Image Name'] = df['Image Name'].astype(str) + '.jpg'\n",
    "\n",
    "    # 计算 x2 和 y2\n",
    "    df['x1'] = df['img_x1']\n",
    "    df['y1'] = df['img_y1']\n",
    "    df['x2'] = df['img_x2']\n",
    "    df['y2'] = df['img_y2']\n",
    "\n",
    "    # 删除重复 'ctr' 列\n",
    "    if 'ctr' in df.columns and df.columns.duplicated().any():\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # 按 'ctr' 列排序并取前50%\n",
    "    df = df.sort_values(by='ctr', ascending=False)\n",
    "    rows_to_keep = int(len(df) * 0.5)  # 取前50% ###############################################\n",
    "    df = df.head(rows_to_keep)\n",
    "    df = df.apply(normalize_coordinates, axis=1)\n",
    "\n",
    "    # 添加这行：按 'Image Name' 分组\n",
    "    grouped = df.groupby('Image Name')\n",
    "\n",
    "    # 按 'Image Name' 分组并处理\n",
    "    # 修改结果收集方式\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_image, name, group, x, y) for name, group in grouped]\n",
    "\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            results = future.result()\n",
    "            if results is not None:\n",
    "                for result in results:\n",
    "                    all_results.append(result)\n",
    "\n",
    "# 处理所有 x 和 y 组合\n",
    "for x, y in itertools.product(x_list, y_list):\n",
    "    main(x, y)\n",
    "\n",
    "# 保存总表\n",
    "output_df = pd.DataFrame(all_results, columns=[\n",
    "    'x', 'y', 'box_no', 'x1', 'y1', 'x2', 'y2', \n",
    "    'Image Name', 'Classification', 'uv', 'click_uv', '最终分层'\n",
    "])\n",
    "output_file = f'D://code//data//{path}//{z}//img_all_classification_results-1.xlsx'\n",
    "output_df.to_excel(output_file, index=False)\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "print(output_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step6.1 - 生成基于商品主体数量的饼图和excel\n",
    "## 不分brand,all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# 读取数据表\n",
    "file_path = f'D://code//data//{path}//{z}//img_all_classification_results-1.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df.dropna()  # 去除空值\n",
    "\n",
    "# 设置保存图像的文件夹\n",
    "output_folder = f'D://code//data//{path}//{z}//url_3'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 现代时尚的配色方案\n",
    "modern_colors = [\n",
    "    '#FF6B6B',  # 珊瑚红\n",
    "    '#4ECDC4',  # 青绿\n",
    "    '#45B7D1',  # 天蓝\n",
    "    '#96CEB4',  # 薄荷绿\n",
    "    '#FFEEAD',  # 淡黄\n",
    "]\n",
    "\n",
    "# 创建一个列表来存储表格数据\n",
    "table_data = []\n",
    "\n",
    "# 1. 饼图部分和数据收集\n",
    "grouped_by_xy = df.groupby(['x', 'y'])\n",
    "\n",
    "for (x, y), group in grouped_by_xy:\n",
    "    # 计算每个box_no的统计信息\n",
    "    box_stats = group.groupby('box_no').agg({\n",
    "        'uv': 'sum',\n",
    "        'click_uv': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # 计算CTR和添加count列\n",
    "    box_stats['ctr'] = box_stats['click_uv'] / box_stats['uv']\n",
    "    box_stats['count'] = group['box_no'].value_counts().reindex(box_stats['box_no']).values\n",
    "    total_count = box_stats['count'].sum()\n",
    "    box_stats['count_pct'] = box_stats['count'] / total_count  # 添加count百分比列\n",
    "    \n",
    "    # 添加到表格数据\n",
    "    for _, row in box_stats.iterrows():\n",
    "        table_data.append({\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'box_no': row['box_no'],\n",
    "            'brand': 'all',\n",
    "            'uv': row['uv'],\n",
    "            'click_uv': row['click_uv'],\n",
    "            'ctr': row['ctr'],\n",
    "            'count': row['count'],\n",
    "            'count_pct': row['count_pct']  # 百分比列\n",
    "        })\n",
    "    \n",
    "    # 绘制饼图\n",
    "    box_no_counts = group['box_no'].value_counts()\n",
    "    labels = [f'No:{idx}\\nCount: {box_no_counts[idx]}\\nCTR: {box_stats[box_stats[\"box_no\"] == idx][\"ctr\"].values[0]:.2%}' \n",
    "            for idx in box_no_counts.index]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(box_no_counts, \n",
    "            labels=labels, \n",
    "            autopct='%1.1f%%', \n",
    "            startangle=140, \n",
    "            colors=modern_colors,\n",
    "            shadow=False,\n",
    "            textprops={'fontsize': 12})\n",
    "    plt.title(f'{x}_{y} Box Distribution', pad=20, fontsize=14)\n",
    "    \n",
    "    pie_path = os.path.join(output_folder, f'{x}_{y}_img_url_3_boxper.png')\n",
    "    plt.savefig(pie_path, bbox_inches='tight', dpi=100)\n",
    "    print(f\"饼图已生成并保存: {pie_path}\")  # 打印饼图路径\n",
    "    plt.close()\n",
    "\n",
    "# 创建并保存表格\n",
    "table_df = pd.DataFrame(table_data)\n",
    "table_df = table_df[['x', 'y', 'box_no', 'brand', 'uv', 'click_uv', 'ctr', 'count', 'count_pct']]  # 确保列的顺序\n",
    "table_df.sort_values(['x', 'y', 'box_no'], inplace=True)  # 排序\n",
    "table_output_path = os.path.join(output_folder, 'img_all_url_3_boxper.xlsx')\n",
    "table_df.to_excel(table_output_path, index=False)\n",
    "print(f\"Excel表格已保存: {table_output_path}\")  # 打印Excel路径\n",
    "\n",
    "# 2. 热力图部分\n",
    "grouped_by_xyz = df.groupby(['x', 'y', 'box_no'])\n",
    "\n",
    "for (x, y, box_no), group in grouped_by_xyz:\n",
    "    heatmap_data = np.zeros((800, 800))\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        scale_factor = 800 / max(800, row['x2'], row['y2'])\n",
    "        x1 = int(row['x1'] * scale_factor)\n",
    "        y1 = int(row['y1'] * scale_factor)\n",
    "        x2 = int(row['x2'] * scale_factor)\n",
    "        y2 = int(row['y2'] * scale_factor)\n",
    "        \n",
    "        x1, x2 = max(0, min(x1, 799)), max(0, min(x2, 799))\n",
    "        y1, y2 = max(0, min(y1, 799)), max(0, min(y2, 799))\n",
    "        heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(heatmap_data, \n",
    "                cmap='YlOrRd',\n",
    "                cbar=False,\n",
    "                xticklabels=False,\n",
    "                yticklabels=False)\n",
    "    \n",
    "    heatmap_path = os.path.join(output_folder, f'{x}_{y}_img_boxno_{box_no}_brand_all_url_3_boxhot.png')\n",
    "    plt.title(f'{x}_{y}_{box_no} Heatmap', pad=20, fontsize=14)\n",
    "    plt.savefig(heatmap_path, bbox_inches='tight', dpi=100, pad_inches=0)\n",
    "    print(f\"热力图已生成并保存: {heatmap_path}\")  # 打印热力图路径\n",
    "    plt.close()\n",
    "\n",
    "print(\"所有图表和统计表格已生成并保存。\")\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import datetime\n",
    "\n",
    "# # 定义 x_list 和 y_list\n",
    "# x_list = ['9736', '9735', '12004']\n",
    "# y_list = ['txt', 'price']\n",
    "# path = 'Lv2期结论'\n",
    "# z = '男士春夏下装_from_0501'\n",
    "\n",
    "# # 读取数据表\n",
    "# file_path = f'D://code//data//{path}//{z}//img_all_classification_results-1.xlsx'\n",
    "# df = pd.read_excel(file_path)\n",
    "# df = df.dropna()  # 去除空值\n",
    "\n",
    "# # 设置保存图像的文件夹\n",
    "# output_folder = f'D://code//data//{path}//{z}//url_3'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # 现代时尚的配色方案\n",
    "# modern_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEEAD']\n",
    "\n",
    "# # 创建一个列表来存储表格数据\n",
    "# table_data = []\n",
    "\n",
    "# # 确保 x 和 y 类型一致\n",
    "# df['x'] = df['x'].astype(str)  # 或根据实际情况改为 int\n",
    "# df['y'] = df['y'].astype(str)\n",
    "\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         # 1. 饼图部分和数据收集\n",
    "#         grouped_by_xy = df[(df['x'] == x) & (df['y'] == y)].groupby(['x', 'y'])\n",
    "\n",
    "#         for (x, y), group in grouped_by_xy:\n",
    "#             # 计算每个 box_no 的统计信息\n",
    "#             box_stats = group.groupby('box_no').agg({\n",
    "#                 'uv': 'sum',\n",
    "#                 'click_uv': 'sum'\n",
    "#             }).reset_index()\n",
    "            \n",
    "#             # 计算 CTR 和添加 count 列\n",
    "#             box_stats['ctr'] = box_stats['click_uv'] / box_stats['uv']\n",
    "#             box_stats['count'] = group['box_no'].value_counts().reindex(box_stats['box_no']).values\n",
    "#             total_count = box_stats['count'].sum()\n",
    "#             box_stats['count_pct'] = box_stats['count'] / total_count  # 添加 count 百分比列\n",
    "            \n",
    "#             # 添加到表格数据\n",
    "#             for _, row in box_stats.iterrows():\n",
    "#                 row_data = {\n",
    "#                     'x': x,\n",
    "#                     'y': y,\n",
    "#                     'box_no': row['box_no'],\n",
    "#                     'uv': row['uv'],\n",
    "#                     'click_uv': row['click_uv'],\n",
    "#                     'ctr': row['ctr'],\n",
    "#                     'count': row['count'],\n",
    "#                     'count_pct': row['count_pct']  # 百分比列\n",
    "#                 }\n",
    "#                 table_data.append(row_data)\n",
    "            \n",
    "#             # 绘制饼图\n",
    "#             box_no_counts = group['box_no'].value_counts()\n",
    "#             labels = [f'No:{idx}\\nCount: {box_no_counts[idx]}\\nCTR: {box_stats[box_stats[\"box_no\"] == idx][\"ctr\"].values[0]:.2%}' \n",
    "#                     for idx in box_no_counts.index]\n",
    "\n",
    "#             plt.figure(figsize=(8, 8))\n",
    "#             plt.pie(box_no_counts, \n",
    "#                     labels=labels, \n",
    "#                     autopct='%1.1f%%', \n",
    "#                     startangle=140, \n",
    "#                     colors=modern_colors,\n",
    "#                     shadow=False,\n",
    "#                     textprops={'fontsize': 12})\n",
    "#             plt.title(f'{x}_{y} Box Distribution', pad=20, fontsize=14)\n",
    "#             pie_path = os.path.join(output_folder, f'{x}_{y}_img_url_3_boxper.png')\n",
    "#             plt.savefig(pie_path, bbox_inches='tight', dpi=100)\n",
    "#             print(f\"饼图已生成并保存: {pie_path}\")  # 打印饼图路径\n",
    "#             plt.close()\n",
    "\n",
    "# # 创建并保存表格\n",
    "# table_df = pd.DataFrame(table_data)\n",
    "# table_df = table_df[['x', 'y', 'box_no', 'uv', 'click_uv', 'ctr', 'count', 'count_pct']]  # 确保列的顺序\n",
    "# table_df.sort_values(['x', 'y', 'box_no'], inplace=True)  # 排序\n",
    "# table_output_path = os.path.join(output_folder, 'img_all_url_3_boxper.xlsx')\n",
    "# table_df.to_excel(table_output_path, index=False)\n",
    "# print(f\"Excel表格已保存: {table_output_path}\")  # 打印Excel路径\n",
    "\n",
    "# # 打印当前时间\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step6.2 - 生成基于商品主体数量的饼图和excel\n",
    "### 分brand, 类似 filter_layer_cases = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # 读取 Excel 文件\n",
    "# df = pd.read_excel(f'D://code//data//{path}//{z}//all_classification_results-1.xlsx')\n",
    "\n",
    "# # 创建字典，用于存储每个 \"Image Name\" 的非空 \"最终分层\" 值\n",
    "# final_layer_map = df.dropna(subset=['最终分层']).set_index('Image Name')['最终分层'].to_dict()\n",
    "\n",
    "# # 定义填充函数\n",
    "# def fill_final_layer(row):\n",
    "#     if pd.isna(row['最终分层']):  # 检查 \"最终分层\" 是否为空\n",
    "#         return final_layer_map.get(row['Image Name'], row['最终分层'])  # 使用字典中的值填充\n",
    "#     return row['最终分层']\n",
    "\n",
    "# # 应用填充函数\n",
    "# df['最终分层'] = df.apply(fill_final_layer, axis=1)\n",
    "\n",
    "# # 过滤层级\n",
    "# output_folder = f'D://code//data//{path}//{z}//url_3'\n",
    "# os.makedirs(output_folder, exist_ok=True)  # 创建主输出文件夹\n",
    "\n",
    "# # 存储统计结果的列表\n",
    "# summary_data = []\n",
    "\n",
    "# # 遍历 filter_layer_cases 中的每个子列表\n",
    "# for layers in filter_layer_cases:\n",
    "#     # 筛选符合当前 layers 的数据\n",
    "#     filtered_df = df[df['最终分层'].isin(layers)]\n",
    "    \n",
    "#     # 将 layers 转换为字符串格式 1.0_2.0_3.0\n",
    "#     layer_suffix = \"_\".join(map(str, layers))\n",
    "\n",
    "#     # 按 x, y, box_no 进行分组\n",
    "#     grouped_by_xyz = filtered_df.groupby(['x', 'y', 'box_no'])\n",
    "\n",
    "#     for (x, y, box_no), group in grouped_by_xyz:\n",
    "#         # 计算 uv、click_uv 和 ctr\n",
    "#         uv = group['uv'].sum()\n",
    "#         click_uv = group['click_uv'].sum()\n",
    "#         ctr = click_uv / uv if uv != 0 else 0  # 避免除以零\n",
    "\n",
    "#         # 将统计结果添加到 summary_data 列表中\n",
    "#         summary_data.append([x, y, box_no, layer_suffix, uv, click_uv, ctr])\n",
    "\n",
    "#         # 创建热力图数据\n",
    "#         heatmap_data = np.zeros((800, 800))\n",
    "\n",
    "#         for _, row in group.iterrows():\n",
    "#             # 计算缩放比例\n",
    "#             scale_factor = 800 / max(800, row['x2'], row['y2'])\n",
    "#             x1 = int(row['x1'] * scale_factor)\n",
    "#             y1 = int(row['y1'] * scale_factor)\n",
    "#             x2 = int(row['x2'] * scale_factor)\n",
    "#             y2 = int(row['y2'] * scale_factor)\n",
    "            \n",
    "#             # 限制在画布范围内\n",
    "#             x1, x2 = max(0, min(x1, 799)), max(0, min(x2, 799))\n",
    "#             y1, y2 = max(0, min(y1, 799)), max(0, min(y2, 799))\n",
    "            \n",
    "#             # 累加区域热度\n",
    "#             heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "#         # 绘制热力图\n",
    "#         plt.figure(figsize=(8, 8))\n",
    "#         sns.heatmap(heatmap_data, \n",
    "#                     cmap='YlOrRd',\n",
    "#                     cbar=False,\n",
    "#                     xticklabels=False,\n",
    "#                     yticklabels=False)\n",
    "        \n",
    "#         plt.title(f'{x}_{y}_{box_no} Heatmap', pad=20, fontsize=14)\n",
    "        \n",
    "#         # 保存热力图\n",
    "#         heatmap_path = os.path.join(output_folder, f'{x}_{y}_boxno_{box_no}_brand_{layer_suffix}_url_3_boxhot.png')\n",
    "#         plt.savefig(heatmap_path, bbox_inches='tight', dpi=100, pad_inches=0)\n",
    "#         plt.close()\n",
    "\n",
    "# # 将统计结果保存为 Excel 文件\n",
    "# summary_df = pd.DataFrame(summary_data, columns=['x', 'y', 'box_no', 'filter_layer_cases', 'uv', 'click_uv', 'ctr'])\n",
    "# summary_file = os.path.join(output_folder, f'{x}_{y}_brand_{layers}_url_3_boxper.xlsx')\n",
    "# summary_df.to_excel(summary_file, index=False)\n",
    "\n",
    "# print(\"所有热力图和统计数据已生成并保存到指定文件夹。\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# # 定义 x_list 和 y_list\n",
    "# x_list = ['9736', '9735', '12004']\n",
    "# y_list = ['txt', 'price']\n",
    "# path = 'Lv2期结论'\n",
    "# z = '男士春夏下装_from_0501'\n",
    "\n",
    "# # 定义过滤层级条件\n",
    "# filter_layer_cases = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]  # 示例分层条件\n",
    "\n",
    "# 读取 Excel 文件\n",
    "df = pd.read_excel(f'D://code//data//{path}//{z}//img_all_classification_results-1.xlsx')\n",
    "\n",
    "# 填充空的“最终分层”列\n",
    "final_layer_map = df.dropna(subset=['最终分层']).set_index('Image Name')['最终分层'].to_dict()\n",
    "df['最终分层'] = df.apply(lambda row: final_layer_map.get(row['Image Name'], row['最终分层']), axis=1)\n",
    "\n",
    "output_folder = f'D://code//data//{path}//{z}//url_3'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 创建一个总汇总表的 DataFrame\n",
    "all_summary_data = []\n",
    "\n",
    "# 遍历 x_list 和 y_list 的每个组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        # 遍历 filter_layer_cases 中的每个子列表\n",
    "        for layers in filter_layer_cases:\n",
    "            summary_data = []  # 每个 layers 的组合重置 summary_data\n",
    "            layer_suffix = \"_\".join(map(str, layers))\n",
    "\n",
    "            # 筛选符合当前 x, y, 和 layers 的数据\n",
    "            filtered_df = df[(df['x'] == int(x)) & (df['y'] == y) & (df['最终分层'].isin(layers))]\n",
    "\n",
    "            grouped_by_xyz = filtered_df.groupby(['x', 'y', 'box_no'])\n",
    "            for (x_val, y_val, box_no), group in grouped_by_xyz:\n",
    "                uv = group['uv'].sum()\n",
    "                click_uv = group['click_uv'].sum()\n",
    "                ctr = click_uv / uv if uv != 0 else 0\n",
    "                count = len(group)\n",
    "                count_pct = count / len(filtered_df) if len(filtered_df) > 0 else 0\n",
    "\n",
    "                summary_data.append([x_val, y_val, box_no, layer_suffix, uv, click_uv, ctr, count, count_pct])\n",
    "\n",
    "                # 创建热力图\n",
    "                heatmap_data = np.zeros((800, 800))\n",
    "                for _, row in group.iterrows():\n",
    "                    scale_factor = 800 / max(800, row['x2'], row['y2'])\n",
    "                    x1 = int(row['x1'] * scale_factor)\n",
    "                    y1 = int(row['y1'] * scale_factor)\n",
    "                    x2 = int(row['x2'] * scale_factor)\n",
    "                    y2 = int(row['y2'] * scale_factor)\n",
    "\n",
    "                    x1, x2 = max(0, min(x1, 799)), max(0, min(x2, 799))\n",
    "                    y1, y2 = max(0, min(y1, 799)), max(0, min(y2, 799))\n",
    "                    heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                sns.heatmap(heatmap_data, cmap='YlOrRd', cbar=False, xticklabels=False, yticklabels=False)\n",
    "                plt.title(f'{x_val}_{y_val}_{box_no} Heatmap - {layer_suffix}', pad=20, fontsize=14)\n",
    "\n",
    "                heatmap_path = os.path.join(output_folder, f'{x_val}_{y_val}_img_boxno_{box_no}_brand_{layer_suffix}_url_3_boxhot.png')\n",
    "                plt.savefig(heatmap_path, bbox_inches='tight', dpi=100, pad_inches=0)\n",
    "                plt.close()\n",
    "                print(f\"Generated heatmap: {heatmap_path}\")\n",
    "\n",
    "            # 将 summary_data 转换为 DataFrame\n",
    "            summary_df = pd.DataFrame(summary_data, columns=['x', 'y', 'box_no', 'brand', 'uv', 'click_uv', 'ctr', 'count', 'count_pct'])\n",
    "            \n",
    "            # 将 summary_df 数据追加到汇总表中\n",
    "            all_summary_data.append(summary_df)\n",
    "\n",
    "            # 绘制饼图\n",
    "            for (x_val, y_val), group in summary_df.groupby(['x', 'y']):\n",
    "                labels = [f'Box {int(row[\"box_no\"])}\\nCount: {row[\"count\"]}\\nCTR: {row[\"ctr\"]:.2%}' for _, row in group.iterrows()]\n",
    "                \n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.pie(group['count'], labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "                plt.title(f'{x_val}_{y_val} Box Distribution - {layer_suffix}', pad=20, fontsize=14)\n",
    "                pie_chart_path = os.path.join(output_folder, f'{x_val}_{y_val}_img_brand_{layer_suffix}_url_3_boxper.png')\n",
    "                plt.savefig(pie_chart_path, bbox_inches='tight', dpi=100)\n",
    "                plt.close()\n",
    "                print(f\"Generated pie chart: {pie_chart_path}\")\n",
    "\n",
    "# 将所有的 summary_df 汇总到一个 DataFrame\n",
    "combined_summary_df = pd.concat(all_summary_data, ignore_index=True)\n",
    "\n",
    "# 保存汇总数据到单一 Excel 文件\n",
    "combined_summary_file = os.path.join(output_folder, 'img_brand_all_url_3_boxper.xlsx')\n",
    "combined_summary_df.to_excel(combined_summary_file, index=False)\n",
    "print(f\"汇总 Excel 文件已保存到: {combined_summary_file}\")\n",
    "\n",
    "print(\"所有热力图、饼图和统计数据已生成并保存到指定文件夹。\")\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n",
    "\n",
    "file1_path = os.path.join(output_folder, 'img_all_url_3_boxper.xlsx')\n",
    "file2_path = os.path.join(output_folder, 'img_brand_all_url_3_boxper.xlsx')\n",
    "\n",
    "# 读取两个Excel文件\n",
    "df1 = pd.read_excel(file1_path)\n",
    "df2 = pd.read_excel(file2_path)\n",
    "\n",
    "# 将两个DataFrame上下拼接\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# 保存合并后的文件\n",
    "combined_file_path = os.path.join(output_folder, 'img_url_3_boxper.xlsx')  # 最终数据\n",
    "combined_df.to_excel(combined_file_path, index=False)\n",
    "\n",
    "print(f\"合并后的Excel文件已保存到: {combined_file_path}\")\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7 - 文本:布局分类&热力图生成\n",
    "### 重新汇总了一个merged_info_ctr-123,并生成了布局分类的热力图,核心是和之前的数据格式不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里重新汇总了一个merged_info_ctr-123\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def merge_excel_files(txt_box_info_file, img_box_info_file, output_file):\n",
    "    # 读取 txt_box_info 文件\n",
    "    txt_df = pd.read_excel(txt_box_info_file)\n",
    "    txt_df = txt_df.rename(columns={'File Name':'Image Name', 'x1': 'txt_x1', 'y1': 'txt_y1', 'x2': 'txt_x2', 'y2': 'txt_y2',})\n",
    "\n",
    "    # 读取 img_box_info 文件\n",
    "    img_df = pd.read_excel(img_box_info_file)\n",
    "\n",
    "    # 重命名列\n",
    "    img_df = img_df.rename(columns={'x1': 'img_x1', 'y1': 'img_y1', 'x3': 'img_x2', 'y3':'img_y2', 'Subfolder':'Style'})\n",
    "    img_df = img_df.loc[:, ['Image Name', 'Style','img_x1', 'img_y1', 'img_x2', 'img_y2', 'box_no', ]]\n",
    "\n",
    "    img_df = img_df.drop_duplicates()\n",
    "    \n",
    "    # 合并两个 DataFrame，使用 txt_box_info 的表头作为准\n",
    "    merged_df = pd.concat([txt_df, img_df], ignore_index=True)\n",
    "    \n",
    "    # 将 img_box_info 中缺少的数据设置为空\n",
    "    merged_df = merged_df.fillna(\"\")\n",
    "\n",
    "    # 使用正则表达式删除 .jpg 或 .png 后缀\n",
    "    merged_df['Image Name'] = merged_df['Image Name'].str.replace(r'\\.(?:jpg|png)$', '', regex=True)\n",
    "    \n",
    "    # 将合并后的 DataFrame 写入新的 Excel 文件\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        merged_df.to_excel(writer, index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    txt_box_info_file = f\"D://code//data//Lv2期结论//{z}//txt_info-1.xlsx\"\n",
    "    img_box_info_file = f\"D://code//data//Lv2期结论//{z}//img_info.xlsx\"\n",
    "    output_file = f\"D://code//data//Lv2期结论//{z}//merged_info-123.xlsx\"\n",
    "    \n",
    "    merge_excel_files(txt_box_info_file, img_box_info_file, output_file)\n",
    "    \n",
    "    print(\"Excel files merged successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 定义路径\n",
    "data_1 = f\"D://code//data//Lv2期结论//{z}//merged_info-123.xlsx\"\n",
    "data_2 = f\"D://code//data//Lv2期结论//{z}//{z}.csv\"\n",
    "# data_3 = 'D://code//data//howtodo_from_0401//服饰鞋靴箱包//品类聚类-服饰鞋靴箱包.csv'\n",
    "output_path = f\"D://code//data//Lv2期结论//{z}//merged_info_ctr-123.xlsx\"\n",
    "\n",
    "# 读取df1\n",
    "df1 = pd.read_excel(data_1)\n",
    "\n",
    "# 读取df2\n",
    "df2 = pd.read_csv(data_2)\n",
    "\n",
    "aggregated_data = df2.groupby('img_url').agg({\n",
    "    'cid2': 'first',  # 使用 'first' 函数来选择分组中的第一个值\n",
    "    'cid3': 'first',\n",
    "    'uv': 'sum',\n",
    "    'click_uv': 'sum',\n",
    "    'gmv_cj':'sum',\n",
    "    'sale_qtty_cj':'sum'\n",
    "#     'folder_path': 'first'  # 同样使用 'first' 函数选择第一个值\n",
    "}).reset_index()  # 重置索引\n",
    "\n",
    "df2 = aggregated_data\n",
    "\n",
    "# 计算ctr字段\n",
    "df2['ctr'] = df2['click_uv'] / df2['uv']\n",
    "\n",
    "\n",
    "def extract_filename(x):\n",
    "    # 分割路径，取倒数第二部分和最后一部分（文件名部分）\n",
    "    parts = x.split('/')\n",
    "    return f\"{parts[-2]}_{os.path.splitext(parts[-1])[0]}\"  # 保留原文件扩展名\n",
    "    # return f\"{os.path.splitext(parts[-1])[0]}.jpg\"  # 保留原文件扩展名\n",
    "\n",
    "# 应用函数\n",
    "df2['only_2'] = df2['img_url'].apply(extract_filename)\n",
    "\n",
    "# 初始化结果列表\n",
    "results = []\n",
    "\n",
    "# 遍历df1的每一行\n",
    "for index, row1 in df1.iterrows():\n",
    "    # 查找df2中匹配的行\n",
    "    matching_rows_df2 = df2[df2['only_2'] == row1['Image Name']]\n",
    "    \n",
    "    # 如果没有找到匹配的行，则只添加df1的当前行\n",
    "    if matching_rows_df2.empty:\n",
    "        results.append(row1.to_dict())\n",
    "    else:\n",
    "        # 对于找到的每个匹配行，先添加df1的当前行，然后添加匹配的df2行\n",
    "        results.append(row1.to_dict())\n",
    "        for _, row2 in matching_rows_df2.iterrows():\n",
    "            # 可能需要添加额外的逻辑来处理多个匹配的情况\n",
    "            # 这里假设每个df1的行在df2中最多只有一个匹配\n",
    "            merged_row = {**row1.to_dict(), **row2.to_dict()}\n",
    "            results.append(merged_row)\n",
    "\n",
    "# 将结果列表转换为DataFrame\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "result_df_drop = result_df.dropna(subset=['uv'])\n",
    "\n",
    "# 保存到指定路径\n",
    "result_df_drop.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Merged file saved to {output_path}\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7.1 - 文本:布局分类\n",
    "### 不包含brand分类, all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心功能: ①从图片中提取文本框,形成线框图; ②将线框图按照九宫格进行分类\n",
    "# 这里是把x和y都采用了list的形式,用来简化人力的输入\n",
    "# 这里是针对整体,没有分structure,也没有分brand\n",
    "# 在这一步拼了一张\"structure-映射表\",用来把人工分类加入进来\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# # x_list = ['6913','6912','6911','6910','6909','6908','12066']  # 添加所有需要的 Subfolder 值\n",
    "# x_list = ['9736','9735','12004']  # 添加所有需要的 Subfolder 值\n",
    "# y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "# path = 'Lv2期结论'\n",
    "\n",
    "\n",
    "# z = '男士春夏下装_from_0501'\n",
    "\n",
    "\n",
    "\n",
    "# 在文件开头定义一个函数来读取原始数据\n",
    "def read_original_data(z):\n",
    "    file_path = os.path.join(f'D://code//data//{path}//{z}//merged_info_ctr-123.xlsx')\n",
    "    # print(f\"Reading data from: {file_path}\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    df['Image Name'] = df['Image Name'] + '.jpg'\n",
    "    # print(f\"Read {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "# 1. 读取和预处理数据\n",
    "def normalize_coordinates(row):\n",
    "    width = 616\n",
    "    height = 616\n",
    "    row['left_norm'] = max(0, min(row['txt_x1'] / width, 1))\n",
    "    row['top_norm'] = max(0, min(row['txt_y1'] / height, 1))\n",
    "    row['right_norm'] = max(0, min(row['txt_x2'] / width, 1))\n",
    "    row['bottom_norm'] = max(0, min(row['txt_y2'] / height, 1))\n",
    "    return row\n",
    "\n",
    "# 2. 绘制矩形和网格\n",
    "def draw_rectangles(group):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        rect = Rectangle((row['left_norm'], 1 - row['bottom_norm']), \n",
    "                         row['right_norm'] - row['left_norm'], \n",
    "                         row['bottom_norm'] - row['top_norm'],\n",
    "                         fill=False, edgecolor='r')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            rect = Rectangle((j/3, 1 - (i+1)/3), 1/3, 1/3, fill=False, edgecolor='b')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    return fig, ax\n",
    "\n",
    "# 3. 判断重叠和分类\n",
    "def check_overlap(rect, grid_cell):\n",
    "    return not (rect[2] < grid_cell[0] or rect[0] > grid_cell[2] or\n",
    "                rect[3] < grid_cell[1] or rect[1] > grid_cell[3])\n",
    "\n",
    "def classify_image(group):\n",
    "    overlaps = [0] * 9\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        rect = (row['left_norm'], row['top_norm'], row['right_norm'], row['bottom_norm'])\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                grid_cell = (j/3, i/3, (j+1)/3, (i+1)/3)\n",
    "                if check_overlap(rect, grid_cell):\n",
    "                    overlaps[i*3 + j] = 1\n",
    "    return ''.join(map(str, overlaps))  # 直接使用join方法生成字符串\n",
    "\n",
    "# 4. 处理单个图像\n",
    "def process_image(name, group, x, y):\n",
    "    try:\n",
    "        classification = classify_image(group)\n",
    "        \n",
    "        # 复制图片\n",
    "        source = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', name)\n",
    "        destination = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classified_images', classification.zfill(9), name)\n",
    "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "        shutil.copy2(source, destination)\n",
    "        \n",
    "        # 保存可视化结果\n",
    "        fig, ax = draw_rectangles(group)\n",
    "        visualization_name = f\"{name.split('.')[0]}_visualization.png\"\n",
    "        fig.savefig(os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_visualizations', visualization_name), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return name, classification\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing image {name}: {str(e)}\")\n",
    "        return name, None\n",
    "\n",
    "# 主处理函数\n",
    "def main(x, y, original_df):\n",
    "    print(f\"Processing for Subfolder={x}, Style={y}...\")\n",
    "    print(\"读取并预处理数据...\")\n",
    "\n",
    "    # 使用原始数据的副本\n",
    "    df = original_df.copy()\n",
    "\n",
    "    # print(f\"Original columns: {df.columns.tolist()}\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "    # 在读取df时添加筛选条件\n",
    "    def filter_by_rectangle(row):\n",
    "        right, bottom = 616 * 0.3, 616 * 0.2\n",
    "        if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # 修改数据筛选逻辑\n",
    "    df = df[(df['img_x1'].isna()) & (df['Subfolder'] == int(x)) & (df['Style'] == y) & (df['Type'] == 'Original')]\n",
    "    df = df.sort_values('ctr', ascending=False)\n",
    "\n",
    "    # 应用左上角的筛选条件\n",
    "    df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "    rows_to_keep = int(len(df) * 0.5)  # 取前50%的样本 ##################################################\n",
    "    df = df.head(rows_to_keep)\n",
    "\n",
    "    # 应用normalize_coordinates函数\n",
    "    df = df.apply(normalize_coordinates, axis=1)\n",
    "\n",
    "    # 检查是否存在所需的列\n",
    "    required_columns = ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Error: Missing columns: {missing_columns}\")\n",
    "        return\n",
    "\n",
    "    # 创建输出目录\n",
    "    classified_images_dir = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', '50%_word_classified_images')\n",
    "    visualizations_dir = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', '50%_word_visualizations')\n",
    "    os.makedirs(classified_images_dir, exist_ok=True)\n",
    "    os.makedirs(visualizations_dir, exist_ok=True)\n",
    "\n",
    "    # 按File name分组并处理\n",
    "    grouped = df.groupby('Image Name')\n",
    "    print(\"处理图像...\")\n",
    "\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_image, name, group, x, y) for name, group in grouped]\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            name, classification = future.result()\n",
    "            results.append((name, classification))\n",
    "    \n",
    "    # 在原Excel文件中新增一列，保存分类结果\n",
    "    classification_df = pd.DataFrame(results, columns=['Image Name', 'Classification'])\n",
    "    df = pd.merge(df, classification_df, on='Image Name', how='left')\n",
    "    df.to_excel(os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification-{x}_{y}.xlsx'), index=False)\n",
    "    \n",
    "    # 读取Excel\n",
    "    df_maping = pd.read_excel(os.path.join(f'D://code//data//{path}//structure-映射表.xlsx'))\n",
    "    df_maping['Classification'] = df_maping['Classification'].astype(str).str.zfill(9)\n",
    "    # 当df_maping的Classification列的值等于df中Classification列的值, 将df_maping的structure列的值添加到df对应行之后\n",
    "    df = pd.merge(df, df_maping, left_on='Classification', right_on='Classification', how='left')\n",
    "    df.to_excel(os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}.xlsx'), index=False)\n",
    "\n",
    "    print(f\"Completed processing for Subfolder={x}, Style={y}\")\n",
    "    print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(\"--------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 在循环开始前读取原始数据\n",
    "    original_df = read_original_data(z)\n",
    "    \n",
    "    for x, y in itertools.product(x_list, y_list):\n",
    "        main(x, y, original_df)\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7.2 - 文本:布局分类\n",
    "### 包含brand分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心功能: ①从图片中提取文本框,形成线框图; ②将线框图按照九宫格进行分类\n",
    "# 这里是把x和y都采用了list的形式,用来简化人力的输入\n",
    "# 这里添加了brand的分类\n",
    "# 在这一步拼了一张\"structure-映射表\",用来把人工分类加入进来\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "\n",
    "# # 定义筛选条件\n",
    "# filter_layer_cases = [[1.0, 2.0, 3.0],[4.0, 5.0, 6.0]]\n",
    "\n",
    "\n",
    "\n",
    "for filter_layers in filter_layer_cases:\n",
    "    # 在文件开头定义一个函数来读取原始数据\n",
    "    def read_original_data(z):\n",
    "        file_path = os.path.join(f'D://code//data//{path}//{z}//merged_info_ctr-123.xlsx')\n",
    "        df = pd.read_excel(file_path)\n",
    "        df['Image Name'] = df['Image Name'] + '.jpg'\n",
    "        return df\n",
    "\n",
    "    # 1. 读取和预处理数据\n",
    "    def normalize_coordinates(row):\n",
    "        width = 616\n",
    "        height = 616\n",
    "        row['left_norm'] = max(0, min(row['txt_x1'] / width, 1))\n",
    "        row['top_norm'] = max(0, min(row['txt_y1'] / height, 1))\n",
    "        row['right_norm'] = max(0, min(row['txt_x2'] / width, 1))\n",
    "        row['bottom_norm'] = max(0, min(row['txt_y2'] / height, 1))\n",
    "        return row\n",
    "\n",
    "    # 2. 绘制矩形和网格\n",
    "    def draw_rectangles(group):\n",
    "        fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            rect = Rectangle((row['left_norm'], 1 - row['bottom_norm']), \n",
    "                            row['right_norm'] - row['left_norm'], \n",
    "                            row['bottom_norm'] - row['top_norm'],\n",
    "                            fill=False, edgecolor='r')\n",
    "            ax.add_patch(rect)\n",
    "        \n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                rect = Rectangle((j/3, 1 - (i+1)/3), 1/3, 1/3, fill=False, edgecolor='b')\n",
    "                ax.add_patch(rect)\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        return fig, ax\n",
    "\n",
    "    # 3. 判断重叠和分类\n",
    "    def check_overlap(rect, grid_cell):\n",
    "        return not (rect[2] < grid_cell[0] or rect[0] > grid_cell[2] or\n",
    "                    rect[3] < grid_cell[1] or rect[1] > grid_cell[3])\n",
    "\n",
    "    def classify_image(group):\n",
    "        overlaps = [0] * 9\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            rect = (row['left_norm'], row['top_norm'], row['right_norm'], row['bottom_norm'])\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    grid_cell = (j/3, i/3, (j+1)/3, (i+1)/3)\n",
    "                    if check_overlap(rect, grid_cell):\n",
    "                        overlaps[i*3 + j] = 1\n",
    "        return ''.join(map(str, overlaps))  # 直接使用join方法生成字符串\n",
    "\n",
    "    # 4. 处理单个图像\n",
    "    def process_image(name, group, x, y, filter_suffix):\n",
    "        try:\n",
    "            classification = classify_image(group)\n",
    "            \n",
    "            # 复制图片\n",
    "            source = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', name)\n",
    "            destination = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classified_images_{filter_suffix}', classification.zfill(9), name)\n",
    "            os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "            shutil.copy2(source, destination)\n",
    "            \n",
    "            # 保存可视化结果\n",
    "            fig, ax = draw_rectangles(group)\n",
    "            visualization_name = f\"{name.split('.')[0]}_visualization.png\"\n",
    "            fig.savefig(os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_visualizations_{filter_suffix}', visualization_name), bbox_inches='tight', pad_inches=0)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            return name, classification\n",
    "        except Exception as e:\n",
    "            return name, None\n",
    "\n",
    "    # 主处理函数\n",
    "    def main(x, y, original_df, filter_layers):\n",
    "        filter_suffix = f\"filter_{'_'.join(map(str, filter_layers))}\"\n",
    "        print(f\"Processing for Subfolder={x}, Style={y}, Filter={filter_suffix}...\")\n",
    "        print(\"读取并预处理数据...\")\n",
    "\n",
    "        # 使用原始数据的副本\n",
    "        df = original_df.copy()\n",
    "\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "        # 在读取df时添加筛选条件\n",
    "        def filter_by_rectangle(row):\n",
    "            right, bottom = 616 * 0.3, 616 * 0.2\n",
    "            if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        # 修改数据筛选逻辑\n",
    "        df = df[(df['img_x1'].isna()) & (df['Subfolder'] == int(x)) & (df['Style'] == y) & (df['Type'] == 'Original') & (df['最终分层'].isin(filter_layers))]\n",
    "        df = df.sort_values('ctr', ascending=False)\n",
    "\n",
    "        # 应用左上角的筛选条件\n",
    "        df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "        rows_to_keep = int(len(df) * 0.5)  # 取前50%的样本 ##################################################\n",
    "        df = df.head(rows_to_keep)\n",
    "\n",
    "        # 应用normalize_coordinates函数\n",
    "        df = df.apply(normalize_coordinates, axis=1)\n",
    "\n",
    "        # 检查是否存在所需的列\n",
    "        required_columns = ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Error: Missing columns: {missing_columns}\")\n",
    "            return\n",
    "\n",
    "        # 创建输出目录\n",
    "        classified_images_dir = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', f'50%_word_classified_images_{filter_suffix}')\n",
    "        visualizations_dir = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', f'50%_word_visualizations_{filter_suffix}')\n",
    "        os.makedirs(classified_images_dir, exist_ok=True)\n",
    "        os.makedirs(visualizations_dir, exist_ok=True)\n",
    "\n",
    "        # 按File name分组并处理\n",
    "        grouped = df.groupby('Image Name')\n",
    "        print(\"处理图像...\")\n",
    "\n",
    "        results = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "            futures = [executor.submit(process_image, name, group, x, y, filter_suffix) for name, group in grouped]\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "                name, classification = future.result()\n",
    "                results.append((name, classification))\n",
    "        \n",
    "        # 在原Excel文件中新增一列，保存分类结果\n",
    "        classification_df = pd.DataFrame(results, columns=['Image Name', 'Classification'])\n",
    "        df = pd.merge(df, classification_df, on='Image Name', how='left')\n",
    "        df.to_excel(os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification-{x}_{y}_{filter_suffix}.xlsx'), index=False)\n",
    "\n",
    "        # 读取Excel\n",
    "        df_maping = pd.read_excel(os.path.join(f'D://code//data//{path}//structure-映射表.xlsx'))\n",
    "        df_maping['Classification'] = df_maping['Classification'].astype(str).str.zfill(9)\n",
    "        # 当df_maping的Classification列的值等于df中Classification列的值, 将df_maping的structure列的值添加到df对应行之后\n",
    "        df = pd.merge(df, df_maping, left_on='Classification', right_on='Classification', how='left')\n",
    "        df.to_excel(os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}_{filter_suffix}.xlsx'), index=False)\n",
    "        \n",
    "        print(f\"Completed processing for Subfolder={x}, Style={y}, Filter={filter_suffix}\")\n",
    "        print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        print(\"--------------------\")\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # 在循环开始前读取原始数据\n",
    "        original_df = read_original_data(z)\n",
    "        \n",
    "        for x, y in itertools.product(x_list, y_list):\n",
    "            main(x, y, original_df, filter_layers)\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7.3 - 文本:分类后的热力图生成\n",
    "### 没有按照structure进行分类, 是将classification全量进行输出\n",
    "### 没有针对brand进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 针对分类结果,绘制文本框热力图,但是通过x和y的list来实现\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# 创建保存结果的文件夹\n",
    "def create_output_folder(x, y):\n",
    "    output_folder = os.path.join(f'D://code//data//{path}//{z}')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 创建 url_2 子文件夹\n",
    "    url_2_folder = os.path.join(output_folder, 'url_2')\n",
    "    os.makedirs(url_2_folder, exist_ok=True)\n",
    "    return output_folder\n",
    "\n",
    "\n",
    "# 读取Excel文件\n",
    "def read_excel_file(x, y):\n",
    "    file_path = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', f'50%_word_classification-{x}_{y}.xlsx')\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "# 绘制热力图\n",
    "def plot_heatmaps(df, output_folder):\n",
    "    # 按Classification列进行分组\n",
    "    grouped = df.groupby('Classification')\n",
    "\n",
    "    # 遍历每个分组\n",
    "    for name, group in grouped:\n",
    "        # 创建一个新的图形\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # 绘制矩形框\n",
    "        ax1.set_xlim(0, 616)\n",
    "        ax1.set_ylim(616, 0)\n",
    "        for _, row in group.iterrows():\n",
    "            x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "            # 排除左上角的文本框\n",
    "            if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "                continue\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            rect = plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='r')\n",
    "            ax1.add_patch(rect)\n",
    "        ax1.set_title(f'Bounding Boxes for {name}')\n",
    "        ax1.set_xlabel('X')\n",
    "        ax1.set_ylabel('Y')\n",
    "\n",
    "        # 创建热力图\n",
    "        heatmap = np.zeros((616, 616))\n",
    "        for _, row in group.iterrows():\n",
    "            x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "            # 排除左上角的文本框\n",
    "            if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "                continue\n",
    "            x1, y1 = max(0, min(x1, 615)), max(0, min(y1, 615))\n",
    "            x2, y2 = max(0, min(x2, 615)), max(0, min(y2, 615))\n",
    "            heatmap[int(y1):int(y2)+1, int(x1):int(x2)+1] += 1\n",
    "\n",
    "        # 绘制热力图\n",
    "        sns.heatmap(heatmap, ax=ax2, cmap='YlOrRd', cbar=True)\n",
    "        ax2.set_title(f'Heatmap for {name}')\n",
    "        ax2.set_xlabel('X')\n",
    "        ax2.set_ylabel('Y')\n",
    "\n",
    "        # 调整子图之间的间距\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 保存图像\n",
    "        plt.savefig(os.path.join(output_folder, f'url_2//{x}_{y}_word_classification_{name}_brand_all_url_2_hotlayout.png'), dpi=100, bbox_inches='tight')\n",
    "        plt.close()  # 关闭图形，释放内存\n",
    "\n",
    "        print(f\"已保存 {name} 的热力图\")\n",
    "\n",
    "    print(\"所有热力图已保存在 output_heatmaps 文件夹中\")\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    for x in x_list:\n",
    "        for y in y_list:\n",
    "            print(f\"Processing for x={x}, y={y}...\")\n",
    "            output_folder = create_output_folder(x, y)\n",
    "            file_path = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', f'50%_word_classification-{x}_{y}.xlsx')\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"No data found for x={x}, y={y}. Skipping...\")\n",
    "                continue\n",
    "            df = read_excel_file(x, y)\n",
    "            plot_heatmaps(df, output_folder)\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7.3 - 文本:分类后的热力图生成\n",
    "### 按照structure统计的最大值来绘制热力图\n",
    "### 没有针对brand进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照structure统计的最大值来绘制热力图\n",
    "# 没有针对brand进行分类\n",
    "# 通过x和y的list来实现\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# 定义左上角排除区域的边界常量\n",
    "EXCLUDE_LEFT_TOP_X_THRESHOLD = 616 * 0.3\n",
    "EXCLUDE_LEFT_TOP_Y_THRESHOLD = 616 * 0.2\n",
    "\n",
    "# 创建保存结果的文件夹，包括创建 url_2 子文件夹\n",
    "def create_output_folder(x, y):\n",
    "    output_folder = os.path.join(f'D://code//data//{path}//{z}')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 创建 url_2 子文件夹\n",
    "    url_2_folder = os.path.join(output_folder, 'url_2')\n",
    "    os.makedirs(url_2_folder, exist_ok=True)\n",
    "    return output_folder\n",
    "\n",
    "# 读取Excel文件并处理，返回特定 structure 值的数据\n",
    "def read_and_process_excel_file_for_structure(x, y):\n",
    "    file_path = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', f'50%_word_classification_structure-{x}_{y}.xlsx')\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    df = pd.read_excel(file_path)\n",
    "    df_unique = df.drop_duplicates(subset='Image Name')\n",
    "    structure_counts = df_unique['structure'].value_counts()\n",
    "    most_common_structure = structure_counts.index[0] if len(structure_counts) > 0 else None\n",
    "    return df_unique[df_unique['structure'] == most_common_structure] if most_common_structure else None\n",
    "\n",
    "# 绘制热力图\n",
    "def plot_heatmap(df, output_folder, most_common_structure, brand='all'):\n",
    "    if df is None:\n",
    "        return\n",
    "    # 获取数量\n",
    "    if most_common_structure is not None:\n",
    "        count = df['structure'].value_counts().get(most_common_structure, 0)\n",
    "    else:\n",
    "        count = 0\n",
    "    print(f\"Most common structure: {most_common_structure}, Count: {count}\")\n",
    "    # 筛选出特定 structure 值的数据\n",
    "    filtered_df = df[df['structure'] == most_common_structure]\n",
    "    # 创建一个新的图形\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    # 绘制矩形框\n",
    "    ax1.set_xlim(0, 616)\n",
    "    ax1.set_ylim(616, 0)\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "        # 排除左上角的文本框\n",
    "        if x1 <= EXCLUDE_LEFT_TOP_X_THRESHOLD and y1 <= EXCLUDE_LEFT_TOP_Y_THRESHOLD:\n",
    "            continue\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        rect = plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='r')\n",
    "        ax1.add_patch(rect)\n",
    "    ax1.set_title(f'Bounding Boxes for {most_common_structure}')\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "\n",
    "    # 创建热力图\n",
    "    heatmap = np.zeros((616, 616))\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "        # 排除左上角的文本框\n",
    "        if x1 <= EXCLUDE_LEFT_TOP_X_THRESHOLD and y1 <= EXCLUDE_LEFT_TOP_Y_THRESHOLD:\n",
    "            continue\n",
    "        x1, y1 = max(0, min(x1, 615)), max(0, min(y1, 615))\n",
    "        x2, y2 = max(0, min(x2, 615)), max(0, min(y2, 615))\n",
    "        heatmap[int(y1):int(y2)+1, int(x1):int(x2)+1] += 1\n",
    "\n",
    "    # 绘制热力图\n",
    "    sns.heatmap(heatmap, ax=ax2, cmap='YlOrRd', cbar=True)\n",
    "    ax2.set_title(f'Heatmap for {most_common_structure}')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "\n",
    "    # 调整子图之间的间距\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存图像\n",
    "    plt.savefig(os.path.join(output_folder, f'url_2//{x}_{y}_word_structure_{most_common_structure}_brand_{brand}_url_2_hotlayout.png'), dpi=100, bbox_inches='tight')\n",
    "    plt.close()  # 关闭图形，释放内存\n",
    "\n",
    "    print(f\"已保存热力图 for {most_common_structure}\")\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    for x in x_list:\n",
    "        for y in y_list:\n",
    "            print(f\"Processing for x={x}, y={y}...\")\n",
    "            output_folder = create_output_folder(x, y)\n",
    "            df = read_and_process_excel_file_for_structure(x, y)\n",
    "            if df is not None:\n",
    "                structure_counts = df['structure'].value_counts()\n",
    "                most_common_structure = structure_counts.index[0] if len(structure_counts) > 0 else None\n",
    "                plot_heatmap(df, output_folder, most_common_structure)\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7.3 - 文本:分类后的热力图生成\n",
    "### 按照structure统计的最大值来绘制热力图\n",
    "### 按照brand进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照structure统计的最大值来绘制热力图\n",
    "# 针对brand进行分类\n",
    "# 通过x和y的list来实现\n",
    "## 这里有可能因为brand分类太少的原因, 造成structure没有值, 所以设计了一个跳过的流程\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# filter_suffix_list = ['filter_1.0_2.0_3.0', 'filter_4.0_5.0_6.0']\n",
    "\n",
    "def create_output_folder(x, y, filter_suffix):\n",
    "    output_folder = os.path.join(f'D://code//data//{path}//{z}')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    return output_folder\n",
    "\n",
    "def read_excel_file(x, y, filter_suffix):\n",
    "    file_path = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}',\n",
    "                             f'50%_word_classification_structure-{x}_{y}_{filter_suffix}.xlsx')\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # 如果 'structure' 列不存在或为空，则跳过\n",
    "    if 'structure' not in df.columns or df['structure'].isnull().all():\n",
    "        print(f\"'structure' column is missing or empty in file: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    # 根据Image Name去重\n",
    "    df_unique = df.drop_duplicates(subset='Image Name')\n",
    "\n",
    "    # 统计structure列各个值的数量\n",
    "    structure_counts = df_unique['structure'].value_counts()\n",
    "    most_common_structure = structure_counts.index[0] if len(structure_counts) > 0 else None\n",
    "\n",
    "    if most_common_structure is None:\n",
    "        print(f\"No valid structure found in file: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Most common structure: {most_common_structure}, Total count: {structure_counts.get(most_common_structure, 0)}\")\n",
    "    return df_unique[df_unique['structure'] == most_common_structure]\n",
    "\n",
    "def create_empty_file(file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    pd.DataFrame().to_excel(file_path, index=False)\n",
    "    print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "def plot_heatmaps(df, output_folder, filter_suffix, most_common_structure):\n",
    "    if df.empty:\n",
    "        print(f\"Skipping empty DataFrame for {filter_suffix}\")\n",
    "        return\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    ax1.set_xlim(0, 616)\n",
    "    ax1.set_ylim(616, 0)\n",
    "    for _, row in df.iterrows():\n",
    "        x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "        if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "            continue\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        rect = plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='r')\n",
    "        ax1.add_patch(rect)\n",
    "    ax1.set_title(f'Bounding Boxes ({filter_suffix})')\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "\n",
    "    heatmap = np.zeros((616, 616))\n",
    "    for _, row in df.iterrows():\n",
    "        x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "        if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "            continue\n",
    "        x1, y1 = max(0, min(x1, 615)), max(0, min(y1, 615))\n",
    "        x2, y2 = max(0, min(x2, 615)), max(0, min(y2, 615))\n",
    "        heatmap[int(y1):int(y2) + 1, int(x1):int(x2) + 1] += 1\n",
    "\n",
    "    sns.heatmap(heatmap, ax=ax2, cmap='YlOrRd', cbar=True)\n",
    "    ax2.set_title(f'Heatmap ({filter_suffix})')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, f'url_2//{x}_{y}_word_structure_{most_common_structure}_brand_{filter_suffix}_url_2_hotlayout.png'), dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"已保存热力图 ({filter_suffix})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for x in x_list:\n",
    "        for y in y_list:\n",
    "            for filter_suffix in filter_suffix_list:\n",
    "                print(f\"Processing for x={x}, y={y}, filter_suffix={filter_suffix}\")\n",
    "                output_folder = create_output_folder(x, y, filter_suffix)\n",
    "                df = read_excel_file(x, y, filter_suffix)\n",
    "\n",
    "                if df is None:\n",
    "                    print(f\"File not found or 'structure' column missing in: {os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', f'50%_word_classification_structure-{x}_{y}_{filter_suffix}.xlsx')}\")\n",
    "                    create_empty_file(os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', f'50%_word_classification_structure-{x}_{y}_{filter_suffix}.xlsx'))\n",
    "                    continue\n",
    "\n",
    "                # 筛选出最终分层值在filter_values中的记录\n",
    "                filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "                df = df[df['最终分层'].isin(filter_values)]\n",
    "\n",
    "                # 获取最常见的结构\n",
    "                most_common_structure = df['structure'].value_counts().idxmax() if len(df) > 0 else None\n",
    "\n",
    "                if most_common_structure is not None:\n",
    "                    plot_heatmaps(df, output_folder, filter_suffix, most_common_structure)\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7.4 - 文本:分类后的热力图生成\n",
    "### 按照structure统计的最大值来绘制热力图\n",
    "### 根据文本大小绘制热力图\n",
    "### 没有对brand进行分层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旧版不, 可能会遇到structure为空的报错情况\n",
    "\n",
    "# # 先针对structure进行统计rank，然后再进行过滤\n",
    "# # 根据文本大小绘制热力图\n",
    "# # 没有对brand进行分层\n",
    "# # \n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import datetime\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# for item_x in x_list:\n",
    "#     for item_y in y_list:\n",
    "#         # 定义文件路径\n",
    "#         file_path = os.path.join(f'D://code//data//{path}//{z}//{item_x}//grounding_output//{item_y}', f'50%_word_classification_structure-{item_x}_{item_y}_{filter_suffix}.xlsx')\n",
    "#         if not os.path.exists(file_path):\n",
    "#             continue\n",
    "\n",
    "#         # # 如果 'structure' 列不存在或为空，则跳过\n",
    "#         # if 'structure' not in df.columns or df['structure'].isnull().all():\n",
    "#         #     print(f\"'structure' column is missing or empty in file: {file_path}\")\n",
    "#         #     continue\n",
    "\n",
    "#         df = pd.read_excel(file_path)\n",
    "\n",
    "#         if 'structure' in df_unique.columns:\n",
    "#             structure_counts = df_unique['structure'].value_counts()\n",
    "#         else:\n",
    "#             print(\"The 'structure' column does not exist in the DataFrame. Please check the data source and processing steps.\")\n",
    "\n",
    "#         df_unique = df.drop_duplicates(subset='Image Name')\n",
    "\n",
    "#         # 统计 structure 列各个值的数量\n",
    "#         structure_counts = df_unique['structure'].value_counts()\n",
    "#         if len(structure_counts) > 0:\n",
    "#             most_common_structure, max_count = structure_counts.index[0], structure_counts.iloc[0]\n",
    "#             print(f\"Most common structure: {most_common_structure}, Total count: {max_count}\")\n",
    "#             new_df = df_unique[df_unique['structure'] == most_common_structure]\n",
    "#         else:\n",
    "#             new_df = pd.DataFrame()\n",
    "\n",
    "#         # 应用过滤\n",
    "#         new_df = new_df[new_df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "#         # 替换 Height_Category 列的值\n",
    "#         new_df['Height_Category'] = new_df['Height_Category'].replace({\n",
    "#             'Height_>38': 'xl',\n",
    "#             'Height_18-29': 'm',\n",
    "#             'Height_29-38': 'l',\n",
    "#             'Height_<18': 's'\n",
    "#         })\n",
    "\n",
    "#         # 删除空值\n",
    "#         new_df = new_df.dropna(subset=['Height_Category', 'structure'])\n",
    "\n",
    "#         # 确保必要的列存在\n",
    "#         required_columns = ['Height_Category', 'txt_x1', 'txt_y1', 'txt_x2', 'txt_y2']\n",
    "#         if not all(col in new_df.columns for col in required_columns):\n",
    "#             raise ValueError(f\"DataFrame must contain all of these columns: {required_columns}\")\n",
    "\n",
    "#         # 定义输出路径\n",
    "#         output_dir = f\"D://code//data//{path}//{z}\"\n",
    "#         url_4_path = os.path.join(output_dir, 'url_4')\n",
    "#         os.makedirs(url_4_path, exist_ok=True)\n",
    "\n",
    "#         # 分组并绘制热力图\n",
    "#         grouped = new_df.groupby(['Height_Category'])\n",
    "#         for box_no, group in grouped:\n",
    "#             heatmap_data = np.zeros((616, 616))\n",
    "\n",
    "#             # 更新矩形框的热度值\n",
    "#             for _, row in group.iterrows():\n",
    "#                 x1, y1, x2, y2 = map(int, [row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']])\n",
    "#                 x1, x2 = max(0, min(x1, 616)), max(0, min(x2, 616))\n",
    "#                 y1, y2 = max(0, min(y1, 616)), max(0, min(y2, 616))\n",
    "#                 heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "#             # 创建热力图\n",
    "#             plt.figure(figsize=(10, 10))\n",
    "#             sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Frequency'}, square=True)\n",
    "#             plt.xlabel('X coordinate')\n",
    "#             plt.ylabel('Y coordinate')\n",
    "#             plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "#             # 保存图像\n",
    "#             output_path = os.path.join(url_4_path, f\"{item_x}_{item_y}_word_structure_{most_common_structure}_brand_all_{box_no}_wordsize_url_4_hotlayout.png\".replace(\"('\", \"\").replace(\"',)\", \"\"))\n",
    "#             plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "#             plt.close()\n",
    "\n",
    "#             print(f\"Heatmap saved to: {output_path}\")\n",
    "\n",
    "# # 显示时间\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新版本, 针对structure为空的情况进行处理, 不理会报错, 直接跳过\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "for item_x in x_list:\n",
    "    for item_y in y_list:\n",
    "        # 定义文件路径\n",
    "        file_path = os.path.join(f'D://code//data//{path}//{z}//{item_x}//grounding_output//{item_y}', \n",
    "                                 f'50%_word_classification_structure-{item_x}_{item_y}_{filter_suffix}.xlsx')\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        # 判断'structure'列是否存在于df中\n",
    "        if 'structure' in df.columns:\n",
    "            df_unique = df.drop_duplicates(subset='Image Name')\n",
    "\n",
    "            # 统计structure列各个值的数量\n",
    "            structure_counts = df_unique['structure'].value_counts()\n",
    "            if len(structure_counts) > 0:\n",
    "                most_common_structure, max_count = structure_counts.index[0], structure_counts.iloc[0]\n",
    "                print(f\"Most common structure: {most_common_structure}, Total count: {max_count}\")\n",
    "                new_df = df_unique[df_unique['structure'] == most_common_structure]\n",
    "            else:\n",
    "                new_df = pd.DataFrame()\n",
    "        else:\n",
    "            print(\"The 'structure' column does not exist in the DataFrame. Please check the data source and processing steps.\")\n",
    "            new_df = pd.DataFrame()\n",
    "\n",
    "        # 判断'Height_Category'列是否存在于new_df中\n",
    "        if 'Height_Category' in new_df.columns:\n",
    "            # 应用过滤\n",
    "            new_df = new_df[new_df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "            # 替换Height_Category列的值\n",
    "            new_df['Height_Category'] = new_df['Height_Category'].replace({\n",
    "                'Height_>38': 'xl',\n",
    "                'Height_18-29': 'm',\n",
    "                'Height_29-38': 'l',\n",
    "                'Height_<18': 's'\n",
    "            })\n",
    "\n",
    "            # 删除空值\n",
    "            new_df = new_df.dropna(subset=['Height_Category', 'structure'])\n",
    "\n",
    "            # 确保必要的列存在\n",
    "            required_columns = ['Height_Category', 'txt_x1', 'txt_y1', 'txt_x2', 'txt_y2']\n",
    "            if not all(col in new_df.columns for col in required_columns):\n",
    "                raise ValueError(f\"DataFrame must contain all of these columns: {required_columns}\")\n",
    "\n",
    "            # 定义输出路径\n",
    "            output_dir = f\"D://code//data//{path}//{z}\"\n",
    "            url_4_path = os.path.join(output_dir, 'url_4')\n",
    "            os.makedirs(url_4_path, exist_ok=True)\n",
    "\n",
    "            # 分组并绘制热力图\n",
    "            grouped = new_df.groupby(['Height_Category'])\n",
    "            for box_no, group in grouped:\n",
    "                heatmap_data = np.zeros((616, 616))\n",
    "\n",
    "                # 更新矩形框的热度值\n",
    "                for _, row in group.iterrows():\n",
    "                    x1, y1, x2, y2 = map(int, [row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']])\n",
    "                    x1, x2 = max(0, min(x1, 616)), max(0, min(x2, 616))\n",
    "                    y1, y2 = max(0, min(y1, 616)), max(0, min(y2, 616))\n",
    "                    heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "                # 创建热力图\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Frequency'}, square=True)\n",
    "                plt.xlabel('X coordinate')\n",
    "                plt.ylabel('Y coordinate')\n",
    "                plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "                # 保存图像\n",
    "                output_path = os.path.join(url_4_path, f\"{item_x}_{item_y}_word_structure_{most_common_structure}_brand_all_{box_no}_wordsize_url_4_hotlayout.png\".replace(\"('\", \"\").replace(\"',)\", \"\"))\n",
    "                plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"Heatmap saved to: {output_path}\")\n",
    "        else:\n",
    "            print(\"The 'Height_Category' column does not exist in the DataFrame. Please check the data source and processing steps.\")\n",
    "\n",
    "# 显示时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7.5 - 文本:分类后的热力图生成\n",
    "### 按照structure统计的最大值来绘制热力图\n",
    "### 根据文本大小绘制热力图\n",
    "### 根据brand进行分层统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先针对structure进行统计rank，然后再进行过滤\n",
    "# 根据文本大小绘制热力图\n",
    "# 根据brand进行分层统计\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# filter_suffix_list = ['filter_1.0_2.0', 'filter_3.0_4.0', 'filter_5.0_6.0']\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def create_empty_file(file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    pd.DataFrame().to_excel(file_path, index=False)\n",
    "    print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "def process_data(item_x, item_y, filter_suffix):\n",
    "    file_path = os.path.join(f'D://code//data//{path}//{z}//{item_x}//grounding_output//{item_y}', f'50%_word_classification_structure-{item_x}_{item_y}_{filter_suffix}.xlsx')\n",
    "    if not os.path.exists(file_path):\n",
    "        return pd.DataFrame(), None  # 返回空 DataFrame 和 None\n",
    "\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    df_unique = df.drop_duplicates(subset='Image Name')\n",
    "    if 'structure' in df_unique.columns:\n",
    "        structure_counts = df_unique['structure'].value_counts()\n",
    "    else:\n",
    "        print(\"The 'structure' column does not exist in the DataFrame. Please check the data source and processing steps.\")\n",
    "        structure_counts = pd.Series()  # 设置为一个空的Series对象作为默认值\n",
    "    \n",
    "    if len(structure_counts) > 0:\n",
    "        most_common_structure = structure_counts.index[0]\n",
    "        print(f\"Most common structure: {most_common_structure}, Total count: {structure_counts.iloc[0]}\")\n",
    "        new_df = df_unique[df_unique['structure'] == most_common_structure]\n",
    "    else:\n",
    "        return pd.DataFrame(), None  # 返回空 DataFrame 和 None\n",
    "\n",
    "    required_columns = ['Height_Category', 'txt_x1', 'txt_y1', 'txt_x2', 'txt_y2', '最终分层']\n",
    "    if not all(col in new_df.columns for col in required_columns):\n",
    "        print(f\"Missing required columns in {file_path}\")\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "    new_df = new_df[new_df.apply(filter_by_rectangle, axis=1)]\n",
    "    new_df['Height_Category'] = new_df['Height_Category'].replace({\n",
    "        'Height_>38': 'xl',\n",
    "        'Height_18-29': 'm',\n",
    "        'Height_29-38': 'l',\n",
    "        'Height_<18': 's'\n",
    "    })\n",
    "    new_df = new_df.dropna(subset=['Height_Category'])\n",
    "    new_df = new_df.dropna(subset=['structure'])\n",
    "\n",
    "    filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "    new_df = new_df[new_df['最终分层'].isin(filter_values)]\n",
    "\n",
    "    return new_df, most_common_structure\n",
    "\n",
    "def generate_heatmap(group, output_path):\n",
    "    if group.empty:\n",
    "        print(f\"Skipping empty group for {output_path}\")\n",
    "        return\n",
    "\n",
    "    heatmap_data = np.zeros((616, 616))\n",
    "    for _, row in group.iterrows():\n",
    "        x1, y1, x2, y2 = map(int, [row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']])\n",
    "        x1, x2 = max(0, min(x1, 616)), max(0, min(x2, 616))\n",
    "        y1, y2 = max(0, min(y1, 616)), max(0, min(y2, 616))\n",
    "        heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Frequency'}, square=True)\n",
    "    plt.xlabel('X coordinate')\n",
    "    plt.ylabel('Y coordinate')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Heatmap saved to: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    for item_x in x_list:\n",
    "        for item_y in y_list:\n",
    "            for filter_suffix in filter_suffix_list:\n",
    "                print(f\"Processing: {item_x} - {item_y} - {filter_suffix}\")\n",
    "                df, most_common_structure = process_data(item_x, item_y, filter_suffix)\n",
    "\n",
    "                if df.empty or most_common_structure is None:\n",
    "                    print(f\"Skipping empty DataFrame for {item_x} - {item_y} - {filter_suffix}\")\n",
    "                    continue\n",
    "\n",
    "                output_dir = f\"D://code//data//{path}//{z}//{item_x}//grounding_output//{item_y}//url_4\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                grouped = df.groupby(['Height_Category'])\n",
    "                for box_no, group in grouped:\n",
    "                    output_path = os.path.join(output_dir, f\"{item_x}_{item_y}_word_structure_{most_common_structure}_brand_{filter_suffix}_{box_no}_wordsize_url_4_hotlayout.png\".replace(\"('\", \"\").replace(\"',)\", \"\"))\n",
    "                    generate_heatmap(group, output_path)\n",
    "\n",
    "    print(\"All heatmaps have been generated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step8.1 - 利用chat进行文本内容统计\n",
    "### 这里是用旧版prompt,通过list读取\n",
    "### 全量的structure\n",
    "### 全量的brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "# def summarize_with_gpt4(text):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "#             {\"role\": \"user\", \"content\": \"\"\"\n",
    "#                 # Role \n",
    "#                     角色: 电商数据分析师。\n",
    "#                 # Profile \n",
    "#                     简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "#                 ## Background \n",
    "#                     背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "#                 ## Goals \n",
    "#                     目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "#                 ## Constrains \n",
    "#                     约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "#                 ## Tone \n",
    "#                     语气风格: 正式的，客观的，科学的。\n",
    "#                 ## Skills \n",
    "#                     技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "#                 ## OutputFormat \n",
    "#                     输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "# # 遍历 x 和 y 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "#         # 读取Excel文件\n",
    "#         file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification-{x}_{y}.xlsx'\n",
    "#         if not os.path.exists(file_path):\n",
    "#             print(f\"File not found: {file_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         df = pd.read_excel(file_path)\n",
    "#         # df = df.dropna(subset=['structure'])\n",
    "\n",
    "#         # 确保 'text' 列中的所有值都是字符串\n",
    "#         df['text'] = df['text'].astype(str)\n",
    "        \n",
    "#         df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "#         df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "#         # 遍历每个分组，合并文本并进行总结\n",
    "#         summaries = []\n",
    "        \n",
    "#         # 遍历每个分组\n",
    "#         for (height_category), group in tqdm(df_grouped):\n",
    "#             # 合并该组的所有文本\n",
    "#             all_text = \" \".join(group['text'].dropna())\n",
    "#             # print(f\"Structure: {structure}\")\n",
    "#             # print(f\"Height Category: {height_category}\")\n",
    "#             # print(f\"Text: {all_text[:100]}...\")  # 只打印前100个字符\n",
    "            \n",
    "#             # 使用 GPT-4 进行总结\n",
    "#             try:\n",
    "#                 summary = summarize_with_gpt4(all_text)\n",
    "#                 # print(f\"Summary: {summary[:100]}...\")  # 只打印前100个字符\n",
    "#             except Exception as e:\n",
    "#                 # print(f\"Error in summarization: {str(e)}\")\n",
    "#                 summary = \"Error in summarization\"\n",
    "            \n",
    "#             # 将结果添加到列表中\n",
    "#             summaries.append({\n",
    "#                 # 'structure': structure,\n",
    "#                 'Height_Category': height_category,\n",
    "#                 'text': all_text,\n",
    "#                 'summary': summary\n",
    "#             })\n",
    "        \n",
    "#         # 创建一个新的DataFrame来存储结果\n",
    "#         result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "#         # 保存结果到Excel文件\n",
    "#         output_file = f\"D://code//data//Lv2期结论//{z}//{x}//url_4//{x}_{y}_word_structure_all_brand_all_wordsummary.xlsx\"\n",
    "#         result_df.to_excel(output_file, index=False)\n",
    "#         print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# print(\"All processing completed.\")\n",
    "\n",
    "\n",
    "# import datetime\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    return row['txt_x2'] >= right or row['txt_y2'] >= bottom\n",
    "\n",
    "# 使用GPT-4模型对文本进行自动摘要\n",
    "def summarize_with_gpt4(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "                # Role \n",
    "                    角色: 电商数据分析师。\n",
    "                # Profile \n",
    "                    简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "                ## Background \n",
    "                    背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "                ## Goals \n",
    "                    目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "                ## Constrains \n",
    "                    约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "                ## Tone \n",
    "                    语气风格: 正式的，客观的，科学的。\n",
    "                ## Skills \n",
    "                    技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "                ## OutputFormat \n",
    "                    输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# 创建一个空的总 DataFrame\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# 遍历 x 和 y 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification-{x}_{y}.xlsx'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_excel(file_path)\n",
    "        df['text'] = df['text'].astype(str)  # 确保 'text' 列为字符串\n",
    "        df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "        df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "        # 遍历每个分组，合并文本并进行总结\n",
    "        summaries = []\n",
    "        for height_category, group in tqdm(df_grouped):\n",
    "            all_text = \" \".join(group['text'].dropna())\n",
    "            try:\n",
    "                summary = summarize_with_gpt4(all_text)\n",
    "            except Exception as e:\n",
    "                summary = \"Error in summarization\"\n",
    "            \n",
    "            # 将结果添加到列表中\n",
    "            summaries.append({\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'Height_Category': height_category,\n",
    "                'text': all_text,\n",
    "                'summary': summary,\n",
    "                'style': \"old\",\n",
    "                'structure': \"all\",\n",
    "                'brand': \"all\"\n",
    "            })\n",
    "        \n",
    "        # 创建一个 DataFrame 来存储当前组合的结果\n",
    "        result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "        # 重新排序列\n",
    "        result_df = result_df[['x', 'y', 'Height_Category', 'text', 'summary', 'style', 'structure', 'brand']]\n",
    "        \n",
    "        # 将当前的结果 DataFrame 添加到总的 DataFrame 中\n",
    "        all_data = pd.concat([all_data, result_df], ignore_index=True)\n",
    "\n",
    "# 保存所有数据到一个汇总的Excel文件中\n",
    "output_file = f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_all_brand_all_summary-old.xlsx\"\n",
    "all_data.to_excel(output_file, index=False)\n",
    "print(f\"All data saved to: {output_file}\")\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step8.2 - 利用chat进行文本内容统计\n",
    "### 这里是用旧版prompt,通过list读取\n",
    "### 先针对structure进行统计rank，然后再进行过滤\n",
    "### 没有对brand进行分层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# '''\n",
    "# 这里是通过读取list形式, 来简化输入的\n",
    "# '''\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "# def summarize_with_gpt4(text):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "#             {\"role\": \"user\", \"content\": \"\"\"\n",
    "#                 # Role \n",
    "#                     角色: 电商数据分析师。\n",
    "#                 # Profile \n",
    "#                     简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "#                 ## Background \n",
    "#                     背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "#                 ## Goals \n",
    "#                     目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "#                 ## Constrains \n",
    "#                     约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "#                 ## Tone \n",
    "#                     语气风格: 正式的，客观的，科学的。\n",
    "#                 ## Skills \n",
    "#                     技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "#                 ## OutputFormat \n",
    "#                     输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "# # 遍历 x 和 y 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "#         # 读取Excel文件\n",
    "#         file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}.xlsx'\n",
    "       \n",
    "#         df = pd.read_excel(file_path)\n",
    "#         # 根据 Image Name 去重\n",
    "#         df_unique = df.drop_duplicates(subset='Image Name')\n",
    "#         # 统计 structure 列各个值的数量\n",
    "#         structure_counts = df_unique['structure'].value_counts()\n",
    "#         if len(structure_counts) > 0:\n",
    "#             most_common_structure, max_count = structure_counts.index[0], structure_counts.iloc[0]\n",
    "#             print(f\"Most common structure: {most_common_structure}, Total count: {max_count}\")\n",
    "#             new_df = df_unique[df_unique['structure'] == most_common_structure]\n",
    "#         else:\n",
    "#             new_df = pd.DataFrame()\n",
    "        \n",
    "#         # 确保 'text' 列中的所有值都是字符串\n",
    "#         new_df.loc[:, 'text'] = new_df['text'].astype(str)\n",
    "        \n",
    "#         new_df = new_df[new_df.apply(filter_by_rectangle, axis=1)]\n",
    "#         # new_df = new_df[['Height_Category']]\n",
    "#         df_grouped = new_df.groupby(['Height_Category'])\n",
    "        \n",
    "#         # 遍历每个分组，合并文本并进行总结\n",
    "#         summaries = []\n",
    "        \n",
    "#         # 遍历每个分组\n",
    "#         for (height_category), group in tqdm(df_grouped):\n",
    "#             # 合并该组的所有文本\n",
    "#             all_text = \" \".join(group['text'].dropna())\n",
    "#             # print(f\"Structure: {structure}\")\n",
    "#             # print(f\"Height Category: {height_category}\")\n",
    "#             # print(f\"Text: {all_text[:100]}...\")  # 只打印前100个字符\n",
    "            \n",
    "#             # 使用 GPT-4 进行总结\n",
    "#             try:\n",
    "#                 summary = summarize_with_gpt4(all_text)\n",
    "#                 # print(f\"Summary: {summary[:100]}...\")  # 只打印前100个字符\n",
    "#             except Exception as e:\n",
    "#                 # print(f\"Error in summarization: {str(e)}\")\n",
    "#                 summary = \"Error in summarization\"\n",
    "            \n",
    "#             # 将结果添加到列表中\n",
    "#             summaries.append({\n",
    "#                 # 'structure': structure,\n",
    "#                 'Height_Category': height_category,\n",
    "#                 'text': all_text,\n",
    "#                 'summary': summary\n",
    "#             })\n",
    "        \n",
    "#         # 创建一个新的DataFrame来存储结果\n",
    "#         result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "#         # 保存结果到Excel文件\n",
    "#         output_file = f\"D://code//data//Lv2期结论//{z}//{x}//url_4//{x}_{y}_word_structure_{most_common_structure}_brand_all_wordsummary.xlsx\"\n",
    "\n",
    "#         result_df.to_excel(output_file, index=False)\n",
    "#         print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# print(\"All processing completed.\")\n",
    "\n",
    "\n",
    "# import datetime\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# # 定义 x_list 和 y_list\n",
    "# x_list = ['9736', '9735', '12004']\n",
    "# y_list = ['txt', 'price']\n",
    "# path = 'Lv2期结论'\n",
    "# z = '男士春夏下装_from_0501'\n",
    "\n",
    "# # 定义过滤层级条件\n",
    "# filter_layer_cases = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]  # 示例分层条件\n",
    "\n",
    "# 用于汇总所有的 DataFrame\n",
    "all_summaries = []\n",
    "\n",
    "# 遍历 x_list 和 y_list 的每个组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}.xlsx'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # 去重并统计 structure 列的最常见值\n",
    "        df_unique = df.drop_duplicates(subset='Image Name')\n",
    "        structure_counts = df_unique['structure'].value_counts()\n",
    "        if len(structure_counts) > 0:\n",
    "            most_common_structure = structure_counts.idxmax()\n",
    "            print(f\"Most common structure: {most_common_structure}\")\n",
    "            new_df = df_unique[df_unique['structure'] == most_common_structure]\n",
    "        else:\n",
    "            new_df = pd.DataFrame()\n",
    "\n",
    "        # 过滤符合条件的数据\n",
    "        new_df['text'] = new_df['text'].astype(str)\n",
    "        new_df = new_df[new_df.apply(filter_by_rectangle, axis=1)]\n",
    "        df_grouped = new_df.groupby(['Height_Category'])\n",
    "\n",
    "        summaries = []\n",
    "        for height_category, group in tqdm(df_grouped):\n",
    "            all_text = \" \".join(group['text'].dropna())\n",
    "            try:\n",
    "                summary = summarize_with_gpt4(all_text)\n",
    "            except Exception as e:\n",
    "                summary = \"Error in summarization\"\n",
    "\n",
    "            summaries.append({\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'Height_Category': height_category,\n",
    "                'text': all_text,\n",
    "                'summary': summary,\n",
    "                'style': \"old\",\n",
    "                'structure': most_common_structure,\n",
    "                'brand': \"all\"\n",
    "            })\n",
    "\n",
    "        result_df = pd.DataFrame(summaries)\n",
    "        all_summaries.append(result_df)  # 将每个结果DataFrame添加到汇总列表中\n",
    "\n",
    "# 汇总所有DataFrame并保存为一个总的Excel文件\n",
    "final_df = pd.concat(all_summaries, ignore_index=True)\n",
    "output_file = f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_all_summary-old.xlsx\"\n",
    "final_df.to_excel(output_file, index=False)\n",
    "print(f\"All summaries saved to: {output_file}\")\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step8.3 - 利用chat进行文本内容统计\n",
    "### 这里是用旧版prompt,通过list读取\n",
    "### 先针对structure进行统计rank，然后再进行过滤\n",
    "### 对brand实行分层统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里把之前的版本折叠了,这个版本是将各个excel单独输出的\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "# import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # filter_suffix_list = ['filter_1.0_2.0_3.0','filter_4.0_5.0_6.0']\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def create_empty_file(file_path):\n",
    "#     os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#     pd.DataFrame().to_excel(file_path, index=False)\n",
    "#     print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "# def summarize_with_gpt4(text):\n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "#                 {\"role\": \"user\", \"content\": \"\"\"\n",
    "#                     # Role \n",
    "#                         角色: 电商数据分析师。\n",
    "#                     # Profile \n",
    "#                         简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "#                     ## Background \n",
    "#                         背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "#                     ## Goals \n",
    "#                         目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "#                     ## Constrains \n",
    "#                         约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "#                     ## Tone \n",
    "#                         语气风格: 正式的，客观的，科学的。\n",
    "#                     ## Skills \n",
    "#                         技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "#                     ## OutputFormat \n",
    "#                         输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "#             ],\n",
    "#         )\n",
    "#         return response.choices[0].message.content.strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in summarization: {str(e)}\")\n",
    "#         return \"\"\n",
    "\n",
    "# # 遍历 x, y 和 filter_suffix 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         for filter_suffix in filter_suffix_list:\n",
    "#             print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "            \n",
    "#             # 读取Excel文件\n",
    "#             file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}_{filter_suffix}.xlsx'\n",
    "#             df = pd.read_excel(file_path)\n",
    "#             # 根据 Image Name 去重\n",
    "#             df_unique = df.drop_duplicates(subset='Image Name')\n",
    "#             # 统计 structure 列各个值的数量\n",
    "#             structure_counts = df_unique['structure'].value_counts()\n",
    "#             if len(structure_counts) > 0:\n",
    "#                 most_common_structure, max_count = structure_counts.index[0], structure_counts.iloc[0]\n",
    "#                 print(f\"Most common structure: {most_common_structure}, Total count: {max_count}\")\n",
    "#                 new_df = df_unique[df_unique['structure'] == most_common_structure]\n",
    "#             else:\n",
    "#                 new_df = pd.DataFrame()\n",
    "\n",
    "#             # 确保 'text' 列中的所有值都是字符串\n",
    "#             new_df['text'] = new_df['text'].astype(str)\n",
    "            \n",
    "#             # 根据 filter_suffix 筛选数据\n",
    "#             filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "#             new_df = new_df[new_df['最终分层'].isin(filter_values)]\n",
    "            \n",
    "#             new_df = new_df[df.apply(filter_by_rectangle, axis=1)]\n",
    "#             df_grouped = new_df.groupby(['Height_Category'])\n",
    "            \n",
    "#             # 遍历每个分组，合并文本并进行总结\n",
    "#             summaries = []\n",
    "            \n",
    "#             # 遍历每个分组\n",
    "#             for (height_category), group in tqdm(df_grouped):\n",
    "#                 # 合并该组的所有文本\n",
    "#                 all_text = \" \".join(group['text'].dropna())\n",
    "                \n",
    "#                 # 使用 GPT-4 进行总结\n",
    "#                 summary = summarize_with_gpt4(all_text)\n",
    "                \n",
    "#                 # 将结果添加到列表中\n",
    "#                 summaries.append({\n",
    "#                     'Height_Category': height_category,\n",
    "#                     'text': all_text,\n",
    "#                     'summary': summary\n",
    "#                 })\n",
    "            \n",
    "#             # 创建一个新的DataFrame来存储结果\n",
    "#             result_df = pd.DataFrame(summaries)\n",
    "            \n",
    "#             # 保存结果到Excel文件，包含 filter_suffix 在文件名中\n",
    "#             output_file = f\"D://code//data//{path}//{z}//url_4//{x}_{y}_word_structure_{most_common_structure}_brand_{filter_suffix}_wordsummary.xlsx\"\n",
    "#             result_df.to_excel(output_file, index=False)\n",
    "#             print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# print(\"All processing completed.\")\n",
    "\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旧版本, 可能会遇到structure为空跳出的情况\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "# import datetime\n",
    "\n",
    "\n",
    "# # filter_suffix_list = ['filter_1.0_2.0_3.0','filter_4.0_5.0_6.0']\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     return not (row['txt_x2'] < right and row['txt_y2'] < bottom)\n",
    "\n",
    "\n",
    "# # 用于汇总所有的 DataFrame\n",
    "# all_summaries = []\n",
    "\n",
    "# # 遍历 x, y 和 filter_suffix 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         for filter_suffix in filter_suffix_list:\n",
    "#             print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "            \n",
    "#             file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}_{filter_suffix}.xlsx'\n",
    "#             if not os.path.exists(file_path):\n",
    "#                 print(f\"File not found: {file_path}\")\n",
    "#                 continue\n",
    "            \n",
    "#             df = pd.read_excel(file_path).drop_duplicates(subset='Image Name')\n",
    "\n",
    "#             if 'structure' in df.columns:\n",
    "#                 structure_counts = df['structure'].value_counts()\n",
    "#             else:\n",
    "#                 print(\"The 'structure' column does not exist in the DataFrame. Please check the data source and processing steps.\")\n",
    "            \n",
    "#             structure_counts = df['structure'].value_counts()\n",
    "            \n",
    "#             if len(structure_counts) > 0:\n",
    "#                 most_common_structure = structure_counts.idxmax()\n",
    "#                 print(f\"Most common structure: {most_common_structure}\")\n",
    "#                 new_df = df[df['structure'] == most_common_structure]\n",
    "#             else:\n",
    "#                 new_df = pd.DataFrame()\n",
    "\n",
    "#             filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "#             new_df = new_df[(new_df['最终分层'].isin(filter_values)) & new_df.apply(filter_by_rectangle, axis=1)]\n",
    "#             new_df['text'] = new_df['text'].astype(str)\n",
    "\n",
    "#             df_grouped = new_df.groupby(['Height_Category'])\n",
    "#             summaries = []\n",
    "\n",
    "#             for height_category, group in tqdm(df_grouped):\n",
    "#                 all_text = \" \".join(group['text'].dropna())\n",
    "\n",
    "#                 summary = summarize_with_gpt4(all_text)\n",
    "#                 summaries.append({\n",
    "#                     'x': x,\n",
    "#                     'y': y,\n",
    "#                     'Height_Category': height_category,\n",
    "#                     'text': all_text,\n",
    "#                     'summary': summary,\n",
    "#                     'style': \"old\",\n",
    "#                     'structure': most_common_structure,\n",
    "#                     'brand': filter_suffix\n",
    "#                 })\n",
    "\n",
    "#             result_df = pd.DataFrame(summaries)\n",
    "#             all_summaries.append(result_df)\n",
    "\n",
    "#             output_file = f\"D://code//data//{path}//{z}//url_4//{x}_{y}_word_structure_{most_common_structure}_brand_{filter_suffix}_wordsummary.xlsx\"\n",
    "#             result_df.to_excel(output_file, index=False)\n",
    "#             print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# # 汇总所有生成的 DataFrame 并保存为一个总的 Excel 文件\n",
    "# final_df = pd.concat(all_summaries, ignore_index=True)\n",
    "# combined_output_file = f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_every_summary-old.xlsx\"\n",
    "# final_df.to_excel(combined_output_file, index=False)\n",
    "# print(f\"All summaries saved to: {combined_output_file}\")\n",
    "\n",
    "# # 打印当前时间\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(f\"All processing completed at: {formatted_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新版本, 会跳过structure为空的情况\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "\n",
    "# filter_suffix_list = ['filter_1.0_2.0_3.0', 'filter_4.0_5.0_6.0']\n",
    "\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    return not (row['txt_x2'] < right and row['txt_y2'] < bottom)\n",
    "\n",
    "\n",
    "# 用于汇总所有的 DataFrame\n",
    "all_summaries = []\n",
    "\n",
    "\n",
    "# 遍历 x, y 和 filter_suffix 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        for filter_suffix in filter_suffix_list:\n",
    "            print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "\n",
    "            file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}_{filter_suffix}.xlsx'\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_excel(file_path).drop_duplicates(subset='Image Name')\n",
    "\n",
    "            if 'structure' in df.columns:\n",
    "                structure_counts = df['structure'].value_counts()\n",
    "                if len(structure_counts) > 0:\n",
    "                    most_common_structure = structure_counts.idxmax()\n",
    "                    print(f\"Most common structure: {most_common_structure}\")\n",
    "                    new_df = df[df['structure'] == most_common_structure]\n",
    "                else:\n",
    "                    new_df = pd.DataFrame()\n",
    "            else:\n",
    "                print(\"The 'structure' column does not exist in the DataFrame. Please check the data source and processing steps.\")\n",
    "                new_df = pd.DataFrame()\n",
    "                continue  # 遇到 'structure' 列不存在的情况，直接跳过本次循环后续处理，进入下一次循环\n",
    "\n",
    "            filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "            new_df = new_df[(new_df['最终分层'].isin(filter_values)) & new_df.apply(filter_by_rectangle, axis=1)]\n",
    "            new_df['text'] = new_df['text'].astype(str)\n",
    "\n",
    "            df_grouped = new_df.groupby(['Height_Category'])\n",
    "            summaries = []\n",
    "\n",
    "            for height_category, group in tqdm(df_grouped):\n",
    "                all_text = \" \".join(group['text'].dropna())\n",
    "\n",
    "                summary = summarize_with_gpt4(all_text)\n",
    "                summaries.append({\n",
    "                    'x': x,\n",
    "                    'y': y,\n",
    "                    'Height_Category': height_category,\n",
    "                    'text': all_text,\n",
    "                    'summary': summary,\n",
    "                    'style': \"old\",\n",
    "                    'structure': most_common_structure,\n",
    "                    'brand': filter_suffix\n",
    "                })\n",
    "\n",
    "            result_df = pd.DataFrame(summaries)\n",
    "            all_summaries.append(result_df)\n",
    "\n",
    "            output_file = f\"D://code//data//{path}//{z}//url_4//{x}_{y}_word_structure_{most_common_structure}_brand_{filter_suffix}_wordsummary.xlsx\"\n",
    "            result_df.to_excel(output_file, index=False)\n",
    "            print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# 汇总所有生成的 DataFrame 并保存为一个总的 Excel 文件\n",
    "final_df = pd.concat(all_summaries, ignore_index=True)\n",
    "combined_output_file = f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_every_summary-old.xlsx\"\n",
    "final_df.to_excel(combined_output_file, index=False)\n",
    "print(f\"All summaries saved to: {combined_output_file}\")\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formulated_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formulated_time}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step8.4 - 利用chat进行文本内容统计\n",
    "### 这里是用新版prompt,通过list读取\n",
    "### 全量structure\n",
    "### 全量brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    return row['txt_x2'] >= right or row['txt_y2'] >= bottom\n",
    "\n",
    "# 使用GPT-4模型对文本进行自动摘要\n",
    "def summarize_with_gpt4_new(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "                # Role \n",
    "                角色: 电商数据分析师。\n",
    "                # Profile \n",
    "                简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "                ## Background \n",
    "                背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，并基于一些前置的定义，找出这些描述信息都是从哪些维度切入的。\n",
    "                ## Goals \n",
    "                目标: 基于我给到的商品描述信息数据集和前置的维度定义，归纳总结出描述的方向维度，需要特别关注与细化商品本身的卖点特性，并统计这些维度出现的频率。\n",
    "                ## Definitions\n",
    "                定义：\n",
    "                1. 直接展示价格：直接展示价格信息，到手价，预估到手价，会员价等，通常包含上述前缀，¥+具体的价格数字或者具体的价格数字+元。\n",
    "                2. 折扣信息：描述商品的折扣，通常包含具体的折扣数字+折。\n",
    "                3. 直降信息：描述商品相较原价进行了大幅降价，通常包含直降、立减。\n",
    "                4. 满减信息：描述若购买到一定金额，可以在此基础上进行金额优惠，通常包含满+具体的金额+减+具体的金额\n",
    "                5. 赠品信息：描述若购买商品则会赠送服务或商品，通常包含赠、送\n",
    "                6. 限时：描述商品促销的时间，通常包含活动时间段、活动开始时间、活动结束时间\n",
    "                7. 品牌名称：描述商品的品牌名称\n",
    "                8. 代言人信息：描述商品的代言人信息\n",
    "                9. 价保：价格保护，通常包含价保\n",
    "                10. 店铺背书：描述店铺的信息，通常包含旗舰店、自营\n",
    "                11. 物流服务：描述商品所包含的物流服务，通常包含物流时效、运费险、物流名称、仓库名称、包邮\n",
    "                12. 直接展示价格属于价格信息一级维度，折扣信息、直降信息、满减信息、赠品信息、限时属于价促活动一级维度，品牌名称、代言人信息属于品牌信息一级维度，价保、店铺背书、物流服务属于服务保障一级维度\n",
    "                ## Constrains \n",
    "                约束条件: 1、时刻保持自己是电商数据分析师的角色，2、可以进行适当的联想和猜测，3、举例的时候禁止出现\"\"，4、统计频率的时候请仔细仔细再仔细，5、若识别到的内容不在上述定义的维度中，可自行命名并统计，请不要忽视未被定义的维度，特别是关于商品本身的卖点信息描述\n",
    "                ## Tone \n",
    "                语气风格: 正式的，客观的，科学的。\n",
    "                ## Skills \n",
    "                技能: 1、你有出色的文本理解能力，能够理解输入数据的含义 2、你有出色的归纳总结能力，能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力，能够精确的统计出各个维度出现的频次。\n",
    "                ## OutputFormat \n",
    "                输出格式:以文字方式输出，一级维度，一级维度下具体内容和举例和频次，输出顺序按照价格信息、价促活动、品牌信息、服务保障、商品卖点进行输出，商品卖点为未定义维度，请你依照自己的知识库信息进行汇总输出，需要特别注意，是关于商品本身的描述，输出格式为1.价格信息 总频次 直接展示价格 频次 举例 以此类推,注意输出要精简，减少不必要的换行\n",
    "                    \"\"\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# 创建一个空的总 DataFrame\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# 遍历 x 和 y 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification-{x}_{y}.xlsx'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_excel(file_path)\n",
    "        df['text'] = df['text'].astype(str)  # 确保 'text' 列为字符串\n",
    "        df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "        df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "        # 遍历每个分组，合并文本并进行总结\n",
    "        summaries = []\n",
    "        for height_category, group in tqdm(df_grouped):\n",
    "            all_text = \" \".join(group['text'].dropna())\n",
    "            try:\n",
    "                summary = summarize_with_gpt4_new(all_text)\n",
    "            except Exception as e:\n",
    "                summary = \"Error in summarization\"\n",
    "            \n",
    "            # 将结果添加到列表中\n",
    "            summaries.append({\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'Height_Category': height_category,\n",
    "                'text': all_text,\n",
    "                'summary': summary,\n",
    "                'style': \"new\",\n",
    "                'structure': \"all\",\n",
    "                'brand': \"all\"\n",
    "            })\n",
    "        \n",
    "        # 创建一个 DataFrame 来存储当前组合的结果\n",
    "        result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "        # 重新排序列\n",
    "        result_df = result_df[['x', 'y', 'Height_Category', 'text', 'summary', 'style', 'structure', 'brand']]\n",
    "        \n",
    "        # 将当前的结果 DataFrame 添加到总的 DataFrame 中\n",
    "        all_data = pd.concat([all_data, result_df], ignore_index=True)\n",
    "\n",
    "# 保存所有数据到一个汇总的Excel文件中\n",
    "output_file = f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_all_brand_all_summary-new.xlsx\"\n",
    "all_data.to_excel(output_file, index=False)\n",
    "print(f\"All data saved to: {output_file}\")\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step8.5 - 利用chat进行文本内容统计\n",
    "### 这里是用新版prompt,通过list读取\n",
    "### 先针对structure进行统计rank，然后再进行过滤\n",
    "### 没有对brand进行分层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# # 定义 x_list 和 y_list\n",
    "# x_list = ['9736', '9735', '12004']\n",
    "# y_list = ['txt', 'price']\n",
    "# path = 'Lv2期结论'\n",
    "# z = '男士春夏下装_from_0501'\n",
    "\n",
    "# # 定义过滤层级条件\n",
    "# filter_layer_cases = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]  # 示例分层条件\n",
    "\n",
    "# 用于汇总所有的 DataFrame\n",
    "all_summaries = []\n",
    "\n",
    "# 遍历 x_list 和 y_list 的每个组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}.xlsx'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # 去重并统计 structure 列的最常见值\n",
    "        df_unique = df.drop_duplicates(subset='Image Name')\n",
    "        structure_counts = df_unique['structure'].value_counts()\n",
    "        if len(structure_counts) > 0:\n",
    "            most_common_structure = structure_counts.idxmax()\n",
    "            print(f\"Most common structure: {most_common_structure}\")\n",
    "            new_df = df_unique[df_unique['structure'] == most_common_structure]\n",
    "        else:\n",
    "            new_df = pd.DataFrame()\n",
    "\n",
    "        # 过滤符合条件的数据\n",
    "        new_df['text'] = new_df['text'].astype(str)\n",
    "        new_df = new_df[new_df.apply(filter_by_rectangle, axis=1)]\n",
    "        df_grouped = new_df.groupby(['Height_Category'])\n",
    "\n",
    "        summaries = []\n",
    "        for height_category, group in tqdm(df_grouped):\n",
    "            all_text = \" \".join(group['text'].dropna())\n",
    "            try:\n",
    "                summary = summarize_with_gpt4_new(all_text)\n",
    "            except Exception as e:\n",
    "                summary = \"Error in summarization\"\n",
    "\n",
    "            summaries.append({\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'Height_Category': height_category,\n",
    "                'text': all_text,\n",
    "                'summary': summary,\n",
    "                'style': \"new\",\n",
    "                'structure': most_common_structure,\n",
    "                'brand': \"all\"\n",
    "            })\n",
    "\n",
    "        result_df = pd.DataFrame(summaries)\n",
    "        all_summaries.append(result_df)  # 将每个结果DataFrame添加到汇总列表中\n",
    "\n",
    "# 汇总所有DataFrame并保存为一个总的Excel文件\n",
    "final_df = pd.concat(all_summaries, ignore_index=True)\n",
    "output_file = f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_all_summary-new.xlsx\"\n",
    "final_df.to_excel(output_file, index=False)\n",
    "print(f\"All summaries saved to: {output_file}\")\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step8.6 - 利用chat进行文本内容统计\n",
    "### 这里是用新版prompt,通过list读取\n",
    "### 先针对structure进行统计rank，然后再进行过滤\n",
    "### 对brand进行了分层处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旧版本, 可能会遇到structure为空跳出的情况\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "# import datetime\n",
    "\n",
    "\n",
    "# # filter_suffix_list = ['filter_1.0_2.0_3.0','filter_4.0_5.0_6.0']\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     return not (row['txt_x2'] < right and row['txt_y2'] < bottom)\n",
    "\n",
    "\n",
    "# # 用于汇总所有的 DataFrame\n",
    "# all_summaries = []\n",
    "\n",
    "# # 遍历 x, y 和 filter_suffix 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         for filter_suffix in filter_suffix_list:\n",
    "#             print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "            \n",
    "#             file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}_{filter_suffix}.xlsx'\n",
    "#             if not os.path.exists(file_path):\n",
    "#                 print(f\"File not found: {file_path}\")\n",
    "#                 continue\n",
    "            \n",
    "#             df = pd.read_excel(file_path).drop_duplicates(subset='Image Name')\n",
    "#             structure_counts = df['structure'].value_counts()\n",
    "#             if len(structure_counts) > 0:\n",
    "#                 most_common_structure = structure_counts.idxmax()\n",
    "#                 print(f\"Most common structure: {most_common_structure}\")\n",
    "#                 new_df = df[df['structure'] == most_common_structure]\n",
    "#             else:\n",
    "#                 new_df = pd.DataFrame()\n",
    "\n",
    "#             filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "#             new_df = new_df[(new_df['最终分层'].isin(filter_values)) & new_df.apply(filter_by_rectangle, axis=1)]\n",
    "#             new_df['text'] = new_df['text'].astype(str)\n",
    "\n",
    "#             df_grouped = new_df.groupby(['Height_Category'])\n",
    "#             summaries = []\n",
    "\n",
    "#             for height_category, group in tqdm(df_grouped):\n",
    "#                 all_text = \" \".join(group['text'].dropna())\n",
    "#                 summary = summarize_with_gpt4_new(all_text)\n",
    "\n",
    "#                 summaries.append({\n",
    "#                     'x': x,\n",
    "#                     'y': y,\n",
    "#                     'Height_Category': height_category,\n",
    "#                     'text': all_text,\n",
    "#                     'summary': summary,\n",
    "#                     'style': \"old\",\n",
    "#                     'structure': most_common_structure,\n",
    "#                     'brand': filter_suffix\n",
    "#                 })\n",
    "\n",
    "#             result_df = pd.DataFrame(summaries)\n",
    "#             all_summaries.append(result_df)\n",
    "\n",
    "#             output_file = f\"D://code//data//{path}//{z}//url_4//{x}_{y}_word_structure_{most_common_structure}_brand_{filter_suffix}_wordsummary.xlsx\"\n",
    "#             result_df.to_excel(output_file, index=False)\n",
    "#             print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# # 汇总所有生成的 DataFrame 并保存为一个总的 Excel 文件\n",
    "# final_df = pd.concat(all_summaries, ignore_index=True)\n",
    "# combined_output_file = f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_every_summary-new.xlsx\"\n",
    "# final_df.to_excel(combined_output_file, index=False)\n",
    "# print(f\"All summaries saved to: {combined_output_file}\")\n",
    "\n",
    "# # 打印当前时间\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新版本, 针对structure为空的情况进行处理, 直接跳过\n",
    "#\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "\n",
    "# filter_suffix_list = ['filter_1.0_2.0_3.0', 'filter_4.0_5.0_6.0']\n",
    "\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    return not (row['txt_x2'] < right and row['txt_y2'] < bottom)\n",
    "\n",
    "\n",
    "# 用于汇总所有的 DataFrame\n",
    "all_summaries = []\n",
    "\n",
    "\n",
    "# 遍历 x, y 和 filter_suffix 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        for filter_suffix in filter_suffix_list:\n",
    "            print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "\n",
    "            file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification_structure-{x}_{y}_{filter_suffix}.xlsx'\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_excel(file_path).drop_duplicates(subset='Image Name')\n",
    "\n",
    "            if 'structure' in df.columns:\n",
    "                structure_counts = df['structure'].value_counts()\n",
    "                if len(structure_counts) > 0:\n",
    "                    most_common_structure = structure_counts.idxmax()\n",
    "                    print(f\"Most common structure: {most_common_structure}\")\n",
    "                    new_df = df[df['structure'] == most_common_structure]\n",
    "                else:\n",
    "                    new_df = pd.DataFrame()\n",
    "            else:\n",
    "                print(\"The 'structure' column does not exist in the DataFrame. Please check the data source and processing steps.\")\n",
    "                new_df = pd.DataFrame()\n",
    "                continue  # 遇到 'structure' 列不存在的情况，直接跳过本次循环后续处理，进入下一次循环\n",
    "\n",
    "            filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "            new_df = new_df[(new_df['最终分层'].isin(filter_values)) & new_df.apply(filter_by_rectangle, axis=1)]\n",
    "            new_df['text']: new_df['text'].astype(str)\n",
    "\n",
    "            df_grouped = new_df.groupby(['Height_Category'])\n",
    "            summaries = []\n",
    "\n",
    "            for height_category, group in tqdm(df_grouped):\n",
    "                all_text = \" \".join(group['text'].dropna())\n",
    "                summary = summarize_with_gpt4_new(all_text)\n",
    "\n",
    "                summaries.append({\n",
    "                    'x': x,\n",
    "                    'y': y,\n",
    "                    'Height_Category': height_category,\n",
    "                    'text': all_text,\n",
    "                    'summary': summary,\n",
    "                    'style': \"old\",\n",
    "                    'structure': most_common_structure,\n",
    "                    'brand': filter_suffix\n",
    "                })\n",
    "\n",
    "            result_df = pd.DataFrame(summaries)\n",
    "            all_summaries.append(result_df)\n",
    "\n",
    "            output_file = f\"D://code//data//{path}//{z}//url_4//{x}_{y}_word_structure_{most_common_structure}_brand_{filter_suffix}_wordsummary.xlsx\"\n",
    "            result_df.to_excel(output_file, index=False)\n",
    "            print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# 汇总所有生成的 DataFrame 并保存为一个总的 Excel 文件\n",
    "final_df = pd.concat(all_summaries, ignore_index=True)\n",
    "combined_output_file = f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_every_summary-new.xlsx\"\n",
    "final_df.to_excel(combined_output_file, index=False)\n",
    "print(f\"All summaries saved to: {combined_output_file}\")\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 指定要拼接的Excel文件路径列表\n",
    "excel_file_paths = [\n",
    "    f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_all_brand_all_summary-old.xlsx\",\n",
    "    f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_all_summary-old.xlsx\",\n",
    "    f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_every_summary-old.xlsx\",\n",
    "    f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_all_brand_all_summary-new.xlsx\",\n",
    "    f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_all_summary-new.xlsx\",\n",
    "    f\"D://code//data//Lv2期结论//{z}//url_4//word_structure_every_brand_every_summary-new.xlsx\"\n",
    "]\n",
    "\n",
    "# 用于存储读取的每个Excel文件的数据框\n",
    "dataframes = []\n",
    "\n",
    "# 逐个读取指定的Excel文件并添加到dataframes列表中\n",
    "for file_path in excel_file_paths:\n",
    "    df = pd.read_excel(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# 将所有数据框上下拼接在一起\n",
    "merged_df = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "# 可以根据需要将拼接后的结果保存为新的Excel文件\n",
    "merged_df.to_excel(f\"D://code//data//Lv2期结论//{z}//url_4//word_summary-all.xlsx\", index=False)\n",
    "\n",
    "# 打印当前时间\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "print(f\"All processing completed at: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.patches import Rectangle\n",
    "# import seaborn as sns\n",
    "# import datetime\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# # filter_suffix_list = ['filter_1.0_2.0','filter_3.0_4.0','filter_5.0_6.0']\n",
    "\n",
    "\n",
    "\n",
    "# def create_output_folder(x, y, filter_suffix):\n",
    "#     output_folder = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', f'50%_word_heatmaps_{filter_suffix}')\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     return output_folder\n",
    "\n",
    "# def read_excel_file(x, y, filter_suffix):\n",
    "#     file_path = os.path.join(f'D://code//data//{path}//{z}//{x}//grounding_output//{y}', f'50%_word_classification-{x}_{y}_{filter_suffix}.xlsx')\n",
    "#     return file_path\n",
    "\n",
    "# def create_empty_file(file_path):\n",
    "#     os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#     pd.DataFrame().to_excel(file_path, index=False)\n",
    "#     print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "# def plot_heatmaps(df, output_folder, filter_suffix):\n",
    "#     if df.empty:\n",
    "#         print(f\"Skipping empty DataFrame for {filter_suffix}\")\n",
    "#         return\n",
    "\n",
    "#     grouped = df.groupby('Classification')\n",
    "#     for name, group in grouped:\n",
    "#         fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "#         ax1.set_xlim(0, 616)\n",
    "#         ax1.set_ylim(616, 0)\n",
    "#         for _, row in group.iterrows():\n",
    "#             x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "#             if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "#                 continue\n",
    "#             width, height = x2 - x1, y2 - y1\n",
    "#             rect = plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='r')\n",
    "#             ax1.add_patch(rect)\n",
    "#         ax1.set_title(f'Bounding Boxes for {name} ({filter_suffix})')\n",
    "#         ax1.set_xlabel('X')\n",
    "#         ax1.set_ylabel('Y')\n",
    "\n",
    "#         heatmap = np.zeros((616, 616))\n",
    "#         for _, row in group.iterrows():\n",
    "#             x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "#             if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "#                 continue\n",
    "#             x1, y1 = max(0, min(x1, 615)), max(0, min(y1, 615))\n",
    "#             x2, y2 = max(0, min(x2, 615)), max(0, min(y2, 615))\n",
    "#             heatmap[int(y1):int(y2)+1, int(x1):int(x2)+1] += 1\n",
    "\n",
    "#         sns.heatmap(heatmap, ax=ax2, cmap='YlOrRd', cbar=True)\n",
    "#         ax2.set_title(f'Heatmap for {name} ({filter_suffix})')\n",
    "#         ax2.set_xlabel('X')\n",
    "#         ax2.set_ylabel('Y')\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(output_folder, f'{name}_{filter_suffix}_heatmap.png'), dpi=100, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "\n",
    "#         print(f\"已保存 {name} 的热力图 ({filter_suffix})\")\n",
    "\n",
    "#     print(f\"所有热力图已保存在 {output_folder} 文件夹中\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     for x in x_list:\n",
    "#         for y in y_list:\n",
    "#             for filter_suffix in filter_suffix_list:\n",
    "#                 print(f\"Processing for x={x}, y={y}, filter_suffix={filter_suffix}...\")\n",
    "#                 output_folder = create_output_folder(x, y, filter_suffix)\n",
    "#                 file_path = read_excel_file(x, y, filter_suffix)\n",
    "                \n",
    "#                 try:\n",
    "#                     df = pd.read_excel(file_path)\n",
    "#                 except FileNotFoundError:\n",
    "#                     print(f\"File not found: {file_path}\")\n",
    "#                     create_empty_file(file_path)\n",
    "#                     df = pd.DataFrame()\n",
    "\n",
    "#                 if not df.empty:\n",
    "#                     filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "#                     df = df[df['最终分层'].isin(filter_values)]\n",
    "                \n",
    "#                 plot_heatmaps(df, output_folder, filter_suffix)\n",
    "\n",
    "#     current_time = datetime.datetime.now()\n",
    "#     formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     print(formatted_time)\n",
    "#     print(formatted_time)\n",
    "#     print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本大小的总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 新流程, 可以通过list方式, 来合并读取\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "\n",
    "# for item_x in x_list:\n",
    "#     for item_y in y_list:\n",
    "#         # 读取Excel文件\n",
    "#         file_path = f'D://code//data//{path}//{z}//{item_x}//grounding_output//{item_y}//50%_word_classification-{item_x}_{item_y}.xlsx'\n",
    "#         if not os.path.exists(file_path):\n",
    "#             print(f\"No data found for x={item_x}, y={item_y}. Skipping...\")\n",
    "#             continue\n",
    "#         df = pd.read_excel(file_path)\n",
    "        \n",
    "#         # 应用 filter_by_rectangle 函数来过滤数据\n",
    "#         df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "#         # 替换 Height_Category 列的值\n",
    "#         df['Height_Category'] = df['Height_Category'].replace({\n",
    "#             'Height_>38': 'Height大于38',\n",
    "#             'Height_18-29': 'Height18到29',\n",
    "#             'Height_29-38': 'Height29到38',\n",
    "#             'Height_<18': 'Height小于18'\n",
    "#         })\n",
    "\n",
    "#         # 删除 structure 为空值的行\n",
    "#         # df = df.dropna(subset=['structure'])\n",
    "#         df = df.dropna(subset=['Height_Category'])\n",
    "\n",
    "#         # 确保必要的列存在\n",
    "#         required_columns = ['Height_Category', 'txt_x1', 'txt_y1', 'txt_x2', 'txt_y2']\n",
    "#         if not all(col in df.columns for col in required_columns):\n",
    "#             raise ValueError(f\"DataFrame must contain all of these columns: {required_columns}\")\n",
    "\n",
    "#         # 创建输出目录\n",
    "#         output_dir = f\"D://code//data//{path}//{z}//{item_x}//grounding_output//{item_y}//50%_word_wordsize_heatmaps\"\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#         # 按 structure 和 box_no 分组\n",
    "#         grouped = df.groupby(['Height_Category'])\n",
    "\n",
    "#         # 遍历每个分组\n",
    "#         for (box_no), group in grouped:\n",
    "#             # 创建一个空的 2D numpy 数组来存储热力图数据，大小为 616x616\n",
    "#             heatmap_data = np.zeros((616, 616))\n",
    "\n",
    "#             # 对每个矩形框增加热度值\n",
    "#             for _, row in group.iterrows():\n",
    "#                 x1, y1, x2, y2 = map(int, [row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']])\n",
    "#                 # 确保坐标不超出边界\n",
    "#                 x1, x2 = max(0, min(x1, 616)), max(0, min(x2, 616))\n",
    "#                 y1, y2 = max(0, min(y1, 616)), max(0, min(y2, 616))\n",
    "#                 heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "#             # 创建图形，设置大小为正方形\n",
    "#             plt.figure(figsize=(10, 10))\n",
    "\n",
    "#             # 使用 seaborn 绘制热力图\n",
    "#             sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Frequency'}, square=True)\n",
    "\n",
    "#             # 设置标题和轴标签\n",
    "#             # plt.title(f'Bounding Box Heatmap - Structure: {structure}, word size: {Height_Category}')\n",
    "#             plt.xlabel('X coordinate')\n",
    "#             plt.ylabel('Y coordinate')\n",
    "\n",
    "#             # 调整图形以保持正方形比例\n",
    "#             plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "#             # 保存图形\n",
    "#             output_path = os.path.join(output_dir, f\"heatmap_wordsize_{box_no}.png\")\n",
    "#             plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "#             plt.close()\n",
    "\n",
    "#             print(f\"Heatmap saved to: {output_path}\")\n",
    "\n",
    "# print(\"All heatmaps have been generated.\")\n",
    "\n",
    "# import datetime\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # filter_suffix_list = ['filter_1.0_2.0','filter_3.0_4.0','filter_5.0_6.0']\n",
    "\n",
    "\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# def create_empty_file(file_path):\n",
    "#     os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#     pd.DataFrame().to_excel(file_path, index=False)\n",
    "#     print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "# def process_data(item_x, item_y, filter_suffix):\n",
    "#     file_path = f'D://code//data//{path}//{z}//{item_x}//grounding_output//{item_y}//50%_word_classification-{item_x}_{item_y}_{filter_suffix}.xlsx'\n",
    "    \n",
    "#     try:\n",
    "#         df = pd.read_excel(file_path)\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "#         create_empty_file(file_path)\n",
    "#         return pd.DataFrame()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     if df.empty:\n",
    "#         print(f\"Empty DataFrame for {file_path}\")\n",
    "#         return df\n",
    "\n",
    "#     required_columns = ['Height_Category', 'txt_x1', 'txt_y1', 'txt_x2', 'txt_y2', '最终分层']\n",
    "#     if not all(col in df.columns for col in required_columns):\n",
    "#         print(f\"Missing required columns in {file_path}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "#     df['Height_Category'] = df['Height_Category'].replace({\n",
    "#         'Height_>38': 'Height大于38',\n",
    "#         'Height_18-29': 'Height18到29',\n",
    "#         'Height_29-38': 'Height29到38',\n",
    "#         'Height_<18': 'Height小于18'\n",
    "#     })\n",
    "#     df = df.dropna(subset=['Height_Category'])\n",
    "\n",
    "#     filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "#     df = df[df['最终分层'].isin(filter_values)]\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def generate_heatmap(group, output_path):\n",
    "#     if group.empty:\n",
    "#         print(f\"Skipping empty group for {output_path}\")\n",
    "#         return\n",
    "\n",
    "#     heatmap_data = np.zeros((616, 616))\n",
    "#     for _, row in group.iterrows():\n",
    "#         x1, y1, x2, y2 = map(int, [row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']])\n",
    "#         x1, x2 = max(0, min(x1, 616)), max(0, min(x2, 616))\n",
    "#         y1, y2 = max(0, min(y1, 616)), max(0, min(y2, 616))\n",
    "#         heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Frequency'}, square=True)\n",
    "#     plt.xlabel('X coordinate')\n",
    "#     plt.ylabel('Y coordinate')\n",
    "#     plt.gca().set_aspect('equal', adjustable='box')\n",
    "#     plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "#     print(f\"Heatmap saved to: {output_path}\")\n",
    "\n",
    "# def main():\n",
    "#     for item_x in x_list:\n",
    "#         for item_y in y_list:\n",
    "#             for filter_suffix in filter_suffix_list:\n",
    "#                 print(f\"Processing: {item_x} - {item_y} - {filter_suffix}\")\n",
    "#                 df = process_data(item_x, item_y, filter_suffix)\n",
    "                \n",
    "#                 if df.empty:\n",
    "#                     print(f\"Skipping empty DataFrame for {item_x} - {item_y} - {filter_suffix}\")\n",
    "#                     continue\n",
    "\n",
    "#                 output_dir = f\"D://code//data//{path}//{z}//{item_x}//grounding_output//{item_y}//50%_word_wordsize_heatmaps_{filter_suffix}\"\n",
    "#                 os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#                 grouped = df.groupby(['Height_Category'])\n",
    "#                 for box_no, group in grouped:\n",
    "#                     output_path = os.path.join(output_dir, f\"heatmap_wordsize_{box_no}_{filter_suffix}.png\")\n",
    "#                     generate_heatmap(group, output_path)\n",
    "\n",
    "#     print(\"All heatmaps have been generated.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "#     current_time = datetime.datetime.now()\n",
    "#     formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     print(formatted_time)\n",
    "#     print(formatted_time)\n",
    "#     print(formatted_time)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 这里是用旧版prompt,通过list读取,针对整体\n",
    "\n",
    "# '''\n",
    "# 这里是通过读取list形式, 来简化输入的\n",
    "# '''\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "# def summarize_with_gpt4(text):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "#             {\"role\": \"user\", \"content\": \"\"\"\n",
    "#                 # Role \n",
    "#                     角色: 电商数据分析师。\n",
    "#                 # Profile \n",
    "#                     简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "#                 ## Background \n",
    "#                     背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "#                 ## Goals \n",
    "#                     目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "#                 ## Constrains \n",
    "#                     约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "#                 ## Tone \n",
    "#                     语气风格: 正式的，客观的，科学的。\n",
    "#                 ## Skills \n",
    "#                     技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "#                 ## OutputFormat \n",
    "#                     输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "# # 遍历 x 和 y 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "#         # 读取Excel文件\n",
    "#         file_path = f'D://code//data//Lv2期结论//{z}//{x}//{path}//{y}//50%_word_classification-{x}_{y}.xlsx'\n",
    "#         if not os.path.exists(file_path):\n",
    "#             print(f\"File not found: {file_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         df = pd.read_excel(file_path)\n",
    "#         # df = df.dropna(subset=['structure'])\n",
    "\n",
    "#         # 确保 'text' 列中的所有值都是字符串\n",
    "#         df['text'] = df['text'].astype(str)\n",
    "        \n",
    "#         df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "#         df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "#         # 遍历每个分组，合并文本并进行总结\n",
    "#         summaries = []\n",
    "        \n",
    "#         # 遍历每个分组\n",
    "#         for (height_category), group in tqdm(df_grouped):\n",
    "#             # 合并该组的所有文本\n",
    "#             all_text = \" \".join(group['text'].dropna())\n",
    "#             # print(f\"Structure: {structure}\")\n",
    "#             # print(f\"Height Category: {height_category}\")\n",
    "#             # print(f\"Text: {all_text[:100]}...\")  # 只打印前100个字符\n",
    "            \n",
    "#             # 使用 GPT-4 进行总结\n",
    "#             try:\n",
    "#                 summary = summarize_with_gpt4(all_text)\n",
    "#                 # print(f\"Summary: {summary[:100]}...\")  # 只打印前100个字符\n",
    "#             except Exception as e:\n",
    "#                 # print(f\"Error in summarization: {str(e)}\")\n",
    "#                 summary = \"Error in summarization\"\n",
    "            \n",
    "#             # 将结果添加到列表中\n",
    "#             summaries.append({\n",
    "#                 # 'structure': structure,\n",
    "#                 'Height_Category': height_category,\n",
    "#                 'text': all_text,\n",
    "#                 'summary': summary\n",
    "#             })\n",
    "        \n",
    "#         # 创建一个新的DataFrame来存储结果\n",
    "#         result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "#         # 保存结果到Excel文件\n",
    "#         output_file = f\"D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_{x}_{y}_文本分类总结.xlsx\"\n",
    "#         result_df.to_excel(output_file, index=False)\n",
    "#         print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# print(\"All processing completed.\")\n",
    "\n",
    "\n",
    "# import datetime\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 这里是用旧版prompt,通过list读取,针对的是品牌维度的分类\n",
    "\n",
    "\n",
    "# '''\n",
    "# 这里是通过读取list形式, 来简化输入的\n",
    "# 添加了针对品牌维度的分类\n",
    "# '''\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "# import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # filter_suffix_list = ['filter_1.0_2.0','filter_3.0_4.0','filter_5.0_6.0']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def create_empty_file(file_path):\n",
    "#     os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#     pd.DataFrame().to_excel(file_path, index=False)\n",
    "#     print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "# def summarize_with_gpt4(text):\n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "#                 {\"role\": \"user\", \"content\": \"\"\"\n",
    "#                     # Role \n",
    "#                         角色: 电商数据分析师。\n",
    "#                     # Profile \n",
    "#                         简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "#                     ## Background \n",
    "#                         背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "#                     ## Goals \n",
    "#                         目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "#                     ## Constrains \n",
    "#                         约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "#                     ## Tone \n",
    "#                         语气风格: 正式的，客观的，科学的。\n",
    "#                     ## Skills \n",
    "#                         技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "#                     ## OutputFormat \n",
    "#                         输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "#             ],\n",
    "#         )\n",
    "#         return response.choices[0].message.content.strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in summarization: {str(e)}\")\n",
    "#         return \"\"\n",
    "\n",
    "# # 遍历 x, y 和 filter_suffix 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         for filter_suffix in filter_suffix_list:\n",
    "#             print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "            \n",
    "#             # 读取Excel文件\n",
    "#             file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification-{x}_{y}_{filter_suffix}.xlsx'\n",
    "#             if not os.path.exists(file_path):\n",
    "#                 print(f\"File not found: {file_path}\")\n",
    "#                 create_empty_file(file_path)\n",
    "#                 continue\n",
    "            \n",
    "#             try:\n",
    "#                 df = pd.read_excel(file_path)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "#                 create_empty_file(file_path)\n",
    "#                 continue\n",
    "\n",
    "#             if df.empty:\n",
    "#                 print(f\"Empty DataFrame for {file_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             # 确保 'text' 列中的所有值都是字符串\n",
    "#             df['text'] = df['text'].astype(str)\n",
    "            \n",
    "#             # 根据 filter_suffix 筛选数据\n",
    "#             filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "#             df = df[df['最终分层'].isin(filter_values)]\n",
    "            \n",
    "#             df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "#             df_grouped = df.groupby(['Height_Category'])\n",
    "            \n",
    "#             # 遍历每个分组，合并文本并进行总结\n",
    "#             summaries = []\n",
    "            \n",
    "#             # 遍历每个分组\n",
    "#             for (height_category), group in tqdm(df_grouped):\n",
    "#                 # 合并该组的所有文本\n",
    "#                 all_text = \" \".join(group['text'].dropna())\n",
    "                \n",
    "#                 # 使用 GPT-4 进行总结\n",
    "#                 summary = summarize_with_gpt4(all_text)\n",
    "                \n",
    "#                 # 将结果添加到列表中\n",
    "#                 summaries.append({\n",
    "#                     'Height_Category': height_category,\n",
    "#                     'text': all_text,\n",
    "#                     'summary': summary\n",
    "#                 })\n",
    "            \n",
    "#             # 创建一个新的DataFrame来存储结果\n",
    "#             result_df = pd.DataFrame(summaries)\n",
    "            \n",
    "#             # 保存结果到Excel文件，包含 filter_suffix 在文件名中\n",
    "#             output_file = f\"D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_{x}_{y}_{filter_suffix}_文本分类总结.xlsx\"\n",
    "#             result_df.to_excel(output_file, index=False)\n",
    "#             print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# print(\"All processing completed.\")\n",
    "\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下使用了新的prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 这里是用新版prompt,通过list读取,针对整体\n",
    "\n",
    "# '''\n",
    "# 这里是通过读取list形式, 来简化输入的\n",
    "# '''\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "# # # 定义 x_list 和 y_list（只需要一次）\n",
    "# # x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']  # \n",
    "# # x_list = ['9736', '9735', '12004']  # 男士春夏下\n",
    "# # x_list = ['1348', '1349', '1350', '9733', '9734', '12005', '21444', '35434']  # 男士春夏上装\n",
    "# # x_list = ['1354','1355','1356','9713','11988','12000','22355','28341','30786']  # 女士春夏上装\n",
    "# # x_list = ['9715', '9716', '9717', '11991']  # 女士春夏下装\n",
    "# # x_list = ['6914', '6916', '6917', '6918', '9775', '9776', '9777']  # 女鞋\n",
    "# # x_list = ['1','2','3']\n",
    "\n",
    "# # y_list = ['price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# # z = '男鞋_新分类_from_0501'\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "\n",
    "# # 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "# def summarize_with_gpt4(text):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "#             {\"role\": \"user\", \"content\": \"\"\"\n",
    "#                 # Role \n",
    "#                 角色: 电商数据分析师。\n",
    "#                 # Profile \n",
    "#                 简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "#                 ## Background \n",
    "#                 背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，并基于一些前置的定义，找出这些描述信息都是从哪些维度切入的。\n",
    "#                 ## Goals \n",
    "#                 目标: 基于我给到的商品描述信息数据集和前置的维度定义，归纳总结出描述的方向维度，需要特别关注与细化商品本身的卖点特性，并统计这些维度出现的频率。\n",
    "#                 ## Definitions\n",
    "#                 定义：\n",
    "#                 1. 直接展示价格：直接展示价格信息，到手价，预估到手价，会员价等，通常包含上述前缀，¥+具体的价格数字或者具体的价格数字+元。\n",
    "#                 2. 折扣信息：描述商品的折扣，通常包含具体的折扣数字+折。\n",
    "#                 3. 直降信息：描述商品相较原价进行了大幅降价，通常包含直降、立减。\n",
    "#                 4. 满减信息：描述若购买到一定金额，可以在此基础上进行金额优惠，通常包含满+具体的金额+减+具体的金额\n",
    "#                 5. 赠品信息：描述若购买商品则会赠送服务或商品，通常包含赠、送\n",
    "#                 6. 限时：描述商品促销的时间，通常包含活动时间段、活动开始时间、活动结束时间\n",
    "#                 7. 品牌名称：描述商品的品牌名称\n",
    "#                 8. 代言人信息：描述商品的代言人信息\n",
    "#                 9. 价保：价格保护，通常包含价保\n",
    "#                 10. 店铺背书：描述店铺的信息，通常包含旗舰店、自营\n",
    "#                 11. 物流服务：描述商品所包含的物流服务，通常包含物流时效、运费险、物流名称、仓库名称、包邮\n",
    "#                 12. 直接展示价格属于价格信息一级维度，折扣信息、直降信息、满减信息、赠品信息、限时属于价促活动一级维度，品牌名称、代言人信息属于品牌信息一级维度，价保、店铺背书、物流服务属于服务保障一级维度\n",
    "#                 ## Constrains \n",
    "#                 约束条件: 1、时刻保持自己是电商数据分析师的角色，2、可以进行适当的联想和猜测，3、举例的时候禁止出现\"\"，4、统计频率的时候请仔细仔细再仔细，5、若识别到的内容不在上述定义的维度中，可自行命名并统计，请不要忽视未被定义的维度，特别是关于商品本身的卖点信息描述\n",
    "#                 ## Tone \n",
    "#                 语气风格: 正式的，客观的，科学的。\n",
    "#                 ## Skills \n",
    "#                 技能: 1、你有出色的文本理解能力，能够理解输入数据的含义 2、你有出色的归纳总结能力，能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力，能够精确的统计出各个维度出现的频次。\n",
    "#                 ## OutputFormat \n",
    "#                 输出格式:以文字方式输出，一级维度，一级维度下具体内容和举例和频次，输出顺序按照价格信息、价促活动、品牌信息、服务保障、商品卖点进行输出，商品卖点为未定义维度，请你依照自己的知识库信息进行汇总输出，需要特别注意，是关于商品本身的描述，输出格式为1.价格信息 总频次 直接展示价格 频次 举例 以此类推,注意输出要精简，减少不必要的换行\n",
    "#                     \"\"\"}\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# # 遍历 x 和 y 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "#         # 读取Excel文件\n",
    "#         file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification-{x}_{y}.xlsx'\n",
    "#         if not os.path.exists(file_path):\n",
    "#             print(f\"File not found: {file_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         df = pd.read_excel(file_path)\n",
    "#         # df = df.dropna(subset=['structure'])\n",
    "\n",
    "#         # 确保 'text' 列中的所有值都是字符串\n",
    "#         df['text'] = df['text'].astype(str)\n",
    "        \n",
    "#         df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "#         df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "#         # 遍历每个分组，合并文本并进行总结\n",
    "#         summaries = []\n",
    "        \n",
    "#         # 遍历每个分组\n",
    "#         for (height_category), group in tqdm(df_grouped):\n",
    "#             # 合并该组的所有文本\n",
    "#             all_text = \" \".join(group['text'].dropna())\n",
    "#             # print(f\"Structure: {structure}\")\n",
    "#             # print(f\"Height Category: {height_category}\")\n",
    "#             # print(f\"Text: {all_text[:100]}...\")  # 只打印前100个字符\n",
    "            \n",
    "#             # 使用 GPT-4 进行总结\n",
    "#             try:\n",
    "#                 summary = summarize_with_gpt4(all_text)\n",
    "#                 # print(f\"Summary: {summary[:100]}...\")  # 只打印前100个字符\n",
    "#             except Exception as e:\n",
    "#                 # print(f\"Error in summarization: {str(e)}\")\n",
    "#                 summary = \"Error in summarization\"\n",
    "            \n",
    "#             # 将结果添加到列表中\n",
    "#             summaries.append({\n",
    "#                 # 'structure': structure,\n",
    "#                 'Height_Category': height_category,\n",
    "#                 'text': all_text,\n",
    "#                 'summary': summary\n",
    "#             })\n",
    "        \n",
    "#         # 创建一个新的DataFrame来存储结果\n",
    "#         result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "#         # 保存结果到Excel文件\n",
    "#         output_file = f\"D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_{x}_{y}_文本分类总结-new.xlsx\"\n",
    "#         result_df.to_excel(output_file, index=False)\n",
    "#         print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# print(\"All processing completed.\")\n",
    "\n",
    "\n",
    "# import datetime\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 这里是用新版prompt,通过list读取,针对的是品牌维度的分类\n",
    "\n",
    "\n",
    "# '''\n",
    "# 这里是通过读取list形式, 来简化输入的\n",
    "# 添加了针对品牌维度的分类\n",
    "# '''\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "# import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 filter_suffix_list\n",
    "# # filter_suffix_list = ['filter_1.0','filter_2.0','filter_3.0','filter_4.0','filter_5.0','filter_6.0']  # 女鞋\n",
    "# # filter_suffix_list = ['filter_1.0_2.0','filter_3.0_4.0','filter_5.0_6.0']  # 男鞋\n",
    "# # filter_suffix_list = ['filter_1.0_2.0_3.0','filter_4.0_5.0_6.0']  # 男士&女士春夏上/下装\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def create_empty_file(file_path):\n",
    "#     os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#     pd.DataFrame().to_excel(file_path, index=False)\n",
    "#     print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "# def summarize_with_gpt4(text):\n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "#                 {\"role\": \"user\", \"content\": \"\"\"\n",
    "#                     # Role \n",
    "#                     角色: 电商数据分析师。\n",
    "#                     # Profile \n",
    "#                     简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "#                     ## Background \n",
    "#                     背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，并基于一些前置的定义，找出这些描述信息都是从哪些维度切入的。\n",
    "#                     ## Goals \n",
    "#                     目标: 基于我给到的商品描述信息数据集和前置的维度定义，归纳总结出描述的方向维度，需要特别关注与细化商品本身的卖点特性，并统计这些维度出现的频率。\n",
    "#                     ## Definitions\n",
    "#                     定义：\n",
    "#                     1. 直接展示价格：直接展示价格信息，到手价，预估到手价，会员价等，通常包含上述前缀，¥+具体的价格数字或者具体的价格数字+元。\n",
    "#                     2. 折扣信息：描述商品的折扣，通常包含具体的折扣数字+折。\n",
    "#                     3. 直降信息：描述商品相较原价进行了大幅降价，通常包含直降、立减。\n",
    "#                     4. 满减信息：描述若购买到一定金额，可以在此基础上进行金额优惠，通常包含满+具体的金额+减+具体的金额\n",
    "#                     5. 赠品信息：描述若购买商品则会赠送服务或商品，通常包含赠、送\n",
    "#                     6. 限时：描述商品促销的时间，通常包含活动时间段、活动开始时间、活动结束时间\n",
    "#                     7. 品牌名称：描述商品的品牌名称\n",
    "#                     8. 代言人信息：描述商品的代言人信息\n",
    "#                     9. 价保：价格保护，通常包含价保\n",
    "#                     10. 店铺背书：描述店铺的信息，通常包含旗舰店、自营\n",
    "#                     11. 物流服务：描述商品所包含的物流服务，通常包含物流时效、运费险、物流名称、仓库名称、包邮\n",
    "#                     12. 直接展示价格属于价格信息一级维度，折扣信息、直降信息、满减信息、赠品信息、限时属于价促活动一级维度，品牌名称、代言人信息属于品牌信息一级维度，价保、店铺背书、物流服务属于服务保障一级维度\n",
    "#                     ## Constrains \n",
    "#                     约束条件: 1、时刻保持自己是电商数据分析师的角色，2、可以进行适当的联想和猜测，3、举例的时候禁止出现\"\"，4、统计频率的时候请仔细仔细再仔细，5、若识别到的内容不在上述定义的维度中，可自行命名并统计，请不要忽视未被定义的维度，特别是关于商品本身的卖点信息描述\n",
    "#                     ## Tone \n",
    "#                     语气风格: 正式的，客观的，科学的。\n",
    "#                     ## Skills \n",
    "#                     技能: 1、你有出色的文本理解能力，能够理解输入数据的含义 2、你有出色的归纳总结能力，能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力，能够精确的统计出各个维度出现的频次。\n",
    "#                     ## OutputFormat \n",
    "#                     输出格式:以文字方式输出，一级维度，一级维度下具体内容和举例和频次，输出顺序按照价格信息、价促活动、品牌信息、服务保障、商品卖点进行输出，商品卖点为未定义维度，请你依照自己的知识库信息进行汇总输出，需要特别注意，是关于商品本身的描述，输出格式为1.价格信息 总频次 直接展示价格 频次 举例 以此类推,注意输出要精简，减少不必要的换行\n",
    "#                         \"\"\"}\n",
    "#             ],\n",
    "#         )\n",
    "#         return response.choices[0].message.content.strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in summarization: {str(e)}\")\n",
    "#         return \"\"\n",
    "\n",
    "# # 遍历 x, y 和 filter_suffix 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         for filter_suffix in filter_suffix_list:\n",
    "#             print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "            \n",
    "#             # 读取Excel文件\n",
    "#             file_path = f'D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_word_classification-{x}_{y}_{filter_suffix}.xlsx'\n",
    "#             if not os.path.exists(file_path):\n",
    "#                 print(f\"File not found: {file_path}\")\n",
    "#                 create_empty_file(file_path)\n",
    "#                 continue\n",
    "            \n",
    "#             try:\n",
    "#                 df = pd.read_excel(file_path)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "#                 create_empty_file(file_path)\n",
    "#                 continue\n",
    "\n",
    "#             if df.empty:\n",
    "#                 print(f\"Empty DataFrame for {file_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             # 确保 'text' 列中的所有值都是字符串\n",
    "#             df['text'] = df['text'].astype(str)\n",
    "            \n",
    "#             # 根据 filter_suffix 筛选数据\n",
    "#             filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "#             df = df[df['最终分层'].isin(filter_values)]\n",
    "            \n",
    "#             df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "#             df_grouped = df.groupby(['Height_Category'])\n",
    "            \n",
    "#             # 遍历每个分组，合并文本并进行总结\n",
    "#             summaries = []\n",
    "            \n",
    "#             # 遍历每个分组\n",
    "#             for (height_category), group in tqdm(df_grouped):\n",
    "#                 # 合并该组的所有文本\n",
    "#                 all_text = \" \".join(group['text'].dropna())\n",
    "                \n",
    "#                 # 使用 GPT-4 进行总结\n",
    "#                 summary = summarize_with_gpt4(all_text)\n",
    "                \n",
    "#                 # 将结果添加到列表中\n",
    "#                 summaries.append({\n",
    "#                     'Height_Category': height_category,\n",
    "#                     'text': all_text,\n",
    "#                     'summary': summary\n",
    "#                 })\n",
    "            \n",
    "#             # 创建一个新的DataFrame来存储结果\n",
    "#             result_df = pd.DataFrame(summaries)\n",
    "            \n",
    "#             # 保存结果到Excel文件，包含 filter_suffix 在文件名中\n",
    "#             output_file = f\"D://code//data//{path}//{z}//{x}//grounding_output//{y}//50%_{x}_{y}_{filter_suffix}_文本分类总结-new.xlsx\"\n",
    "#             result_df.to_excel(output_file, index=False)\n",
    "#             print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# print(\"All processing completed.\")\n",
    "\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
