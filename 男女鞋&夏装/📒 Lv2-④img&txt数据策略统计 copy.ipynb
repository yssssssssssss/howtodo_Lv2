{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1 - 对于图片主体的box数量统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\12066\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6908\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6909\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6910\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6911\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6912\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6913\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\9783\\grounding_output\\grounding_results_processed.xlsx\n",
      "所有文件夹处理完成\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "z = '男鞋_from_0501'\n",
    "\n",
    "def process_excel(input_file, output_file):\n",
    "    # 读取Excel文件\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # 定义一个函数来移除后缀\n",
    "    def remove_suffix(name):\n",
    "        return re.sub(r'_\\d+$', '', name)\n",
    "\n",
    "    # 应用函数到'Image Name'列\n",
    "    df['Image Name'] = df['Image Name'].apply(remove_suffix)\n",
    "\n",
    "    # 计算每个Image Name的出现次数\n",
    "    name_counts = df['Image Name'].value_counts()\n",
    "\n",
    "    # 创建一个新的'box_no'列，并填充对应的计数\n",
    "    df['box_no'] = df['Image Name'].map(name_counts)\n",
    "\n",
    "    # 保存修改后的DataFrame到新的Excel文件\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"处理完成，结果已保存到 {output_file}\")\n",
    "\n",
    "def process_all_folders(base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'grounding_output' in dirs:\n",
    "            grounding_output_path = os.path.join(root, 'grounding_output')\n",
    "            input_file = os.path.join(grounding_output_path, 'grounding_results.xlsx')\n",
    "            if os.path.exists(input_file):\n",
    "                output_file = os.path.join(grounding_output_path, 'grounding_results_processed.xlsx')\n",
    "                process_excel(input_file, output_file)\n",
    "\n",
    "# 设置基础路径\n",
    "base_path = f'D://code//data//Lv2期结论//{z}'\n",
    "\n",
    "# 处理所有文件夹\n",
    "process_all_folders(base_path)\n",
    "\n",
    "print(\"所有文件夹处理完成\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\12066\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6908\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6909\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6910\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6911\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6912\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\6913\\grounding_output\\grounding_results_processed.xlsx\n",
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501\\9783\\grounding_output\\grounding_results_processed.xlsx\n",
      "所有文件夹处理完成\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# z = '女士春夏下装_from_0501'\n",
    "\n",
    "csv_file_path = f'D://code//data//Lv2期结论//{z}//{z}.csv'\n",
    "brand_path = f'D://code//data//Lv2期结论//{z}//男鞋品牌分层.xlsx'\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(csv_file_path)\n",
    "df_brand = pd.read_excel(brand_path)\n",
    "\n",
    "# 合并df和df_brand数据\n",
    "df = pd.merge(df, df_brand, on='main_brand_code', how='left')\n",
    "\n",
    "# # 定义筛选条件\n",
    "filter_layers = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
    "# # filter_layers = [4.0, 5.0, 6.0]\n",
    "# filter_layers = [2.0]\n",
    "\n",
    "\n",
    "# 筛选数据\n",
    "filtered_df = df[df['最终分层'].isin(filter_layers)]\n",
    "\n",
    "def extract_matching_part(img_url):\n",
    "    if pd.isna(img_url):\n",
    "        return None\n",
    "    img_url = img_url.split('?')[0]\n",
    "    img_url = os.path.splitext(img_url)[0]\n",
    "    parts = img_url.split('/')\n",
    "    if len(parts) >= 2:\n",
    "        return f\"{parts[-2]}_{parts[-1]}\"\n",
    "    return None\n",
    "\n",
    "filtered_df['matching_part'] = filtered_df['img_url'].apply(extract_matching_part)\n",
    "filtered_df = filtered_df[['matching_part', 'main_brand_code', '最终分层']]\n",
    "\n",
    "def process_excel(input_file, filtered_df):\n",
    "    # 读取Excel文件\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # 定义一个函数来移除后缀\n",
    "    def remove_suffix(name):\n",
    "        return re.sub(r'_\\d+$', '', name)\n",
    "\n",
    "    # 应用函数到'Image Name'列\n",
    "    df['Image Name'] = df['Image Name'].apply(remove_suffix)\n",
    "\n",
    "    # 计算每个Image Name的出现次数\n",
    "    name_counts = df['Image Name'].value_counts()\n",
    "\n",
    "    # 创建一个新的'box_no'列，并填充对应的计数\n",
    "    df['box_no'] = df['Image Name'].map(name_counts)\n",
    "\n",
    "    # 将filtered_df中的数据添加到df中\n",
    "    df = pd.merge(df, filtered_df, left_on='Image Name', right_on='matching_part', how='left')\n",
    "\n",
    "    # 删除matching_part列，因为它与Image Name重复\n",
    "    df = df.drop(columns=['matching_part'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_all_folders(base_path, filtered_df):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'grounding_output' in dirs:\n",
    "            grounding_output_path = os.path.join(root, 'grounding_output')\n",
    "            input_file = os.path.join(grounding_output_path, 'grounding_results.xlsx')\n",
    "            if os.path.exists(input_file):\n",
    "                processed_df = process_excel(input_file, filtered_df)\n",
    "                output_file = os.path.join(grounding_output_path, 'grounding_results_processed.xlsx')\n",
    "                processed_df.to_excel(output_file, index=False)\n",
    "                print(f\"处理完成，结果已保存到 {output_file}\")\n",
    "\n",
    "# 设置基础路径\n",
    "base_path = f'D://code//data//Lv2期结论//{z}'\n",
    "\n",
    "# 处理所有文件夹\n",
    "process_all_folders(base_path, filtered_df)\n",
    "\n",
    "print(\"所有文件夹处理完成\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2 - 文本框识别, 并合并相邻的文本框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//男鞋_from_0501\\12066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 12066: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//男鞋_from_0501\\6908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 6908: 100%|██████████| 582/582 [03:18<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//男鞋_from_0501\\6909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 6909: 100%|██████████| 721/721 [03:52<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//男鞋_from_0501\\6910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 6910: 100%|██████████| 232/232 [01:28<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//男鞋_from_0501\\6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 6911:  34%|███▍      | 120/349 [00:55<01:17,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No text detected in the image: D://code//data//Lv2期结论//男鞋_from_0501\\6911\\grounding_output\\txt\\660621ddFcc5803dc_2476bf1c759fdfbe.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 6911: 100%|██████████| 349/349 [02:33<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//男鞋_from_0501\\6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 6912: 100%|██████████| 843/843 [12:35<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//男鞋_from_0501\\6913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 6913: 100%|██████████| 680/680 [08:48<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件夹: D://code//data//Lv2期结论//男鞋_from_0501\\9783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in 9783: 0it [00:00, ?it/s]\n",
      "Analyzing text: 100%|██████████| 35457/35457 [00:06<00:00, 5185.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成\n",
      "完成时间: 2024-10-27 18:15:13\n"
     ]
    }
   ],
   "source": [
    "# 修改后的代码, 先从图片中识别出文本, 然后分两步\n",
    "# ① 对文本框进行阈值下的合并; 同时也保留原文本框\n",
    "# ② 对文本进行高度和关键词的分类\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from paddleocr import PaddleOCR\n",
    "from PIL import Image\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "# z = '女士春夏上装_from_0501'\n",
    "\n",
    "\n",
    "# 设置输入和输出路径\n",
    "input_folder_path = f'D://code//data//Lv2期结论//{z}'\n",
    "output_file_path = f'D://code//data//Lv2期结论//{z}//txt_info.xlsx'\n",
    "\n",
    "# 加载 OCR 模型\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", show_log=False)\n",
    "\n",
    "def calculate_shortest_distance(point_a, points_bcd):\n",
    "    shortest_distance = float('inf')\n",
    "    for point_bcd in points_bcd:\n",
    "        distance = ((point_bcd[0] - point_a[0]) ** 2 + (point_bcd[1] - point_a[1]) ** 2) ** 0.5\n",
    "        if distance < shortest_distance:\n",
    "            shortest_distance = distance\n",
    "    return shortest_distance\n",
    "\n",
    "def merge_text_boxes(img_path, style):\n",
    "    result = ocr.ocr(img_path, cls=True)\n",
    "    img = Image.open(img_path)\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    if not result or not result[0]:\n",
    "        print(f\"No text detected in the image: {img_path}\")\n",
    "        return None, None\n",
    "\n",
    "    rectangles_with_text = result[0]\n",
    "\n",
    "    original_text_box_info = []\n",
    "    for rectangle in rectangles_with_text:\n",
    "        points = rectangle[0]\n",
    "        original_text_box_info.append({\n",
    "            'File Name': os.path.basename(img_path),\n",
    "            'Style': style,\n",
    "            'x1': points[0][0],\n",
    "            'y1': points[0][1],\n",
    "            'x2': points[2][0],\n",
    "            'y2': points[2][1],\n",
    "            'text': rectangle[1][0]\n",
    "        })\n",
    "\n",
    "    merged_text_boxes = []\n",
    "\n",
    "    for index, row in pd.DataFrame(original_text_box_info).iterrows():\n",
    "        if not merged_text_boxes:\n",
    "            merged_text_boxes.append(row.to_dict())\n",
    "        else:\n",
    "            last_merged_box = merged_text_boxes[-1]\n",
    "\n",
    "            if calculate_shortest_distance((row['x1'], row['y1']), [(last_merged_box['x1'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y2']), (last_merged_box['x1'], last_merged_box['y2'])]) < 100:\n",
    "                last_merged_box['text'] += ' ' + row['text']\n",
    "                last_merged_box['x1'] = min(last_merged_box['x1'], row['x1'])\n",
    "                last_merged_box['y1'] = min(last_merged_box['y1'], row['y1'])\n",
    "                last_merged_box['x2'] = max(last_merged_box['x2'], row['x2'])\n",
    "                last_merged_box['y2'] = max(last_merged_box['y2'], row['y2'])\n",
    "            else:\n",
    "                merged_text_boxes.append(row.to_dict())\n",
    "\n",
    "    original_text_box_df = pd.DataFrame(original_text_box_info)\n",
    "    merged_text_box_df = pd.DataFrame(merged_text_boxes)\n",
    "\n",
    "    for i, box in original_text_box_df.iterrows():\n",
    "        if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "            region = '上半'\n",
    "        elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "            region = '下半'\n",
    "        elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "            region = '左半'\n",
    "        else:\n",
    "            region = '右半'\n",
    "        original_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "        box_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "        box_per = box_area / (img_width * img_height)\n",
    "        original_text_box_df.at[i, 'txt_Area'] = box_area\n",
    "        original_text_box_df.at[i, 'txt_Per'] = box_per\n",
    "\n",
    "    for i, box in merged_text_box_df.iterrows():\n",
    "        if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "            region = '上半'\n",
    "        elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "            region = '下半'\n",
    "        elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "            region = '左半'\n",
    "        else:\n",
    "            region = '右半'\n",
    "        merged_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "        merge_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "        merge_per = merge_area / (img_width * img_height)\n",
    "        merged_text_box_df.at[i, 'Area'] = merge_area\n",
    "        merged_text_box_df.at[i, 'Per'] = merge_per\n",
    "\n",
    "    return merged_text_box_df, original_text_box_df\n",
    "\n",
    "keyword_groups = {\n",
    "    '通用': ['以旧换新', '只换不修', '包邮', '无理由退', '先用后付', '京东白条', '期免息', '送货上门', '保修'],\n",
    "    '价保': ['价保', '保价'],\n",
    "    '纯价格': ['¥', '夫', '￥', r'\\b价\\b', '到手价', '活动价'],\n",
    "    '直降': ['立减', '直降', '降', '立省', r'^(?!.*升降).*$', r'^(?!.*降温).*$', r'^(?!.*降噪).*$', r'^(?!.*降低).*$'],\n",
    "    '折扣': ['折', r'^(?!.*折叠).*$', r'^(?!.*翻折).*$'],\n",
    "    '满减': [r'.*满.*减.*', r'.*满.*-.*', r'.*满.*免.*'],\n",
    "    '用券': ['用券', '领券', '券'],\n",
    "    '返券': ['返券', '京豆', '返现', r'.*返.*E卡.*', r'.*返.*红包.*'],\n",
    "    '限时': ['.*小时$', '.*天$', '时间', 'time', 'TIME', '限时', r'.*月.*日.*', r'.*日.*点.*', r'.*:.*', r'.*:.*', r'.*：.*', r'\\b\\d{1,2}\\.\\d{1,2}-\\d{1,2}\\b'],\n",
    "    'xx元任选': [r'.*元.*件.*'],\n",
    "    '赠品': [r'.*满.*赠.*', r'.*满.*送.*', '送', '抽', '奖励', '赠', r'^(?!.*送货).*$', r'^(?!.*送礼).*$', r'^(?!.*送装).*$', r'^(?!.*配送).*$', r'^(?!.*送达).*$'],\n",
    "    '节日名称': ['节', '出游季', '购物季', '毕业季', '开学季', '黑五', '周年庆', '儿童节', '父亲节', '端午节', '七夕', '中秋节', '国庆', '万圣节', '感恩节', '元旦', '圣诞', '情人节', '春节', '元宵节', '38节', '3.8节', '清明节', '母亲节', '618', '购物季', '开学季', '11.11', '黑五', '12.12', '女神节', '出游季', '放价季', '吃货节', '家装节'],\n",
    "    '是否限购': ['限购', '限量']\n",
    "}\n",
    "\n",
    "def keyword_analysis(text):\n",
    "    results = {}\n",
    "    for key, words in keyword_groups.items():\n",
    "        results[key] = any(re.search(word, text) for word in words)\n",
    "    return results\n",
    "\n",
    "def height_analysis(x1, y1, x2, y2):\n",
    "    height = abs(y2 - y1)\n",
    "    return height\n",
    "\n",
    "def process_images(folder_path, subfolder_name):\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        if 'grounding_output' in root and ('price' in root or 'txt' in root):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                    image_files.append(os.path.join(root, file))\n",
    "    \n",
    "    combined_results = []\n",
    "    for img_path in tqdm(image_files, desc=f'Processing images in {subfolder_name}'):\n",
    "        style = 'price' if 'price' in img_path else 'txt'\n",
    "        merged_df, original_df = merge_text_boxes(img_path, style)\n",
    "        if merged_df is not None and original_df is not None:\n",
    "            merged_df['Subfolder'] = subfolder_name\n",
    "            original_df['Subfolder'] = subfolder_name\n",
    "            combined_results.append({\n",
    "                'original': original_df,\n",
    "                'merged': merged_df\n",
    "            })\n",
    "    \n",
    "    return combined_results\n",
    "\n",
    "# 主程序\n",
    "all_results = []\n",
    "for folder in os.listdir(input_folder_path):\n",
    "    if folder.isdigit():\n",
    "        folder_path = os.path.join(input_folder_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"正在处理文件夹: {folder_path}\")\n",
    "            results = process_images(folder_path, folder)\n",
    "            all_results.extend(results)\n",
    "\n",
    "final_combined_data = []\n",
    "for result in all_results:\n",
    "    result['original']['Type'] = 'Original'\n",
    "    result['merged']['Type'] = 'Merged'\n",
    "    combined = pd.concat([result['original'], result['merged']], ignore_index=True)\n",
    "    final_combined_data.append(combined)\n",
    "\n",
    "final_combined_df = pd.concat(final_combined_data, ignore_index=True)\n",
    "final_combined_df.sort_values(by=['Subfolder', 'File Name', 'Type'], inplace=True)\n",
    "\n",
    "for index, row in tqdm(final_combined_df.iterrows(), total=final_combined_df.shape[0], desc=\"Analyzing text\"):\n",
    "    keyword_results = keyword_analysis(row['text'])\n",
    "    for key, value in keyword_results.items():\n",
    "        final_combined_df.at[index, key] = value\n",
    "    \n",
    "    height = height_analysis(row['x1'], row['y1'], row['x2'], row['y2'])\n",
    "    final_combined_df.at[index, 'Height'] = height\n",
    "    final_combined_df.at[index, 'Height_Category'] = (\n",
    "        'Height_<18' if height < 18 else\n",
    "        'Height_18-29' if 18 <= height < 29 else\n",
    "        'Height_29-38' if 29 <= height < 38 else\n",
    "        'Height_>38'\n",
    "    )\n",
    "\n",
    "final_combined_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print('处理完成')\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成时间: 2024-10-27 18:15:39\n",
      "完成时间: 2024-10-27 18:15:39\n",
      "完成时间: 2024-10-27 18:15:39\n"
     ]
    }
   ],
   "source": [
    "# 在现有txt_info的基础上, 拼接品牌分类\n",
    "# 在现有txt_info的基础上, 拼接品牌分类\n",
    "# 在现有txt_info的基础上, 拼接品牌分类\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from paddleocr import PaddleOCR\n",
    "from PIL import Image\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "# z = '女鞋_from_0501'\n",
    "\n",
    "in_file_path = f'D://code//data//Lv2期结论//{z}//txt_info.xlsx'\n",
    "output_file_path = f'D://code//data//Lv2期结论//{z}//txt_info-1.xlsx'\n",
    "\n",
    "df = pd.read_excel(in_file_path)\n",
    "\n",
    "# 删除df的File Name列中的.jpg扩展名\n",
    "df['File Name'] = df['File Name'].str.replace('.jpg', '')\n",
    "\n",
    "filtered_df = filtered_df.drop_duplicates()\n",
    "\n",
    "# 删除filtered_df的matching_part列中的.jpg扩展名\n",
    "filtered_df['matching_part'] = filtered_df['matching_part'].str.replace('.jpg', '')\n",
    "\n",
    "df1 = pd.merge(df, filtered_df, left_on='File Name', right_on='matching_part', how='left')\n",
    "\n",
    "df1.to_excel(output_file_path)\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 自动文件名进行处理的方法, 但是还没有验证\n",
    "\n",
    "\n",
    "# import os\n",
    "# import glob\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# from paddleocr import PaddleOCR\n",
    "# from PIL import Image\n",
    "# import math\n",
    "# import re\n",
    "\n",
    "# # 设置输入和输出路径\n",
    "# input_folder_path = 'D://code//data//Lv2期结论//京喜_from_0501//筛选'\n",
    "# output_file_path = 'D://code//data//Lv2期结论//京喜_from_0501//筛选//txt_info.xlsx'\n",
    "\n",
    "# # 加载 OCR 模型\n",
    "# ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", show_log=False)\n",
    "\n",
    "# def calculate_shortest_distance(point_a, points_bcd):\n",
    "#     shortest_distance = float('inf')\n",
    "#     for point_bcd in points_bcd:\n",
    "#         distance = ((point_bcd[0] - point_a[0]) ** 2 + (point_bcd[1] - point_a[1]) ** 2) ** 0.5\n",
    "#         if distance < shortest_distance:\n",
    "#             shortest_distance = distance\n",
    "#     return shortest_distance\n",
    "\n",
    "# def merge_text_boxes(img_path, style):\n",
    "#     result = ocr.ocr(img_path, cls=True)\n",
    "#     img = Image.open(img_path)\n",
    "#     img_width, img_height = img.size\n",
    "\n",
    "#     if not result or not result[0]:\n",
    "#         print(f\"No text detected in the image: {img_path}\")\n",
    "#         return None, None\n",
    "\n",
    "#     rectangles_with_text = result[0]\n",
    "\n",
    "#     # 提取文件名并处理\n",
    "#     file_name = os.path.basename(img_path)\n",
    "#     # 移除扩展名\n",
    "#     file_name = os.path.splitext(file_name)[0]\n",
    "#     # 移除可能的 'txt_' 或 'price_' 前缀\n",
    "#     file_name = re.sub(r'^(txt_|price_)', '', file_name)\n",
    "#     # 确保文件名格式为 XXXXXXXXXXXXXXXX_XXXXXXXXXXXXXXXX\n",
    "#     if '_' in file_name:\n",
    "#         parts = file_name.split('_')\n",
    "#         if len(parts) >= 2:\n",
    "#             file_name = f\"{parts[-2]}_{parts[-1]}\"\n",
    "\n",
    "#     original_text_box_info = []\n",
    "#     for rectangle in rectangles_with_text:\n",
    "#         points = rectangle[0]\n",
    "#         original_text_box_info.append({\n",
    "#             'File Name': file_name,  # 使用处理后的文件名\n",
    "#             'Style': style,\n",
    "#             'x1': points[0][0],\n",
    "#             'y1': points[0][1],\n",
    "#             'x2': points[2][0],\n",
    "#             'y2': points[2][1],\n",
    "#             'text': rectangle[1][0]\n",
    "#         })\n",
    "\n",
    "#     merged_text_boxes = []\n",
    "\n",
    "#     for index, row in pd.DataFrame(original_text_box_info).iterrows():\n",
    "#         if not merged_text_boxes:\n",
    "#             merged_text_boxes.append(row.to_dict())\n",
    "#         else:\n",
    "#             last_merged_box = merged_text_boxes[-1]\n",
    "\n",
    "#             if calculate_shortest_distance((row['x1'], row['y1']), [(last_merged_box['x1'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y1']), (last_merged_box['x2'], last_merged_box['y2']), (last_merged_box['x1'], last_merged_box['y2'])]) < 100:\n",
    "#                 last_merged_box['text'] += ' ' + row['text']\n",
    "#                 last_merged_box['x1'] = min(last_merged_box['x1'], row['x1'])\n",
    "#                 last_merged_box['y1'] = min(last_merged_box['y1'], row['y1'])\n",
    "#                 last_merged_box['x2'] = max(last_merged_box['x2'], row['x2'])\n",
    "#                 last_merged_box['y2'] = max(last_merged_box['y2'], row['y2'])\n",
    "#             else:\n",
    "#                 merged_text_boxes.append(row.to_dict())\n",
    "\n",
    "#     original_text_box_df = pd.DataFrame(original_text_box_info)\n",
    "#     merged_text_box_df = pd.DataFrame(merged_text_boxes)\n",
    "\n",
    "#     for i, box in original_text_box_df.iterrows():\n",
    "#         if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "#             region = '上半'\n",
    "#         elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "#             region = '下半'\n",
    "#         elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "#             region = '左半'\n",
    "#         else:\n",
    "#             region = '右半'\n",
    "#         original_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "#         box_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "#         box_per = box_area / (img_width * img_height)\n",
    "#         original_text_box_df.at[i, 'txt_Area'] = box_area\n",
    "#         original_text_box_df.at[i, 'txt_Per'] = box_per\n",
    "\n",
    "#     for i, box in merged_text_box_df.iterrows():\n",
    "#         if box['y1'] < img_height / 2 and box['y2'] < img_height / 2:\n",
    "#             region = '上半'\n",
    "#         elif box['y1'] >= img_height / 2 and box['y2'] >= img_height / 2:\n",
    "#             region = '下半'\n",
    "#         elif box['x1'] < img_width / 2 and box['x2'] < img_width / 2:\n",
    "#             region = '左半'\n",
    "#         else:\n",
    "#             region = '右半'\n",
    "#         merged_text_box_df.at[i, 'Region'] = region\n",
    "\n",
    "#         merge_area = (box['x2'] - box['x1']) * (box['y2'] - box['y1'])\n",
    "#         merge_per = merge_area / (img_width * img_height)\n",
    "#         merged_text_box_df.at[i, 'Area'] = merge_area\n",
    "#         merged_text_box_df.at[i, 'Per'] = merge_per\n",
    "\n",
    "#     return merged_text_box_df, original_text_box_df\n",
    "\n",
    "# keyword_groups = {\n",
    "#     '通用': ['以旧换新', '只换不修', '包邮', '无理由退', '先用后付', '京东白条', '期免息', '送货上门', '保修'],\n",
    "#     '价保': ['价保', '保价'],\n",
    "#     '纯价格': ['¥', '夫', '￥', r'\\b价\\b', '到手价', '活动价'],\n",
    "#     '直降': ['立减', '直降', '降', '立省', r'^(?!.*升降).*$', r'^(?!.*降温).*$', r'^(?!.*降噪).*$', r'^(?!.*降低).*$'],\n",
    "#     '折扣': ['折', r'^(?!.*折叠).*$', r'^(?!.*翻折).*$'],\n",
    "#     '满减': [r'.*满.*减.*', r'.*满.*-.*', r'.*满.*免.*'],\n",
    "#     '用券': ['用券', '领券', '券'],\n",
    "#     '返券': ['返券', '京豆', '返现', r'.*返.*E卡.*', r'.*返.*红包.*'],\n",
    "#     '限时': ['.*小时$', '.*天$', '时间', 'time', 'TIME', '限时', r'.*月.*日.*', r'.*日.*点.*', r'.*:.*', r'.*:.*', r'.*：.*', r'\\b\\d{1,2}\\.\\d{1,2}-\\d{1,2}\\b'],\n",
    "#     'xx元任选': [r'.*元.*件.*'],\n",
    "#     '赠品': [r'.*满.*赠.*', r'.*满.*送.*', '送', '抽', '奖励', '赠', r'^(?!.*送货).*$', r'^(?!.*送礼).*$', r'^(?!.*送装).*$', r'^(?!.*配送).*$', r'^(?!.*送达).*$'],\n",
    "#     '节日名称': ['节', '出游季', '购物季', '毕业季', '开学季', '黑五', '周年庆', '儿童节', '父亲节', '端午节', '七夕', '中秋节', '国庆', '万圣节', '感恩节', '元旦', '圣诞', '情人节', '春节', '元宵节', '38节', '3.8节', '清明节', '母亲节', '618', '购物季', '开学季', '11.11', '黑五', '12.12', '女神节', '出游季', '放价季', '吃货节', '家装节'],\n",
    "#     '是否限购': ['限购', '限量']\n",
    "# }\n",
    "\n",
    "# def keyword_analysis(text):\n",
    "#     results = {}\n",
    "#     for key, words in keyword_groups.items():\n",
    "#         results[key] = any(re.search(word, text) for word in words)\n",
    "#     return results\n",
    "\n",
    "# def height_analysis(x1, y1, x2, y2):\n",
    "#     height = abs(y2 - y1)\n",
    "#     return height\n",
    "\n",
    "# def process_images(folder_path, subfolder_name):\n",
    "#     image_files = []\n",
    "#     for root, dirs, files in os.walk(folder_path):\n",
    "#         if 'grounding_output' in root and ('price' in root or 'txt' in root):\n",
    "#             for file in files:\n",
    "#                 if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "#                     image_files.append(os.path.join(root, file))\n",
    "    \n",
    "#     combined_results = []\n",
    "#     for img_path in tqdm(image_files, desc=f'Processing images in {subfolder_name}'):\n",
    "#         style = 'price' if 'price' in img_path else 'txt'\n",
    "#         merged_df, original_df = merge_text_boxes(img_path, style)\n",
    "#         if merged_df is not None and original_df is not None:\n",
    "#             merged_df['Subfolder'] = subfolder_name\n",
    "#             original_df['Subfolder'] = subfolder_name\n",
    "#             combined_results.append({\n",
    "#                 'original': original_df,\n",
    "#                 'merged': merged_df\n",
    "#             })\n",
    "    \n",
    "#     return combined_results\n",
    "\n",
    "# # 主程序\n",
    "# all_results = []\n",
    "# for folder in os.listdir(input_folder_path):\n",
    "#     if folder.isdigit():\n",
    "#         folder_path = os.path.join(input_folder_path, folder)\n",
    "#         if os.path.isdir(folder_path):\n",
    "#             print(f\"正在处理文件夹: {folder_path}\")\n",
    "#             results = process_images(folder_path, folder)\n",
    "#             all_results.extend(results)\n",
    "\n",
    "# final_combined_data = []\n",
    "# for result in all_results:\n",
    "#     result['original']['Type'] = 'Original'\n",
    "#     result['merged']['Type'] = 'Merged'\n",
    "#     combined = pd.concat([result['original'], result['merged']], ignore_index=True)\n",
    "#     final_combined_data.append(combined)\n",
    "\n",
    "# final_combined_df = pd.concat(final_combined_data, ignore_index=True)\n",
    "# final_combined_df.sort_values(by=['Subfolder', 'File Name', 'Type'], inplace=True)\n",
    "\n",
    "# for index, row in tqdm(final_combined_df.iterrows(), total=final_combined_df.shape[0], desc=\"Analyzing text\"):\n",
    "#     keyword_results = keyword_analysis(row['text'])\n",
    "#     for key, value in keyword_results.items():\n",
    "#         final_combined_df.at[index, key] = value\n",
    "    \n",
    "#     height = height_analysis(row['x1'], row['y1'], row['x2'], row['y2'])\n",
    "#     final_combined_df.at[index, 'Height'] = height\n",
    "#     final_combined_df.at[index, 'Height_Category'] = (\n",
    "#         'Height_<18' if height < 18 else\n",
    "#         'Height_18-29' if 18 <= height < 29 else\n",
    "#         'Height_29-38' if 29 <= height < 38 else\n",
    "#         'Height_>38'\n",
    "#     )\n",
    "\n",
    "# final_combined_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "# print('处理完成')\n",
    "\n",
    "# import datetime\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3 - 将分散在各个grounding_output文件夹中的grounding_results_processed.xlsx合并起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并完成，结果保存在: D://code//data//Lv2期结论//男鞋_from_0501\\img_info.xlsx\n",
      "完成时间: 2024-10-27 18:25:11\n",
      "完成时间: 2024-10-27 18:25:11\n",
      "完成时间: 2024-10-27 18:25:11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "def merge_excel_files(base_path):\n",
    "    # 用于存储所有数据框的列表\n",
    "    all_dataframes = []\n",
    "\n",
    "    # 遍历基础路径下的所有文件夹\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if 'grounding_output' in dirs:\n",
    "            grounding_output_path = os.path.join(root, 'grounding_output')\n",
    "            excel_file = os.path.join(grounding_output_path, 'grounding_results_processed.xlsx')\n",
    "            \n",
    "            if os.path.exists(excel_file):\n",
    "                # 读取Excel文件\n",
    "                df = pd.read_excel(excel_file)\n",
    "                \n",
    "                # 添加新列，值为当前子文件夹的名称\n",
    "                subfolder_name = os.path.basename(os.path.dirname(grounding_output_path))\n",
    "                df['Subfolder'] = subfolder_name\n",
    "                \n",
    "                # 将数据框添加到列表中\n",
    "                all_dataframes.append(df)\n",
    "\n",
    "    # 合并所有数据框\n",
    "    if all_dataframes:\n",
    "        merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        \n",
    "        # 保存合并后的数据框到新的Excel文件\n",
    "        output_file = os.path.join(base_path, 'img_info.xlsx')\n",
    "        merged_df.to_excel(output_file, index=False)\n",
    "        print(f\"合并完成，结果保存在: {output_file}\")\n",
    "    else:\n",
    "        print(\"没有找到符合条件的Excel文件\")\n",
    "\n",
    "# 使用示例\n",
    "base_path = f\"D://code//data//Lv2期结论//{z}\"\n",
    "merge_excel_files(base_path)\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成时间: 2024-10-27 18:30:51\n",
      "完成时间: 2024-10-27 18:30:51\n",
      "完成时间: 2024-10-27 18:30:51\n"
     ]
    }
   ],
   "source": [
    "# 在img_info后面拼接品牌分类信息\n",
    "\n",
    "\n",
    "# z = '女士春夏上装_from_0501'\n",
    "\n",
    "in_file_path = f'D://code//data//Lv2期结论//{z}//img_info.xlsx'\n",
    "output_file_path = f'D://code//data//Lv2期结论//{z}//img_info-1.xlsx'\n",
    "\n",
    "df = pd.read_excel(in_file_path)\n",
    "\n",
    "# 删除df的File Name列中的.jpg扩展名\n",
    "df['Image Name'] = df['Image Name'].str.replace('.jpg', '')\n",
    "\n",
    "filtered_df = filtered_df.drop_duplicates()\n",
    "\n",
    "# 删除filtered_df的matching_part列中的.jpg扩展名\n",
    "filtered_df['matching_part'] = filtered_df['matching_part'].str.replace('.jpg', '')\n",
    "\n",
    "df1 = pd.merge(df, filtered_df, left_on='Image Name', right_on='matching_part', how='left')\n",
    "df1 = df1.drop_duplicates()\n",
    "\n",
    "df1.to_excel(output_file_path)\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "print(f\"完成时间: {formatted_time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step-4 将布局和ctr参数进行合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel files merged successfully!\n",
      "Merged file saved to D://code//data//Lv2期结论//男鞋_from_0501//merged_info_ctr-1.xlsx\n",
      "2024-10-27 18:32:57\n",
      "2024-10-27 18:32:57\n",
      "2024-10-27 18:32:57\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "\n",
    "# z = '女士春夏上装_from_0501'\n",
    "\n",
    "\n",
    "def merge_excel_files(txt_box_info_file, img_box_info_file, output_file):\n",
    "    # 读取 txt_box_info 文件\n",
    "    txt_df = pd.read_excel(txt_box_info_file)\n",
    "    txt_df = txt_df.rename(columns={'File Name':'Image Name', 'x1': 'txt_x1', 'y1': 'txt_y1', 'x2': 'txt_x2', 'y2': 'txt_y2',})\n",
    "\n",
    "    # 读取 img_box_info 文件\n",
    "    img_df = pd.read_excel(img_box_info_file)\n",
    "\n",
    "    # 重命名列\n",
    "    img_df = img_df.rename(columns={'x1': 'img_x1', 'y1': 'img_y1', 'x3': 'img_x2', 'y3':'img_y2', 'Subfolder':'Style'})\n",
    "    img_df = img_df.loc[:, ['Image Name', 'Style','img_x1', 'img_y1', 'img_x2', 'img_y2', 'box_no', ]]\n",
    "\n",
    "    img_df = img_df.drop_duplicates()\n",
    "    \n",
    "    # 合并两个 DataFrame，使用 txt_box_info 的表头作为准\n",
    "    merged_df = pd.concat([txt_df, img_df], ignore_index=True)\n",
    "    \n",
    "    # 将 img_box_info 中缺少的数据设置为空\n",
    "    merged_df = merged_df.fillna(\"\")\n",
    "\n",
    "    # 使用正则表达式删除 .jpg 或 .png 后缀\n",
    "    merged_df['Image Name'] = merged_df['Image Name'].str.replace(r'\\.(?:jpg|png)$', '', regex=True)\n",
    "    \n",
    "    # 将合并后的 DataFrame 写入新的 Excel 文件\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        merged_df.to_excel(writer, index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    txt_box_info_file = f\"D://code//data//Lv2期结论//{z}//txt_info-1.xlsx\"\n",
    "    img_box_info_file = f\"D://code//data//Lv2期结论//{z}//img_info.xlsx\"\n",
    "    output_file = f\"D://code//data//Lv2期结论//{z}//merged_info-1.xlsx\"\n",
    "    \n",
    "    merge_excel_files(txt_box_info_file, img_box_info_file, output_file)\n",
    "    \n",
    "    print(\"Excel files merged successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 定义路径\n",
    "data_1 = f\"D://code//data//Lv2期结论//{z}//merged_info-1.xlsx\"\n",
    "data_2 = f\"D://code//data//Lv2期结论//{z}//{z}.csv\"\n",
    "# data_3 = 'D://code//data//howtodo_from_0401//服饰鞋靴箱包//品类聚类-服饰鞋靴箱包.csv'\n",
    "output_path = f\"D://code//data//Lv2期结论//{z}//merged_info_ctr-1.xlsx\"\n",
    "\n",
    "# 读取df1\n",
    "df1 = pd.read_excel(data_1)\n",
    "\n",
    "# 读取df2\n",
    "df2 = pd.read_csv(data_2)\n",
    "\n",
    "aggregated_data = df2.groupby('img_url').agg({\n",
    "    'cid2': 'first',  # 使用 'first' 函数来选择分组中的第一个值\n",
    "    'cid3': 'first',\n",
    "    'uv': 'sum',\n",
    "    'click_uv': 'sum',\n",
    "    'gmv_cj':'sum',\n",
    "    'sale_qtty_cj':'sum'\n",
    "#     'folder_path': 'first'  # 同样使用 'first' 函数选择第一个值\n",
    "}).reset_index()  # 重置索引\n",
    "\n",
    "df2 = aggregated_data\n",
    "\n",
    "# 计算ctr字段\n",
    "df2['ctr'] = df2['click_uv'] / df2['uv']\n",
    "\n",
    "\n",
    "def extract_filename(x):\n",
    "    # 分割路径，取倒数第二部分和最后一部分（文件名部分）\n",
    "    parts = x.split('/')\n",
    "    return f\"{parts[-2]}_{os.path.splitext(parts[-1])[0]}\"  # 保留原文件扩展名\n",
    "    # return f\"{os.path.splitext(parts[-1])[0]}.jpg\"  # 保留原文件扩展名\n",
    "\n",
    "# 应用函数\n",
    "df2['only_2'] = df2['img_url'].apply(extract_filename)\n",
    "\n",
    "# 初始化结果列表\n",
    "results = []\n",
    "\n",
    "# 遍历df1的每一行\n",
    "for index, row1 in df1.iterrows():\n",
    "    # 查找df2中匹配的行\n",
    "    matching_rows_df2 = df2[df2['only_2'] == row1['Image Name']]\n",
    "    \n",
    "    # 如果没有找到匹配的行，则只添加df1的当前行\n",
    "    if matching_rows_df2.empty:\n",
    "        results.append(row1.to_dict())\n",
    "    else:\n",
    "        # 对于找到的每个匹配行，先添加df1的当前行，然后添加匹配的df2行\n",
    "        results.append(row1.to_dict())\n",
    "        for _, row2 in matching_rows_df2.iterrows():\n",
    "            # 可能需要添加额外的逻辑来处理多个匹配的情况\n",
    "            # 这里假设每个df1的行在df2中最多只有一个匹配\n",
    "            merged_row = {**row1.to_dict(), **row2.to_dict()}\n",
    "            results.append(merged_row)\n",
    "\n",
    "# 将结果列表转换为DataFrame\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "result_df_drop = result_df.dropna(subset=['uv'])\n",
    "\n",
    "# 保存到指定路径\n",
    "result_df_drop.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Merged file saved to {output_path}\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step4 - 将图片按照比例进行分类\n",
    "### x<0.77 / 0.77<x<1.3 / x>1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到 D://code//data//Lv2期结论//男鞋_from_0501//0.77-1.3-1.xlsx\n",
      "2024-10-27 20:09:35\n",
      "2024-10-27 20:09:35\n",
      "2024-10-27 20:09:35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# z = '女士春夏上装_from_0501'\n",
    "\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel(f'D://code//data//Lv2期结论//{z}//img_info-1.xlsx')\n",
    "\n",
    "# # 图片路径前缀\n",
    "# path_to_your_images = f'D://code//data//background_color//服饰鞋靴箱包//{x}//grounding_output//{y}'\n",
    "\n",
    "# # 去掉File Name列中的后缀\n",
    "# df['File Name'] = df['File Name'].str.split('_').str[0]\n",
    "\n",
    "# # 定义一个函数来计算新的矩形框坐标\n",
    "# def calculate_new_coordinates(group):\n",
    "#     x_coords = group['main_box_x'] + group['main_box_width']\n",
    "#     y_coords = group['main_box_y'] + group['main_box_height']\n",
    "    \n",
    "#     min_x = group['main_box_x'].min()\n",
    "#     min_y = group['main_box_y'].min()\n",
    "#     max_x = x_coords.max()\n",
    "#     max_y = y_coords.max()\n",
    "    \n",
    "#     return pd.Series({\n",
    "#         'merge_x1': min_x,\n",
    "#         'merge_y1': min_y,\n",
    "#         'merge_x2': max_x,\n",
    "#         'merge_y2': max_y\n",
    "#     })\n",
    "\n",
    "\n",
    "# 定义一个函数来计算新的矩形框坐标\n",
    "def calculate_new_coordinates(group):\n",
    "\n",
    "    min_x = group['x1'].min()\n",
    "    min_y = group['y1'].min()\n",
    "    max_x = group['x3'].max()\n",
    "    max_y = group['y3'].max()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'merge_x1': min_x,\n",
    "        'merge_y1': min_y,\n",
    "        'merge_x2': max_x,\n",
    "        'merge_y2': max_y\n",
    "    })\n",
    "\n",
    "\n",
    "# 按File Name分组并计算新坐标\n",
    "df = df.groupby('Image Name').apply(calculate_new_coordinates).reset_index()\n",
    "\n",
    "# 计算矩形框的横纵比\n",
    "df['aspect_ratio'] = (df['merge_x2'] - df['merge_x1']) / (df['merge_y2'] - df['merge_y1'])\n",
    "\n",
    "# # 在图片路径前缀下创建新文件夹\n",
    "# os.makedirs(os.path.join(path_to_your_images, '小于0.77'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(path_to_your_images, '0.77到1.3'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(path_to_your_images, '大于1.3'), exist_ok=True)\n",
    "\n",
    "# 定义一个函数来分类图片并复制到相应文件夹\n",
    "def classify_and_copy_image(row):\n",
    "    # image_path = os.path.join(path_to_your_images, f\"{row['File Name']}\")\n",
    "    if row['aspect_ratio'] < 0.77:\n",
    "        # shutil.copy(image_path, os.path.join(path_to_your_images, '小于0.77', f\"{row['File Name']}.jpg\"))\n",
    "        return '小于0.77'\n",
    "    elif 0.77 <= row['aspect_ratio'] <= 1.3:\n",
    "        # shutil.copy(image_path, os.path.join(path_to_your_images, '0.77到1.3', f\"{row['File Name']}.jpg\"))\n",
    "        return '0.77到1.3'\n",
    "    else:\n",
    "        # shutil.copy(image_path, os.path.join(path_to_your_images, '大于1.3', f\"{row['File Name']}.jpg\"))\n",
    "        return '大于1.3'\n",
    "\n",
    "# 应用分类函数并添加结果列\n",
    "df['classification'] = df.apply(classify_and_copy_image, axis=1)\n",
    "\n",
    "# 保存结果到Excel\n",
    "output_file = os.path.join(f'D://code//data//Lv2期结论//{z}//0.77-1.3-1.xlsx')\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"处理完成，结果已保存到 {output_file}\")\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step5 - 文本:布局分类&热力图生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这部分是采用x和y分别输入,比较费人手\n",
    "\n",
    "\n",
    "# # 识别文本框是在3x3网格中，并将图片复制到相应的分类目录中，并保存可视化结果\n",
    "# # 这段代码是包含左上角的,即还是对可能的logo进行了统计\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.patches import Rectangle\n",
    "# import shutil\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# import concurrent.futures\n",
    "# import datetime\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "# x = '9735'\n",
    "# y = 'txt'\n",
    "# z = '男士春夏下装_from_0501'\n",
    "\n",
    "# # 1. 读取和预处理数据\n",
    "# def normalize_coordinates(row):\n",
    "#     width = 616\n",
    "#     height = 616\n",
    "#     row['left_norm'] = max(0, min(row['txt_x1'] / width, 1))\n",
    "#     row['top_norm'] = max(0, min(row['txt_y1'] / height, 1))\n",
    "#     row['right_norm'] = max(0, min(row['txt_x2'] / width, 1))\n",
    "#     row['bottom_norm'] = max(0, min(row['txt_y2'] / height, 1))\n",
    "#     return row\n",
    "\n",
    "# # 2. 绘制矩形和网格\n",
    "# def draw_rectangles(group):\n",
    "#     fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "    \n",
    "#     for _, row in group.iterrows():\n",
    "#         rect = Rectangle((row['left_norm'], 1 - row['bottom_norm']), \n",
    "#                          row['right_norm'] - row['left_norm'], \n",
    "#                          row['bottom_norm'] - row['top_norm'],\n",
    "#                          fill=False, edgecolor='r')\n",
    "#         ax.add_patch(rect)\n",
    "    \n",
    "#     for i in range(3):\n",
    "#         for j in range(3):\n",
    "#             rect = Rectangle((j/3, 1 - (i+1)/3), 1/3, 1/3, fill=False, edgecolor='b')\n",
    "#             ax.add_patch(rect)\n",
    "    \n",
    "#     ax.set_xlim(0, 1)\n",
    "#     ax.set_ylim(0, 1)\n",
    "#     ax.axis('off')\n",
    "#     return fig, ax\n",
    "\n",
    "# # 3. 判断重叠和分类\n",
    "# def check_overlap(rect, grid_cell):\n",
    "#     return not (rect[2] < grid_cell[0] or rect[0] > grid_cell[2] or\n",
    "#                 rect[3] < grid_cell[1] or rect[1] > grid_cell[3])\n",
    "\n",
    "# def classify_image(group):\n",
    "#     overlaps = [0] * 9\n",
    "    \n",
    "#     for _, row in group.iterrows():\n",
    "#         rect = (row['left_norm'], row['top_norm'], row['right_norm'], row['bottom_norm'])\n",
    "#         for i in range(3):\n",
    "#             for j in range(3):\n",
    "#                 grid_cell = (j/3, i/3, (j+1)/3, (i+1)/3)\n",
    "#                 if check_overlap(rect, grid_cell):\n",
    "#                     overlaps[i*3 + j] = 1\n",
    "#     return ''.join(map(str, overlaps))  # 直接使用join方法生成字符串\n",
    "\n",
    "# # 4. 处理单个图像\n",
    "# def process_image(name, group, x, y):\n",
    "#     classification = classify_image(group)\n",
    "    \n",
    "#     # 复制图片\n",
    "#     source = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', name)\n",
    "#     destination = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_classified_images', classification.zfill(9), name)  # 使用zfill方法填充前导零\n",
    "#     os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "#     shutil.copy2(source, destination)\n",
    "    \n",
    "#     # 保存可视化结果\n",
    "#     fig, ax = draw_rectangles(group)\n",
    "#     visualization_name = f\"{name.split('.')[0]}_visualization.png\"  # 保留原文件名的前导零\n",
    "#     fig.savefig(os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_visualizations', visualization_name), bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close(fig)\n",
    "    \n",
    "#     return name, classification\n",
    "\n",
    "\n",
    "# # 主处理函数\n",
    "# def main(x, y):\n",
    "#     # 读取CSV文件\n",
    "#     print(\"读取并预处理数据...\")\n",
    "\n",
    "#     # 在读取df时添加筛选条件\n",
    "#     def filter_by_rectangle(row):\n",
    "#         right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#         if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#             return False\n",
    "#         return True\n",
    "\n",
    "#     df = pd.read_excel(os.path.join(f'D://code//data//Lv2期结论//{z}//merged_info_ctr.xlsx'))\n",
    "#     # df = df[df['img_x1'].isna()]\n",
    "#     # df = df[df['Subfolder'] == 9775]\n",
    "#     # df = df[df['Style'] == 'txt']\n",
    "#     # df = df[df['Type'] == 'Original']\n",
    "#     # df = df.sort_values('ctr', ascending=False)\n",
    "#     # df['File Name1'] = df['File Name1'] + '.jpg'\n",
    "\n",
    "#     # df = df[(df['img_x1'].isna()) & (df['Subfolder'] == 9775) & (df['Style'] == 'txt') & (df['Type'] == 'Original')]\n",
    "#     # df = df.sort_values('ctr', ascending=False)\n",
    "#     # df['File Name1'] = df['File Name1'] + '.jpg'\n",
    "\n",
    "#     df = df[(df['img_x1'].isna()) & (df['Subfolder'] == x) & (df['Style'] == y) & (df['Type'] == 'Original')]\n",
    "#     df = df.sort_values('ctr', ascending=False)\n",
    "#     df['Image Name'] = df['Image Name'] + '.jpg'\n",
    "\n",
    "\n",
    "#     # 应用左上角的筛选条件\n",
    "#     df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "#     rows_to_keep = int(len(df) * 0.5)\n",
    "#     df = df.head(rows_to_keep)\n",
    "\n",
    "#     # 应用normalize_coordinates函数\n",
    "#     df = df.apply(normalize_coordinates, axis=1)\n",
    "\n",
    "#     # 创建输出目录\n",
    "#     classified_images_dir = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', '50%_txt_classified_images')\n",
    "#     visualizations_dir = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', '50%_txt_visualizations')\n",
    "#     os.makedirs(classified_images_dir, exist_ok=True)\n",
    "#     os.makedirs(visualizations_dir, exist_ok=True)\n",
    "\n",
    "#     # 按File name分组并处理\n",
    "#     grouped = df.groupby('Image Name')\n",
    "#     print(\"处理图像...\")\n",
    "\n",
    "#     results = []\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "#         futures = [executor.submit(process_image, name, group, x, y) for name, group in grouped]\n",
    "        \n",
    "#         for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "#             name, classification = future.result()\n",
    "#             results.append((name, classification))\n",
    "    \n",
    "#     # 在原Excel文件中新增一列，保存分类结果\n",
    "#     classification_df = pd.DataFrame(results, columns=['Image Name', 'Classification'])\n",
    "#     df = pd.merge(df, classification_df, on='Image Name', how='left')\n",
    "#     df.to_excel(os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'), index=False)\n",
    "    \n",
    "#     print(\"处理完成！\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # for x in x_list:\n",
    "#     #     for y in y_list:\n",
    "#     #         print(f\"Processing for x={x}, y={y}...\")\n",
    "#     main(x, y)\n",
    "\n",
    "\n",
    "\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(x, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for Subfolder=9783, Style=txt...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "Processing for Subfolder=9783, Style=price...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "Processing for Subfolder=6913, Style=txt...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:09<00:00, 24.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6913, Style=txt\n",
      "2024-10-27 20:12:59\n",
      "--------------------\n",
      "Processing for Subfolder=6913, Style=price...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:04<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6913, Style=price\n",
      "2024-10-27 20:13:05\n",
      "--------------------\n",
      "Processing for Subfolder=6912, Style=txt...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:01<00:00, 28.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6912, Style=txt\n",
      "2024-10-27 20:13:07\n",
      "--------------------\n",
      "Processing for Subfolder=6912, Style=price...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [00:16<00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6912, Style=price\n",
      "2024-10-27 20:13:29\n",
      "--------------------\n",
      "Processing for Subfolder=6911, Style=txt...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:04<00:00, 28.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6911, Style=txt\n",
      "2024-10-27 20:13:35\n",
      "--------------------\n",
      "Processing for Subfolder=6911, Style=price...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 31.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6911, Style=price\n",
      "2024-10-27 20:13:36\n",
      "--------------------\n",
      "Processing for Subfolder=6910, Style=txt...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:03<00:00, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6910, Style=txt\n",
      "2024-10-27 20:13:40\n",
      "--------------------\n",
      "Processing for Subfolder=6910, Style=price...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 31.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6910, Style=price\n",
      "2024-10-27 20:13:42\n",
      "--------------------\n",
      "Processing for Subfolder=6909, Style=txt...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:04<00:00, 30.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6909, Style=txt\n",
      "2024-10-27 20:13:47\n",
      "--------------------\n",
      "Processing for Subfolder=6909, Style=price...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:08<00:00, 23.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6909, Style=price\n",
      "2024-10-27 20:13:59\n",
      "--------------------\n",
      "Processing for Subfolder=6908, Style=txt...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:01<00:00, 32.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6908, Style=txt\n",
      "2024-10-27 20:14:01\n",
      "--------------------\n",
      "Processing for Subfolder=6908, Style=price...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:09<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6908, Style=price\n",
      "2024-10-27 20:14:14\n",
      "--------------------\n",
      "Processing for Subfolder=12066, Style=txt...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "Processing for Subfolder=12066, Style=price...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "2024-10-27 20:14:14\n",
      "2024-10-27 20:14:14\n",
      "2024-10-27 20:14:14\n"
     ]
    }
   ],
   "source": [
    "# 这里是把x和y都采用了list的形式,用来简化人力的输入\n",
    "# 这里是针对整体\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "# 定义 x_list 和 y_list（只需要一次）\n",
    "x_list = ['9783','6913','6912','6911','6910','6909','6908','12066']  # 添加所有需要的 Subfolder 值\n",
    "# x_list = ['9736','9735','12004']  # 添加所有需要的 Subfolder 值\n",
    "y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# z = '女士春夏上装_from_0501'\n",
    "\n",
    "\n",
    "\n",
    "# 在文件开头定义一个函数来读取原始数据\n",
    "def read_original_data(z):\n",
    "    file_path = os.path.join(f'D://code//data//Lv2期结论//{z}//merged_info_ctr-1.xlsx')\n",
    "    # print(f\"Reading data from: {file_path}\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    df['Image Name'] = df['Image Name'] + '.jpg'\n",
    "    # print(f\"Read {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "# 1. 读取和预处理数据\n",
    "def normalize_coordinates(row):\n",
    "    width = 616\n",
    "    height = 616\n",
    "    row['left_norm'] = max(0, min(row['txt_x1'] / width, 1))\n",
    "    row['top_norm'] = max(0, min(row['txt_y1'] / height, 1))\n",
    "    row['right_norm'] = max(0, min(row['txt_x2'] / width, 1))\n",
    "    row['bottom_norm'] = max(0, min(row['txt_y2'] / height, 1))\n",
    "    return row\n",
    "\n",
    "# 2. 绘制矩形和网格\n",
    "def draw_rectangles(group):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        rect = Rectangle((row['left_norm'], 1 - row['bottom_norm']), \n",
    "                         row['right_norm'] - row['left_norm'], \n",
    "                         row['bottom_norm'] - row['top_norm'],\n",
    "                         fill=False, edgecolor='r')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            rect = Rectangle((j/3, 1 - (i+1)/3), 1/3, 1/3, fill=False, edgecolor='b')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    return fig, ax\n",
    "\n",
    "# 3. 判断重叠和分类\n",
    "def check_overlap(rect, grid_cell):\n",
    "    return not (rect[2] < grid_cell[0] or rect[0] > grid_cell[2] or\n",
    "                rect[3] < grid_cell[1] or rect[1] > grid_cell[3])\n",
    "\n",
    "def classify_image(group):\n",
    "    overlaps = [0] * 9\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        rect = (row['left_norm'], row['top_norm'], row['right_norm'], row['bottom_norm'])\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                grid_cell = (j/3, i/3, (j+1)/3, (i+1)/3)\n",
    "                if check_overlap(rect, grid_cell):\n",
    "                    overlaps[i*3 + j] = 1\n",
    "    return ''.join(map(str, overlaps))  # 直接使用join方法生成字符串\n",
    "\n",
    "# 4. 处理单个图像\n",
    "def process_image(name, group, x, y):\n",
    "    try:\n",
    "        classification = classify_image(group)\n",
    "        \n",
    "        # 复制图片\n",
    "        source = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', name)\n",
    "        destination = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_classified_images', classification.zfill(9), name)\n",
    "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "        shutil.copy2(source, destination)\n",
    "        \n",
    "        # 保存可视化结果\n",
    "        fig, ax = draw_rectangles(group)\n",
    "        visualization_name = f\"{name.split('.')[0]}_visualization.png\"\n",
    "        fig.savefig(os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_visualizations', visualization_name), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return name, classification\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing image {name}: {str(e)}\")\n",
    "        return name, None\n",
    "\n",
    "# 主处理函数\n",
    "def main(x, y, original_df):\n",
    "    print(f\"Processing for Subfolder={x}, Style={y}...\")\n",
    "    print(\"读取并预处理数据...\")\n",
    "\n",
    "    # 使用原始数据的副本\n",
    "    df = original_df.copy()\n",
    "\n",
    "    # print(f\"Original columns: {df.columns.tolist()}\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "    # 在读取df时添加筛选条件\n",
    "    def filter_by_rectangle(row):\n",
    "        right, bottom = 616 * 0.3, 616 * 0.2\n",
    "        if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # 修改数据筛选逻辑\n",
    "    df = df[(df['img_x1'].isna()) & (df['Subfolder'] == int(x)) & (df['Style'] == y) & (df['Type'] == 'Original')]\n",
    "    df = df.sort_values('ctr', ascending=False)\n",
    "\n",
    "    # print(f\"After filtering shape: {df.shape}\")\n",
    "\n",
    "    # if 'Image Name' not in df.columns:\n",
    "    #     print(\"'Image Name' not found. Trying to use 'File Name1' instead.\")\n",
    "    #     if 'File Name1' in df.columns:\n",
    "    #         df['Image Name'] = df['File Name1'] + '.jpg'\n",
    "    #     else:\n",
    "    #         print(f\"Neither 'Image Name' nor 'File Name1' found. Available columns: {df.columns.tolist()}\")\n",
    "    #         return\n",
    "\n",
    "    # 应用左上角的筛选条件\n",
    "    df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "    rows_to_keep = int(len(df) * 0.5)\n",
    "    df = df.head(rows_to_keep)\n",
    "\n",
    "    # 应用normalize_coordinates函数\n",
    "    df = df.apply(normalize_coordinates, axis=1)\n",
    "\n",
    "    # print(f\"Final shape: {df.shape}\")\n",
    "    # print(f\"Final columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # 检查是否存在所需的列\n",
    "    required_columns = ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Error: Missing columns: {missing_columns}\")\n",
    "        return\n",
    "\n",
    "    # 创建输出目录\n",
    "    classified_images_dir = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', '50%_txt_classified_images')\n",
    "    visualizations_dir = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', '50%_txt_visualizations')\n",
    "    os.makedirs(classified_images_dir, exist_ok=True)\n",
    "    os.makedirs(visualizations_dir, exist_ok=True)\n",
    "\n",
    "    # 按File name分组并处理\n",
    "    grouped = df.groupby('Image Name')\n",
    "    print(\"处理图像...\")\n",
    "\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_image, name, group, x, y) for name, group in grouped]\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            name, classification = future.result()\n",
    "            results.append((name, classification))\n",
    "    \n",
    "    # 在原Excel文件中新增一列，保存分类结果\n",
    "    classification_df = pd.DataFrame(results, columns=['Image Name', 'Classification'])\n",
    "    df = pd.merge(df, classification_df, on='Image Name', how='left')\n",
    "    df.to_excel(os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'), index=False)\n",
    "    \n",
    "    print(f\"Completed processing for Subfolder={x}, Style={y}\")\n",
    "    print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(\"--------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 在循环开始前读取原始数据\n",
    "    original_df = read_original_data(z)\n",
    "    \n",
    "    for x, y in itertools.product(x_list, y_list):\n",
    "        main(x, y, original_df)\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for Subfolder=9783, Style=txt, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "Processing for Subfolder=9783, Style=price, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "Processing for Subfolder=6913, Style=txt, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [00:06<00:00, 26.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6913, Style=txt, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:04\n",
      "--------------------\n",
      "Processing for Subfolder=6913, Style=price, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:02<00:00, 24.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6913, Style=price, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:07\n",
      "--------------------\n",
      "Processing for Subfolder=6912, Style=txt, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 36.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6912, Style=txt, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:07\n",
      "--------------------\n",
      "Processing for Subfolder=6912, Style=price, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "Processing for Subfolder=6911, Style=txt, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:03<00:00, 25.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6911, Style=txt, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:12\n",
      "--------------------\n",
      "Processing for Subfolder=6911, Style=price, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 31.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6911, Style=price, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:12\n",
      "--------------------\n",
      "Processing for Subfolder=6910, Style=txt, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:02<00:00, 31.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6910, Style=txt, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:15\n",
      "--------------------\n",
      "Processing for Subfolder=6910, Style=price, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6910, Style=price, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:15\n",
      "--------------------\n",
      "Processing for Subfolder=6909, Style=txt, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 27.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6909, Style=txt, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:19\n",
      "--------------------\n",
      "Processing for Subfolder=6909, Style=price, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "Processing for Subfolder=6908, Style=txt, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 37.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6908, Style=txt, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:20\n",
      "--------------------\n",
      "Processing for Subfolder=6908, Style=price, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for Subfolder=6908, Style=price, Filter=filter_5.0_6.0\n",
      "2024-10-27 20:55:20\n",
      "--------------------\n",
      "Processing for Subfolder=12066, Style=txt, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "Processing for Subfolder=12066, Style=price, Filter=filter_5.0_6.0...\n",
      "读取并预处理数据...\n",
      "Original shape: (55591, 47)\n",
      "Error: Missing columns: ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
      "2024-10-27 20:55:20\n",
      "2024-10-27 20:55:20\n",
      "2024-10-27 20:55:20\n"
     ]
    }
   ],
   "source": [
    "# 这里是把x和y都采用了list的形式,用来简化人力的输入\n",
    "# 这里添加了针对品牌分类的\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']  # 添加所有需要的 Subfolder 值\n",
    "# # x_list = ['9736','9735','12004']  # 添加所有需要的 Subfolder 值\n",
    "# y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# z = '男士春夏上装_from_0501'\n",
    "\n",
    "# 定义筛选条件\n",
    "filter_layers = [5.0, 6.0]  # 可以根据需要修改这个列表\n",
    "# filter_layers = [4.0, 5.0, 6.0]  # 可以根据需要修改这个列表\n",
    "\n",
    "\n",
    "# 在文件开头定义一个函数来读取原始数据\n",
    "def read_original_data(z):\n",
    "    file_path = os.path.join(f'D://code//data//Lv2期结论//{z}//merged_info_ctr-1.xlsx')\n",
    "    df = pd.read_excel(file_path)\n",
    "    df['Image Name'] = df['Image Name'] + '.jpg'\n",
    "    return df\n",
    "\n",
    "# 1. 读取和预处理数据\n",
    "def normalize_coordinates(row):\n",
    "    width = 616\n",
    "    height = 616\n",
    "    row['left_norm'] = max(0, min(row['txt_x1'] / width, 1))\n",
    "    row['top_norm'] = max(0, min(row['txt_y1'] / height, 1))\n",
    "    row['right_norm'] = max(0, min(row['txt_x2'] / width, 1))\n",
    "    row['bottom_norm'] = max(0, min(row['txt_y2'] / height, 1))\n",
    "    return row\n",
    "\n",
    "# 2. 绘制矩形和网格\n",
    "def draw_rectangles(group):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        rect = Rectangle((row['left_norm'], 1 - row['bottom_norm']), \n",
    "                         row['right_norm'] - row['left_norm'], \n",
    "                         row['bottom_norm'] - row['top_norm'],\n",
    "                         fill=False, edgecolor='r')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            rect = Rectangle((j/3, 1 - (i+1)/3), 1/3, 1/3, fill=False, edgecolor='b')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    return fig, ax\n",
    "\n",
    "# 3. 判断重叠和分类\n",
    "def check_overlap(rect, grid_cell):\n",
    "    return not (rect[2] < grid_cell[0] or rect[0] > grid_cell[2] or\n",
    "                rect[3] < grid_cell[1] or rect[1] > grid_cell[3])\n",
    "\n",
    "def classify_image(group):\n",
    "    overlaps = [0] * 9\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        rect = (row['left_norm'], row['top_norm'], row['right_norm'], row['bottom_norm'])\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                grid_cell = (j/3, i/3, (j+1)/3, (i+1)/3)\n",
    "                if check_overlap(rect, grid_cell):\n",
    "                    overlaps[i*3 + j] = 1\n",
    "    return ''.join(map(str, overlaps))  # 直接使用join方法生成字符串\n",
    "\n",
    "# 4. 处理单个图像\n",
    "def process_image(name, group, x, y, filter_suffix):\n",
    "    try:\n",
    "        classification = classify_image(group)\n",
    "        \n",
    "        # 复制图片\n",
    "        source = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', name)\n",
    "        destination = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_classified_images_{filter_suffix}', classification.zfill(9), name)\n",
    "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "        shutil.copy2(source, destination)\n",
    "        \n",
    "        # 保存可视化结果\n",
    "        fig, ax = draw_rectangles(group)\n",
    "        visualization_name = f\"{name.split('.')[0]}_visualization.png\"\n",
    "        fig.savefig(os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_visualizations_{filter_suffix}', visualization_name), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return name, classification\n",
    "    except Exception as e:\n",
    "        return name, None\n",
    "\n",
    "# 主处理函数\n",
    "def main(x, y, original_df, filter_layers):\n",
    "    filter_suffix = f\"filter_{'_'.join(map(str, filter_layers))}\"\n",
    "    print(f\"Processing for Subfolder={x}, Style={y}, Filter={filter_suffix}...\")\n",
    "    print(\"读取并预处理数据...\")\n",
    "\n",
    "    # 使用原始数据的副本\n",
    "    df = original_df.copy()\n",
    "\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "    # 在读取df时添加筛选条件\n",
    "    def filter_by_rectangle(row):\n",
    "        right, bottom = 616 * 0.3, 616 * 0.2\n",
    "        if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # 修改数据筛选逻辑\n",
    "    df = df[(df['img_x1'].isna()) & (df['Subfolder'] == int(x)) & (df['Style'] == y) & (df['Type'] == 'Original') & (df['最终分层'].isin(filter_layers))]\n",
    "    df = df.sort_values('ctr', ascending=False)\n",
    "\n",
    "    # 应用左上角的筛选条件\n",
    "    df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "    rows_to_keep = int(len(df) * 0.5)\n",
    "    df = df.head(rows_to_keep)\n",
    "\n",
    "    # 应用normalize_coordinates函数\n",
    "    df = df.apply(normalize_coordinates, axis=1)\n",
    "\n",
    "    # 检查是否存在所需的列\n",
    "    required_columns = ['left_norm', 'top_norm', 'right_norm', 'bottom_norm']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Error: Missing columns: {missing_columns}\")\n",
    "        return\n",
    "\n",
    "    # 创建输出目录\n",
    "    classified_images_dir = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', f'50%_txt_classified_images_{filter_suffix}')\n",
    "    visualizations_dir = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', f'50%_txt_visualizations_{filter_suffix}')\n",
    "    os.makedirs(classified_images_dir, exist_ok=True)\n",
    "    os.makedirs(visualizations_dir, exist_ok=True)\n",
    "\n",
    "    # 按File name分组并处理\n",
    "    grouped = df.groupby('Image Name')\n",
    "    print(\"处理图像...\")\n",
    "\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_image, name, group, x, y, filter_suffix) for name, group in grouped]\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            name, classification = future.result()\n",
    "            results.append((name, classification))\n",
    "    \n",
    "    # 在原Excel文件中新增一列，保存分类结果\n",
    "    classification_df = pd.DataFrame(results, columns=['Image Name', 'Classification'])\n",
    "    df = pd.merge(df, classification_df, on='Image Name', how='left')\n",
    "    df.to_excel(os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}_{filter_suffix}.xlsx'), index=False)\n",
    "    \n",
    "    print(f\"Completed processing for Subfolder={x}, Style={y}, Filter={filter_suffix}\")\n",
    "    print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(\"--------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 在循环开始前读取原始数据\n",
    "    original_df = read_original_data(z)\n",
    "    \n",
    "    for x, y in itertools.product(x_list, y_list):\n",
    "        main(x, y, original_df, filter_layers)\n",
    "\n",
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 针对分类结果, 绘制每个类别的文本框热力图, 手动输入x和y比较费手\n",
    "# #\n",
    "\n",
    "# # 创建保存结果的文件夹\n",
    "# output_folder = os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}', '50%_txt_output_heatmaps')\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # 读取Excel文件\n",
    "# df = pd.read_excel(os.path.join(f'D://code//data//Lv2期结论//京喜_from_0501//筛选//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'))\n",
    "\n",
    "# # 按Classification列进行分组\n",
    "# grouped = df.groupby('Classification')\n",
    "\n",
    "# # 遍历每个分组\n",
    "# for name, group in grouped:\n",
    "#     # 创建一个新的图形\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "#     # 绘制矩形框\n",
    "#     ax1.set_xlim(0, 616)\n",
    "#     ax1.set_ylim(616, 0)\n",
    "#     for _, row in group.iterrows():\n",
    "#         x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "#         # 排除左上角的文本框\n",
    "#         if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "#             continue\n",
    "#         width = x2 - x1\n",
    "#         height = y2 - y1\n",
    "#         rect = plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='r')\n",
    "#         ax1.add_patch(rect)\n",
    "#     ax1.set_title(f'Bounding Boxes for {name}')\n",
    "#     ax1.set_xlabel('X')\n",
    "#     ax1.set_ylabel('Y')\n",
    "\n",
    "#     # 创建热力图\n",
    "#     heatmap = np.zeros((616, 616))\n",
    "#     for _, row in group.iterrows():\n",
    "#         x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "#         # 排除左上角的文本框\n",
    "#         if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "#             continue\n",
    "#         x1, y1 = max(0, min(x1, 615)), max(0, min(y1, 615))\n",
    "#         x2, y2 = max(0, min(x2, 615)), max(0, min(y2, 615))\n",
    "#         heatmap[int(y1):int(y2)+1, int(x1):int(x2)+1] += 1\n",
    "\n",
    "#     # 绘制热力图\n",
    "#     sns.heatmap(heatmap, ax=ax2, cmap='YlOrRd', cbar=True)\n",
    "#     ax2.set_title(f'Heatmap for {name}')\n",
    "#     ax2.set_xlabel('X')\n",
    "#     ax2.set_ylabel('Y')\n",
    "\n",
    "#     # 调整子图之间的间距\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     # 保存图像\n",
    "#     plt.savefig(os.path.join(output_folder, f'{name}_heatmap.png'), dpi=100, bbox_inches='tight')\n",
    "#     plt.close()  # 关闭图形，释放内存\n",
    "\n",
    "#     print(f\"已保存 {name} 的热力图\")\n",
    "\n",
    "# print(\"所有热力图已保存在 output_heatmaps 文件夹中\")\n",
    "\n",
    "# import datetime\n",
    "\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(x, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 针对分类结果,绘制文本框热力图,但是通过x和y的list来实现\n",
    "\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.patches import Rectangle\n",
    "# import seaborn as sns\n",
    "# import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # # 定义 x_list 和 y_list（只需要一次）\n",
    "# # x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']  # 添加所有需要的 Subfolder 值\n",
    "# # y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# # z = '男士春夏上装_from_0501'\n",
    "\n",
    "\n",
    "\n",
    "# # 创建保存结果的文件夹\n",
    "# def create_output_folder(x, y):\n",
    "#     output_folder = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', '50%_txt_output_heatmaps')\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     return output_folder\n",
    "\n",
    "# # 读取Excel文件\n",
    "# def read_excel_file(x, y):\n",
    "#     file_path = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', f'50%_txt_info_with_classification-{x}_{y}.xlsx')\n",
    "#     return pd.read_excel(file_path)\n",
    "\n",
    "# # 绘制热力图\n",
    "# def plot_heatmaps(df, output_folder):\n",
    "#     # 按Classification列进行分组\n",
    "#     grouped = df.groupby('Classification')\n",
    "\n",
    "#     # 遍历每个分组\n",
    "#     for name, group in grouped:\n",
    "#         # 创建一个新的图形\n",
    "#         fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "#         # 绘制矩形框\n",
    "#         ax1.set_xlim(0, 616)\n",
    "#         ax1.set_ylim(616, 0)\n",
    "#         for _, row in group.iterrows():\n",
    "#             x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "#             # 排除左上角的文本框\n",
    "#             if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "#                 continue\n",
    "#             width = x2 - x1\n",
    "#             height = y2 - y1\n",
    "#             rect = plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='r')\n",
    "#             ax1.add_patch(rect)\n",
    "#         ax1.set_title(f'Bounding Boxes for {name}')\n",
    "#         ax1.set_xlabel('X')\n",
    "#         ax1.set_ylabel('Y')\n",
    "\n",
    "#         # 创建热力图\n",
    "#         heatmap = np.zeros((616, 616))\n",
    "#         for _, row in group.iterrows():\n",
    "#             x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "#             # 排除左上角的文本框\n",
    "#             if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "#                 continue\n",
    "#             x1, y1 = max(0, min(x1, 615)), max(0, min(y1, 615))\n",
    "#             x2, y2 = max(0, min(x2, 615)), max(0, min(y2, 615))\n",
    "#             heatmap[int(y1):int(y2)+1, int(x1):int(x2)+1] += 1\n",
    "\n",
    "#         # 绘制热力图\n",
    "#         sns.heatmap(heatmap, ax=ax2, cmap='YlOrRd', cbar=True)\n",
    "#         ax2.set_title(f'Heatmap for {name}')\n",
    "#         ax2.set_xlabel('X')\n",
    "#         ax2.set_ylabel('Y')\n",
    "\n",
    "#         # 调整子图之间的间距\n",
    "#         plt.tight_layout()\n",
    "\n",
    "#         # 保存图像\n",
    "#         plt.savefig(os.path.join(output_folder, f'{name}_heatmap.png'), dpi=100, bbox_inches='tight')\n",
    "#         plt.close()  # 关闭图形，释放内存\n",
    "\n",
    "#         print(f\"已保存 {name} 的热力图\")\n",
    "\n",
    "#     print(\"所有热力图已保存在 output_heatmaps 文件夹中\")\n",
    "\n",
    "# # 主函数\n",
    "# if __name__ == \"__main__\":\n",
    "#     for x in x_list:\n",
    "#         for y in y_list:\n",
    "#             print(f\"Processing for x={x}, y={y}...\")\n",
    "#             output_folder = create_output_folder(x, y)\n",
    "#             df = read_excel_file(x, y)\n",
    "#             plot_heatmaps(df, output_folder)\n",
    "\n",
    "#     current_time = datetime.datetime.now()\n",
    "#     formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     print(formatted_time)\n",
    "#     print(formatted_time)\n",
    "#     print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for x=9783, y=txt...\n",
      "No data found for x=9783, y=txt. Skipping...\n",
      "Processing for x=9783, y=price...\n",
      "No data found for x=9783, y=price. Skipping...\n",
      "Processing for x=6913, y=txt...\n",
      "已保存 1 的热力图\n",
      "已保存 100 的热力图\n",
      "已保存 101 的热力图\n",
      "已保存 1001 的热力图\n",
      "已保存 1011 的热力图\n",
      "已保存 1100 的热力图\n",
      "已保存 10111 的热力图\n",
      "已保存 100100 的热力图\n",
      "已保存 101100 的热力图\n",
      "已保存 110110 的热力图\n",
      "已保存 1001000 的热力图\n",
      "已保存 1001001 的热力图\n",
      "已保存 1001101 的热力图\n",
      "已保存 1001110 的热力图\n",
      "已保存 1101101 的热力图\n",
      "已保存 1101111 的热力图\n",
      "已保存 10011001 的热力图\n",
      "已保存 11000000 的热力图\n",
      "已保存 11000010 的热力图\n",
      "已保存 11000100 的热力图\n",
      "已保存 11000110 的热力图\n",
      "已保存 11000111 的热力图\n",
      "已保存 11001000 的热力图\n",
      "已保存 11001100 的热力图\n",
      "已保存 11010000 的热力图\n",
      "已保存 11010110 的热力图\n",
      "已保存 11011000 的热力图\n",
      "已保存 11011011 的热力图\n",
      "已保存 11111111 的热力图\n",
      "已保存 100000000 的热力图\n",
      "已保存 100000100 的热力图\n",
      "已保存 100000110 的热力图\n",
      "已保存 100100000 的热力图\n",
      "已保存 100100100 的热力图\n",
      "已保存 100100110 的热力图\n",
      "已保存 100110100 的热力图\n",
      "已保存 100111100 的热力图\n",
      "已保存 100111111 的热力图\n",
      "已保存 101100000 的热力图\n",
      "已保存 110000000 的热力图\n",
      "已保存 110000100 的热力图\n",
      "已保存 110000101 的热力图\n",
      "已保存 110000110 的热力图\n",
      "已保存 110000111 的热力图\n",
      "已保存 110010010 的热力图\n",
      "已保存 110100000 的热力图\n",
      "已保存 110100001 的热力图\n",
      "已保存 110100011 的热力图\n",
      "已保存 110100100 的热力图\n",
      "已保存 110100111 的热力图\n",
      "已保存 110110000 的热力图\n",
      "已保存 110110100 的热力图\n",
      "已保存 110110110 的热力图\n",
      "已保存 110110111 的热力图\n",
      "已保存 110111011 的热力图\n",
      "已保存 110111111 的热力图\n",
      "已保存 111000000 的热力图\n",
      "已保存 111000001 的热力图\n",
      "已保存 111000010 的热力图\n",
      "已保存 111000011 的热力图\n",
      "已保存 111000100 的热力图\n",
      "已保存 111000111 的热力图\n",
      "已保存 111001000 的热力图\n",
      "已保存 111001100 的热力图\n",
      "已保存 111001111 的热力图\n",
      "已保存 111010000 的热力图\n",
      "已保存 111010110 的热力图\n",
      "已保存 111011000 的热力图\n",
      "已保存 111100000 的热力图\n",
      "已保存 111100110 的热力图\n",
      "已保存 111110000 的热力图\n",
      "已保存 111110110 的热力图\n",
      "已保存 111111100 的热力图\n",
      "已保存 111111101 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6913, y=price...\n",
      "已保存 100 的热力图\n",
      "已保存 110 的热力图\n",
      "已保存 111 的热力图\n",
      "已保存 100100 的热力图\n",
      "已保存 11000110 的热力图\n",
      "已保存 11001000 的热力图\n",
      "已保存 11011000 的热力图\n",
      "已保存 11011011 的热力图\n",
      "已保存 100100000 的热力图\n",
      "已保存 100100100 的热力图\n",
      "已保存 100100111 的热力图\n",
      "已保存 100101111 的热力图\n",
      "已保存 100110110 的热力图\n",
      "已保存 110000000 的热力图\n",
      "已保存 110000100 的热力图\n",
      "已保存 110000111 的热力图\n",
      "已保存 110100000 的热力图\n",
      "已保存 110100111 的热力图\n",
      "已保存 110110000 的热力图\n",
      "已保存 110110001 的热力图\n",
      "已保存 111000000 的热力图\n",
      "已保存 111000001 的热力图\n",
      "已保存 111000100 的热力图\n",
      "已保存 111001100 的热力图\n",
      "已保存 111001111 的热力图\n",
      "已保存 111010100 的热力图\n",
      "已保存 111110000 的热力图\n",
      "已保存 111110010 的热力图\n",
      "已保存 111110011 的热力图\n",
      "已保存 111110110 的热力图\n",
      "已保存 111110111 的热力图\n",
      "已保存 111111111 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6912, y=txt...\n",
      "已保存 1 的热力图\n",
      "已保存 110 的热力图\n",
      "已保存 111 的热力图\n",
      "已保存 100000 的热力图\n",
      "已保存 100100 的热力图\n",
      "已保存 110000 的热力图\n",
      "已保存 1001000 的热力图\n",
      "已保存 10000000 的热力图\n",
      "已保存 11001000 的热力图\n",
      "已保存 100000000 的热力图\n",
      "已保存 100100000 的热力图\n",
      "已保存 100110110 的热力图\n",
      "已保存 101100111 的热力图\n",
      "已保存 110000000 的热力图\n",
      "已保存 110000111 的热力图\n",
      "已保存 110110000 的热力图\n",
      "已保存 110110111 的热力图\n",
      "已保存 111000000 的热力图\n",
      "已保存 111000001 的热力图\n",
      "已保存 111010000 的热力图\n",
      "已保存 111011111 的热力图\n",
      "已保存 111100000 的热力图\n",
      "已保存 111100111 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6912, y=price...\n",
      "已保存 111 的热力图\n",
      "已保存 1111 的热力图\n",
      "已保存 100100 的热力图\n",
      "已保存 100111 的热力图\n",
      "已保存 1000111 的热力图\n",
      "已保存 1001111 的热力图\n",
      "已保存 10100000 的热力图\n",
      "已保存 11000111 的热力图\n",
      "已保存 11100100 的热力图\n",
      "已保存 100000011 的热力图\n",
      "已保存 100000111 的热力图\n",
      "已保存 100001111 的热力图\n",
      "已保存 100100111 的热力图\n",
      "已保存 100101111 的热力图\n",
      "已保存 101000111 的热力图\n",
      "已保存 101100111 的热力图\n",
      "已保存 101101111 的热力图\n",
      "已保存 110000111 的热力图\n",
      "已保存 110001111 的热力图\n",
      "已保存 111000010 的热力图\n",
      "已保存 111000111 的热力图\n",
      "已保存 111001111 的热力图\n",
      "已保存 111010111 的热力图\n",
      "已保存 111100111 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6911, y=txt...\n",
      "已保存 100 的热力图\n",
      "已保存 110 的热力图\n",
      "已保存 111 的热力图\n",
      "已保存 100100 的热力图\n",
      "已保存 100111 的热力图\n",
      "已保存 111111 的热力图\n",
      "已保存 1000000 的热力图\n",
      "已保存 11000110 的热力图\n",
      "已保存 11011000 的热力图\n",
      "已保存 11011101 的热力图\n",
      "已保存 11111000 的热力图\n",
      "已保存 11111111 的热力图\n",
      "已保存 100000000 的热力图\n",
      "已保存 100100000 的热力图\n",
      "已保存 100100100 的热力图\n",
      "已保存 100100111 的热力图\n",
      "已保存 100110111 的热力图\n",
      "已保存 101100100 的热力图\n",
      "已保存 110000000 的热力图\n",
      "已保存 110110111 的热力图\n",
      "已保存 111000000 的热力图\n",
      "已保存 111000011 的热力图\n",
      "已保存 111000111 的热力图\n",
      "已保存 111100000 的热力图\n",
      "已保存 111100100 的热力图\n",
      "已保存 111100110 的热力图\n",
      "已保存 111101100 的热力图\n",
      "已保存 111111000 的热力图\n",
      "已保存 111111011 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6911, y=price...\n",
      "已保存 111 的热力图\n",
      "已保存 100100 的热力图\n",
      "已保存 101101 的热力图\n",
      "已保存 11000111 的热力图\n",
      "已保存 100100000 的热力图\n",
      "已保存 100100100 的热力图\n",
      "已保存 111000111 的热力图\n",
      "已保存 111101100 的热力图\n",
      "已保存 111111100 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6910, y=txt...\n",
      "已保存 100 的热力图\n",
      "已保存 111 的热力图\n",
      "已保存 1100 的热力图\n",
      "已保存 1111 的热力图\n",
      "已保存 10011 的热力图\n",
      "已保存 1000000 的热力图\n",
      "已保存 1000111 的热力图\n",
      "已保存 1001000 的热力图\n",
      "已保存 1001111 的热力图\n",
      "已保存 10010000 的热力图\n",
      "已保存 11000100 的热力图\n",
      "已保存 11000111 的热力图\n",
      "已保存 100000000 的热力图\n",
      "已保存 100000111 的热力图\n",
      "已保存 100100000 的热力图\n",
      "已保存 100100001 的热力图\n",
      "已保存 100100101 的热力图\n",
      "已保存 100100111 的热力图\n",
      "已保存 101000111 的热力图\n",
      "已保存 101001000 的热力图\n",
      "已保存 101111111 的热力图\n",
      "已保存 110000001 的热力图\n",
      "已保存 110100000 的热力图\n",
      "已保存 110110000 的热力图\n",
      "已保存 110110100 的热力图\n",
      "已保存 110110110 的热力图\n",
      "已保存 111000001 的热力图\n",
      "已保存 111000100 的热力图\n",
      "已保存 111000111 的热力图\n",
      "已保存 111100000 的热力图\n",
      "已保存 111100001 的热力图\n",
      "已保存 111100100 的热力图\n",
      "已保存 111100111 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6910, y=price...\n",
      "已保存 111 的热力图\n",
      "已保存 1100100 的热力图\n",
      "已保存 100100111 的热力图\n",
      "已保存 110000111 的热力图\n",
      "已保存 110010111 的热力图\n",
      "已保存 110100011 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6909, y=txt...\n",
      "已保存 11 的热力图\n",
      "已保存 100 的热力图\n",
      "已保存 110 的热力图\n",
      "已保存 111 的热力图\n",
      "已保存 11011 的热力图\n",
      "已保存 100011 的热力图\n",
      "已保存 100100 的热力图\n",
      "已保存 110000 的热力图\n",
      "已保存 110100 的热力图\n",
      "已保存 110110 的热力图\n",
      "已保存 1000111 的热力图\n",
      "已保存 1001111 的热力图\n",
      "已保存 11000100 的热力图\n",
      "已保存 11000111 的热力图\n",
      "已保存 11011101 的热力图\n",
      "已保存 100000000 的热力图\n",
      "已保存 100100000 的热力图\n",
      "已保存 100100100 的热力图\n",
      "已保存 110000111 的热力图\n",
      "已保存 110001001 的热力图\n",
      "已保存 110101000 的热力图\n",
      "已保存 111000000 的热力图\n",
      "已保存 111000011 的热力图\n",
      "已保存 111000110 的热力图\n",
      "已保存 111000111 的热力图\n",
      "已保存 111100000 的热力图\n",
      "已保存 111111000 的热力图\n",
      "已保存 111111111 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6909, y=price...\n",
      "已保存 110 的热力图\n",
      "已保存 111 的热力图\n",
      "已保存 100111 的热力图\n",
      "已保存 111111 的热力图\n",
      "已保存 1000110 的热力图\n",
      "已保存 1000111 的热力图\n",
      "已保存 10100000 的热力图\n",
      "已保存 10111111 的热力图\n",
      "已保存 100000111 的热力图\n",
      "已保存 100001111 的热力图\n",
      "已保存 100100111 的热力图\n",
      "已保存 100101111 的热力图\n",
      "已保存 110000111 的热力图\n",
      "已保存 110100111 的热力图\n",
      "已保存 111000111 的热力图\n",
      "已保存 111001111 的热力图\n",
      "已保存 111100111 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6908, y=txt...\n",
      "已保存 100 的热力图\n",
      "已保存 111 的热力图\n",
      "已保存 100011 的热力图\n",
      "已保存 100100 的热力图\n",
      "已保存 111111 的热力图\n",
      "已保存 1000010 的热力图\n",
      "已保存 1000111 的热力图\n",
      "已保存 10000111 的热力图\n",
      "已保存 11011111 的热力图\n",
      "已保存 11110000 的热力图\n",
      "已保存 11111111 的热力图\n",
      "已保存 100000111 的热力图\n",
      "已保存 100100000 的热力图\n",
      "已保存 100100100 的热力图\n",
      "已保存 100100111 的热力图\n",
      "已保存 101000000 的热力图\n",
      "已保存 110000000 的热力图\n",
      "已保存 110001000 的热力图\n",
      "已保存 110100000 的热力图\n",
      "已保存 110110000 的热力图\n",
      "已保存 111000000 的热力图\n",
      "已保存 111000100 的热力图\n",
      "已保存 111000110 的热力图\n",
      "已保存 111001000 的热力图\n",
      "已保存 111001111 的热力图\n",
      "已保存 111011000 的热力图\n",
      "已保存 111100000 的热力图\n",
      "已保存 111110000 的热力图\n",
      "已保存 111111000 的热力图\n",
      "已保存 111111011 的热力图\n",
      "已保存 111111111 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=6908, y=price...\n",
      "已保存 111 的热力图\n",
      "已保存 1111 的热力图\n",
      "已保存 100100 的热力图\n",
      "已保存 100111 的热力图\n",
      "已保存 1000111 的热力图\n",
      "已保存 10000111 的热力图\n",
      "已保存 100000111 的热力图\n",
      "已保存 100001111 的热力图\n",
      "已保存 100100101 的热力图\n",
      "已保存 110000000 的热力图\n",
      "已保存 110000111 的热力图\n",
      "已保存 110001111 的热力图\n",
      "已保存 110100111 的热力图\n",
      "已保存 111000001 的热力图\n",
      "已保存 111000111 的热力图\n",
      "已保存 111001111 的热力图\n",
      "已保存 111100111 的热力图\n",
      "已保存 111101111 的热力图\n",
      "已保存 111110111 的热力图\n",
      "所有热力图已保存在 output_heatmaps 文件夹中\n",
      "Processing for x=12066, y=txt...\n",
      "No data found for x=12066, y=txt. Skipping...\n",
      "Processing for x=12066, y=price...\n",
      "No data found for x=12066, y=price. Skipping...\n",
      "2024-10-27 21:23:21\n",
      "2024-10-27 21:23:21\n",
      "2024-10-27 21:23:21\n"
     ]
    }
   ],
   "source": [
    "# 针对分类结果,绘制文本框热力图,但是通过x和y的list来实现\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']  # 添加所有需要的 Subfolder 值\n",
    "# y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# z = '男士春夏上装_from_0501'\n",
    "\n",
    "\n",
    "\n",
    "# 创建保存结果的文件夹\n",
    "def create_output_folder(x, y):\n",
    "    output_folder = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', '50%_txt_output_heatmaps')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    return output_folder\n",
    "\n",
    "# 读取Excel文件\n",
    "def read_excel_file(x, y):\n",
    "    file_path = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', f'50%_txt_info_with_classification-{x}_{y}.xlsx')\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "# 绘制热力图\n",
    "def plot_heatmaps(df, output_folder):\n",
    "    # 按Classification列进行分组\n",
    "    grouped = df.groupby('Classification')\n",
    "\n",
    "    # 遍历每个分组\n",
    "    for name, group in grouped:\n",
    "        # 创建一个新的图形\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # 绘制矩形框\n",
    "        ax1.set_xlim(0, 616)\n",
    "        ax1.set_ylim(616, 0)\n",
    "        for _, row in group.iterrows():\n",
    "            x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "            # 排除左上角的文本框\n",
    "            if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "                continue\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            rect = plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='r')\n",
    "            ax1.add_patch(rect)\n",
    "        ax1.set_title(f'Bounding Boxes for {name}')\n",
    "        ax1.set_xlabel('X')\n",
    "        ax1.set_ylabel('Y')\n",
    "\n",
    "        # 创建热力图\n",
    "        heatmap = np.zeros((616, 616))\n",
    "        for _, row in group.iterrows():\n",
    "            x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "            # 排除左上角的文本框\n",
    "            if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "                continue\n",
    "            x1, y1 = max(0, min(x1, 615)), max(0, min(y1, 615))\n",
    "            x2, y2 = max(0, min(x2, 615)), max(0, min(y2, 615))\n",
    "            heatmap[int(y1):int(y2)+1, int(x1):int(x2)+1] += 1\n",
    "\n",
    "        # 绘制热力图\n",
    "        sns.heatmap(heatmap, ax=ax2, cmap='YlOrRd', cbar=True)\n",
    "        ax2.set_title(f'Heatmap for {name}')\n",
    "        ax2.set_xlabel('X')\n",
    "        ax2.set_ylabel('Y')\n",
    "\n",
    "        # 调整子图之间的间距\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 保存图像\n",
    "        plt.savefig(os.path.join(output_folder, f'{name}_heatmap.png'), dpi=100, bbox_inches='tight')\n",
    "        plt.close()  # 关闭图形，释放内存\n",
    "\n",
    "        print(f\"已保存 {name} 的热力图\")\n",
    "\n",
    "    print(\"所有热力图已保存在 output_heatmaps 文件夹中\")\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    for x in x_list:\n",
    "        for y in y_list:\n",
    "            print(f\"Processing for x={x}, y={y}...\")\n",
    "            output_folder = create_output_folder(x, y)\n",
    "            file_path = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', f'50%_txt_info_with_classification-{x}_{y}.xlsx')\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"No data found for x={x}, y={y}. Skipping...\")\n",
    "                continue\n",
    "            df = read_excel_file(x, y)\n",
    "            plot_heatmaps(df, output_folder)\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for x=9783, y=txt, filter_suffix=filter_1.0_2.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt\\50%_txt_info_with_classification-9783_txt_filter_1.0_2.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt\\50%_txt_info_with_classification-9783_txt_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for filter_1.0_2.0\n",
      "Processing for x=9783, y=txt, filter_suffix=filter_3.0_4.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt\\50%_txt_info_with_classification-9783_txt_filter_3.0_4.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt\\50%_txt_info_with_classification-9783_txt_filter_3.0_4.0.xlsx\n",
      "Skipping empty DataFrame for filter_3.0_4.0\n",
      "Processing for x=9783, y=txt, filter_suffix=filter_5.0_6.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt\\50%_txt_info_with_classification-9783_txt_filter_5.0_6.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt\\50%_txt_info_with_classification-9783_txt_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for filter_5.0_6.0\n",
      "Processing for x=9783, y=price, filter_suffix=filter_1.0_2.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price\\50%_txt_info_with_classification-9783_price_filter_1.0_2.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price\\50%_txt_info_with_classification-9783_price_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for filter_1.0_2.0\n",
      "Processing for x=9783, y=price, filter_suffix=filter_3.0_4.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price\\50%_txt_info_with_classification-9783_price_filter_3.0_4.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price\\50%_txt_info_with_classification-9783_price_filter_3.0_4.0.xlsx\n",
      "Skipping empty DataFrame for filter_3.0_4.0\n",
      "Processing for x=9783, y=price, filter_suffix=filter_5.0_6.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price\\50%_txt_info_with_classification-9783_price_filter_5.0_6.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price\\50%_txt_info_with_classification-9783_price_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for filter_5.0_6.0\n",
      "Processing for x=6913, y=txt, filter_suffix=filter_1.0_2.0...\n",
      "已保存 11011011 的热力图 (filter_1.0_2.0)\n",
      "已保存 11110110 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6913, y=txt, filter_suffix=filter_3.0_4.0...\n",
      "已保存 100 的热力图 (filter_3.0_4.0)\n",
      "已保存 1001 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 101100 的热力图 (filter_3.0_4.0)\n",
      "已保存 1001000 的热力图 (filter_3.0_4.0)\n",
      "已保存 1001001 的热力图 (filter_3.0_4.0)\n",
      "已保存 1001101 的热力图 (filter_3.0_4.0)\n",
      "已保存 1101111 的热力图 (filter_3.0_4.0)\n",
      "已保存 11000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 11001000 的热力图 (filter_3.0_4.0)\n",
      "已保存 11010110 的热力图 (filter_3.0_4.0)\n",
      "已保存 11011000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000110 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100110 的热力图 (filter_3.0_4.0)\n",
      "已保存 100110100 的热力图 (filter_3.0_4.0)\n",
      "已保存 100111111 的热力图 (filter_3.0_4.0)\n",
      "已保存 101100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000110 的热力图 (filter_3.0_4.0)\n",
      "已保存 110100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110100001 的热力图 (filter_3.0_4.0)\n",
      "已保存 110100011 的热力图 (filter_3.0_4.0)\n",
      "已保存 110100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 110110000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110110100 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111001000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111001111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111010000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111110000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111111100 的热力图 (filter_3.0_4.0)\n",
      "已保存 111111101 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6913, y=txt, filter_suffix=filter_5.0_6.0...\n",
      "已保存 1 的热力图 (filter_5.0_6.0)\n",
      "已保存 101 的热力图 (filter_5.0_6.0)\n",
      "已保存 110 的热力图 (filter_5.0_6.0)\n",
      "已保存 111 的热力图 (filter_5.0_6.0)\n",
      "已保存 1011 的热力图 (filter_5.0_6.0)\n",
      "已保存 10111 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100 的热力图 (filter_5.0_6.0)\n",
      "已保存 1000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 11000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 11000010 的热力图 (filter_5.0_6.0)\n",
      "已保存 11000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 11000110 的热力图 (filter_5.0_6.0)\n",
      "已保存 11001000 的热力图 (filter_5.0_6.0)\n",
      "已保存 11001100 的热力图 (filter_5.0_6.0)\n",
      "已保存 11011000 的热力图 (filter_5.0_6.0)\n",
      "已保存 100000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100100 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100110 的热力图 (filter_5.0_6.0)\n",
      "已保存 100111100 的热力图 (filter_5.0_6.0)\n",
      "已保存 110000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 110000001 的热力图 (filter_5.0_6.0)\n",
      "已保存 110000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 110000101 的热力图 (filter_5.0_6.0)\n",
      "已保存 110000111 的热力图 (filter_5.0_6.0)\n",
      "已保存 110010010 的热力图 (filter_5.0_6.0)\n",
      "已保存 110100000 的热力图 (filter_5.0_6.0)\n",
      "已保存 110100100 的热力图 (filter_5.0_6.0)\n",
      "已保存 110100111 的热力图 (filter_5.0_6.0)\n",
      "已保存 110110000 的热力图 (filter_5.0_6.0)\n",
      "已保存 110110100 的热力图 (filter_5.0_6.0)\n",
      "已保存 110110110 的热力图 (filter_5.0_6.0)\n",
      "已保存 110110111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000001 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000011 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111001100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111001111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111010110 的热力图 (filter_5.0_6.0)\n",
      "已保存 111011000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111100000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111100110 的热力图 (filter_5.0_6.0)\n",
      "已保存 111110000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111110110 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=6913, y=price, filter_suffix=filter_1.0_2.0...\n",
      "已保存 111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110000011 的热力图 (filter_1.0_2.0)\n",
      "已保存 110000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110010111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111001111 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6913, y=price, filter_suffix=filter_3.0_4.0...\n",
      "已保存 100 的热力图 (filter_3.0_4.0)\n",
      "已保存 110 的热力图 (filter_3.0_4.0)\n",
      "已保存 111 的热力图 (filter_3.0_4.0)\n",
      "已保存 11000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 11000110 的热力图 (filter_3.0_4.0)\n",
      "已保存 11011000 的热力图 (filter_3.0_4.0)\n",
      "已保存 11011011 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100111 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111110000 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6913, y=price, filter_suffix=filter_5.0_6.0...\n",
      "已保存 100100 的热力图 (filter_5.0_6.0)\n",
      "已保存 11001000 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100111 的热力图 (filter_5.0_6.0)\n",
      "已保存 100101111 的热力图 (filter_5.0_6.0)\n",
      "已保存 100110110 的热力图 (filter_5.0_6.0)\n",
      "已保存 110100000 的热力图 (filter_5.0_6.0)\n",
      "已保存 110110000 的热力图 (filter_5.0_6.0)\n",
      "已保存 110110001 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000001 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111001100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111001111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111010100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111110010 的热力图 (filter_5.0_6.0)\n",
      "已保存 111110011 的热力图 (filter_5.0_6.0)\n",
      "已保存 111110111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111111111 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=6912, y=txt, filter_suffix=filter_1.0_2.0...\n",
      "已保存 100 的热力图 (filter_1.0_2.0)\n",
      "已保存 111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100000 的热力图 (filter_1.0_2.0)\n",
      "已保存 111000001 的热力图 (filter_1.0_2.0)\n",
      "已保存 111010000 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6912, y=txt, filter_suffix=filter_3.0_4.0...\n",
      "已保存 110 的热力图 (filter_3.0_4.0)\n",
      "已保存 111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000 的热力图 (filter_3.0_4.0)\n",
      "已保存 1001000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 110110000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111011111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111100111 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6912, y=txt, filter_suffix=filter_5.0_6.0...\n",
      "已保存 10000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 11001000 的热力图 (filter_5.0_6.0)\n",
      "已保存 100110110 的热力图 (filter_5.0_6.0)\n",
      "已保存 101100111 的热力图 (filter_5.0_6.0)\n",
      "已保存 110000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 110110111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111001000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111100000 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=6912, y=price, filter_suffix=filter_1.0_2.0...\n",
      "已保存 111 的热力图 (filter_1.0_2.0)\n",
      "已保存 1111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100100 的热力图 (filter_1.0_2.0)\n",
      "已保存 100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 1000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 1001111 的热力图 (filter_1.0_2.0)\n",
      "已保存 10100000 的热力图 (filter_1.0_2.0)\n",
      "已保存 11000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 11100100 的热力图 (filter_1.0_2.0)\n",
      "已保存 100000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100101111 的热力图 (filter_1.0_2.0)\n",
      "已保存 101000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 101100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 101101111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110001111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111000010 的热力图 (filter_1.0_2.0)\n",
      "已保存 111010111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111100111 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6912, y=price, filter_suffix=filter_3.0_4.0...\n",
      "已保存 111 的热力图 (filter_3.0_4.0)\n",
      "已保存 1111 的热力图 (filter_3.0_4.0)\n",
      "已保存 1000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000011 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000100 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100001111 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111001111 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6912, y=price, filter_suffix=filter_5.0_6.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price\\50%_txt_info_with_classification-6912_price_filter_5.0_6.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price\\50%_txt_info_with_classification-6912_price_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for filter_5.0_6.0\n",
      "Processing for x=6911, y=txt, filter_suffix=filter_1.0_2.0...\n",
      "已保存 11 的热力图 (filter_1.0_2.0)\n",
      "已保存 111 的热力图 (filter_1.0_2.0)\n",
      "已保存 1000000 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6911, y=txt, filter_suffix=filter_3.0_4.0...\n",
      "已保存 110 的热力图 (filter_3.0_4.0)\n",
      "已保存 111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 111111 的热力图 (filter_3.0_4.0)\n",
      "已保存 11000110 的热力图 (filter_3.0_4.0)\n",
      "已保存 11111111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 101100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000011 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111101100 的热力图 (filter_3.0_4.0)\n",
      "已保存 111111000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111111011 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6911, y=txt, filter_suffix=filter_5.0_6.0...\n",
      "已保存 100 的热力图 (filter_5.0_6.0)\n",
      "已保存 11011000 的热力图 (filter_5.0_6.0)\n",
      "已保存 11011101 的热力图 (filter_5.0_6.0)\n",
      "已保存 11111000 的热力图 (filter_5.0_6.0)\n",
      "已保存 100000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100000 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100111 的热力图 (filter_5.0_6.0)\n",
      "已保存 100110111 的热力图 (filter_5.0_6.0)\n",
      "已保存 110000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 110110111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111100000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111100100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111100110 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=6911, y=price, filter_suffix=filter_1.0_2.0...\n",
      "已保存 101101 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6911, y=price, filter_suffix=filter_3.0_4.0...\n",
      "已保存 111 的热力图 (filter_3.0_4.0)\n",
      "已保存 1001 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 11000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 111101100 的热力图 (filter_3.0_4.0)\n",
      "已保存 111111100 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6911, y=price, filter_suffix=filter_5.0_6.0...\n",
      "已保存 111 的热力图 (filter_5.0_6.0)\n",
      "已保存 11000111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000111 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=6910, y=txt, filter_suffix=filter_1.0_2.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt\\50%_txt_info_with_classification-6910_txt_filter_1.0_2.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt\\50%_txt_info_with_classification-6910_txt_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for filter_1.0_2.0\n",
      "Processing for x=6910, y=txt, filter_suffix=filter_3.0_4.0...\n",
      "已保存 1000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 11000100 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100001 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100111 的热力图 (filter_3.0_4.0)\n",
      "已保存 101000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 101001000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110110000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110110110 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6910, y=txt, filter_suffix=filter_5.0_6.0...\n",
      "已保存 100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111 的热力图 (filter_5.0_6.0)\n",
      "已保存 1100 的热力图 (filter_5.0_6.0)\n",
      "已保存 1111 的热力图 (filter_5.0_6.0)\n",
      "已保存 10011 的热力图 (filter_5.0_6.0)\n",
      "已保存 1000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 1001000 的热力图 (filter_5.0_6.0)\n",
      "已保存 1001111 的热力图 (filter_5.0_6.0)\n",
      "已保存 11000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 11000111 的热力图 (filter_5.0_6.0)\n",
      "已保存 100000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 100000111 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100000 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100101 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100111 的热力图 (filter_5.0_6.0)\n",
      "已保存 101111111 的热力图 (filter_5.0_6.0)\n",
      "已保存 110000001 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111100000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111100001 的热力图 (filter_5.0_6.0)\n",
      "已保存 111100100 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=6910, y=price, filter_suffix=filter_1.0_2.0...\n",
      "已保存 100100111 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6910, y=price, filter_suffix=filter_3.0_4.0...\n",
      "已保存 111 的热力图 (filter_3.0_4.0)\n",
      "已保存 1100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 110010111 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6910, y=price, filter_suffix=filter_5.0_6.0...\n",
      "已保存 100100111 的热力图 (filter_5.0_6.0)\n",
      "已保存 110100011 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=6909, y=txt, filter_suffix=filter_1.0_2.0...\n",
      "已保存 111 的热力图 (filter_1.0_2.0)\n",
      "已保存 1001111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110001001 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6909, y=txt, filter_suffix=filter_3.0_4.0...\n",
      "已保存 11011 的热力图 (filter_3.0_4.0)\n",
      "已保存 100011 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000 的热力图 (filter_3.0_4.0)\n",
      "已保存 11000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000011 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000110 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000111 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6909, y=txt, filter_suffix=filter_5.0_6.0...\n",
      "已保存 100 的热力图 (filter_5.0_6.0)\n",
      "已保存 110 的热力图 (filter_5.0_6.0)\n",
      "已保存 110100 的热力图 (filter_5.0_6.0)\n",
      "已保存 110110 的热力图 (filter_5.0_6.0)\n",
      "已保存 11000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000011 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111110001 的热力图 (filter_5.0_6.0)\n",
      "已保存 111111111 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=6909, y=price, filter_suffix=filter_1.0_2.0...\n",
      "已保存 110 的热力图 (filter_1.0_2.0)\n",
      "已保存 111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 1000110 的热力图 (filter_1.0_2.0)\n",
      "已保存 1000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 10100000 的热力图 (filter_1.0_2.0)\n",
      "已保存 100000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100001111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100101111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111001111 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6909, y=price, filter_suffix=filter_3.0_4.0...\n",
      "已保存 111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111111 的热力图 (filter_3.0_4.0)\n",
      "已保存 10111111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111100111 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6909, y=price, filter_suffix=filter_5.0_6.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price\\50%_txt_info_with_classification-6909_price_filter_5.0_6.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price\\50%_txt_info_with_classification-6909_price_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for filter_5.0_6.0\n",
      "Processing for x=6908, y=txt, filter_suffix=filter_1.0_2.0...\n",
      "已保存 10 的热力图 (filter_1.0_2.0)\n",
      "已保存 111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110001000 的热力图 (filter_1.0_2.0)\n",
      "已保存 111000000 的热力图 (filter_1.0_2.0)\n",
      "已保存 111000110 的热力图 (filter_1.0_2.0)\n",
      "已保存 111001000 的热力图 (filter_1.0_2.0)\n",
      "已保存 111110000 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6908, y=txt, filter_suffix=filter_3.0_4.0...\n",
      "已保存 111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 1000010 的热力图 (filter_3.0_4.0)\n",
      "已保存 1000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 10000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 11000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100100 的热力图 (filter_3.0_4.0)\n",
      "已保存 101000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110000000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 110110000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111001111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111100000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111110000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111111000 的热力图 (filter_3.0_4.0)\n",
      "已保存 111111111 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6908, y=txt, filter_suffix=filter_5.0_6.0...\n",
      "已保存 100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100000 的热力图 (filter_5.0_6.0)\n",
      "已保存 100100111 的热力图 (filter_5.0_6.0)\n",
      "已保存 110000000 的热力图 (filter_5.0_6.0)\n",
      "已保存 111000100 的热力图 (filter_5.0_6.0)\n",
      "已保存 111101111 的热力图 (filter_5.0_6.0)\n",
      "已保存 111111011 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=6908, y=price, filter_suffix=filter_1.0_2.0...\n",
      "已保存 111 的热力图 (filter_1.0_2.0)\n",
      "已保存 1111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100100 的热力图 (filter_1.0_2.0)\n",
      "已保存 100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 1000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100000101 的热力图 (filter_1.0_2.0)\n",
      "已保存 100000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 100001111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110000000 的热力图 (filter_1.0_2.0)\n",
      "已保存 110000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110001111 的热力图 (filter_1.0_2.0)\n",
      "已保存 110100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111000111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111001111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111100111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111101111 的热力图 (filter_1.0_2.0)\n",
      "已保存 111110111 的热力图 (filter_1.0_2.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price\\50%_txt_output_heatmaps_filter_1.0_2.0 文件夹中\n",
      "Processing for x=6908, y=price, filter_suffix=filter_3.0_4.0...\n",
      "已保存 111 的热力图 (filter_3.0_4.0)\n",
      "已保存 10000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100101 的热力图 (filter_3.0_4.0)\n",
      "已保存 100100111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000001 的热力图 (filter_3.0_4.0)\n",
      "已保存 111000111 的热力图 (filter_3.0_4.0)\n",
      "已保存 111100111 的热力图 (filter_3.0_4.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price\\50%_txt_output_heatmaps_filter_3.0_4.0 文件夹中\n",
      "Processing for x=6908, y=price, filter_suffix=filter_5.0_6.0...\n",
      "已保存 111000100 的热力图 (filter_5.0_6.0)\n",
      "所有热力图已保存在 D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price\\50%_txt_output_heatmaps_filter_5.0_6.0 文件夹中\n",
      "Processing for x=12066, y=txt, filter_suffix=filter_1.0_2.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt\\50%_txt_info_with_classification-12066_txt_filter_1.0_2.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt\\50%_txt_info_with_classification-12066_txt_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for filter_1.0_2.0\n",
      "Processing for x=12066, y=txt, filter_suffix=filter_3.0_4.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt\\50%_txt_info_with_classification-12066_txt_filter_3.0_4.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt\\50%_txt_info_with_classification-12066_txt_filter_3.0_4.0.xlsx\n",
      "Skipping empty DataFrame for filter_3.0_4.0\n",
      "Processing for x=12066, y=txt, filter_suffix=filter_5.0_6.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt\\50%_txt_info_with_classification-12066_txt_filter_5.0_6.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt\\50%_txt_info_with_classification-12066_txt_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for filter_5.0_6.0\n",
      "Processing for x=12066, y=price, filter_suffix=filter_1.0_2.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price\\50%_txt_info_with_classification-12066_price_filter_1.0_2.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price\\50%_txt_info_with_classification-12066_price_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for filter_1.0_2.0\n",
      "Processing for x=12066, y=price, filter_suffix=filter_3.0_4.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price\\50%_txt_info_with_classification-12066_price_filter_3.0_4.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price\\50%_txt_info_with_classification-12066_price_filter_3.0_4.0.xlsx\n",
      "Skipping empty DataFrame for filter_3.0_4.0\n",
      "Processing for x=12066, y=price, filter_suffix=filter_5.0_6.0...\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price\\50%_txt_info_with_classification-12066_price_filter_5.0_6.0.xlsx\n",
      "Created empty file: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price\\50%_txt_info_with_classification-12066_price_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for filter_5.0_6.0\n",
      "2024-10-27 21:31:18\n",
      "2024-10-27 21:31:18\n",
      "2024-10-27 21:31:18\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']\n",
    "# y_list = ['txt', 'price']\n",
    "\n",
    "# z = '男士春夏上装_from_0501'\n",
    "\n",
    "filter_suffix_list = ['filter_1.0_2.0','filter_3.0_4.0','filter_5.0_6.0']\n",
    "\n",
    "\n",
    "\n",
    "def create_output_folder(x, y, filter_suffix):\n",
    "    output_folder = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', f'50%_txt_output_heatmaps_{filter_suffix}')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    return output_folder\n",
    "\n",
    "def read_excel_file(x, y, filter_suffix):\n",
    "    file_path = os.path.join(f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}', f'50%_txt_info_with_classification-{x}_{y}_{filter_suffix}.xlsx')\n",
    "    return file_path\n",
    "\n",
    "def create_empty_file(file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    pd.DataFrame().to_excel(file_path, index=False)\n",
    "    print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "def plot_heatmaps(df, output_folder, filter_suffix):\n",
    "    if df.empty:\n",
    "        print(f\"Skipping empty DataFrame for {filter_suffix}\")\n",
    "        return\n",
    "\n",
    "    grouped = df.groupby('Classification')\n",
    "    for name, group in grouped:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        ax1.set_xlim(0, 616)\n",
    "        ax1.set_ylim(616, 0)\n",
    "        for _, row in group.iterrows():\n",
    "            x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "            if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "                continue\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            rect = plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='r')\n",
    "            ax1.add_patch(rect)\n",
    "        ax1.set_title(f'Bounding Boxes for {name} ({filter_suffix})')\n",
    "        ax1.set_xlabel('X')\n",
    "        ax1.set_ylabel('Y')\n",
    "\n",
    "        heatmap = np.zeros((616, 616))\n",
    "        for _, row in group.iterrows():\n",
    "            x1, y1, x2, y2 = row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']\n",
    "            if x1 <= 616 * 0.3 and y1 <= 616 * 0.2:\n",
    "                continue\n",
    "            x1, y1 = max(0, min(x1, 615)), max(0, min(y1, 615))\n",
    "            x2, y2 = max(0, min(x2, 615)), max(0, min(y2, 615))\n",
    "            heatmap[int(y1):int(y2)+1, int(x1):int(x2)+1] += 1\n",
    "\n",
    "        sns.heatmap(heatmap, ax=ax2, cmap='YlOrRd', cbar=True)\n",
    "        ax2.set_title(f'Heatmap for {name} ({filter_suffix})')\n",
    "        ax2.set_xlabel('X')\n",
    "        ax2.set_ylabel('Y')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f'{name}_{filter_suffix}_heatmap.png'), dpi=100, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"已保存 {name} 的热力图 ({filter_suffix})\")\n",
    "\n",
    "    print(f\"所有热力图已保存在 {output_folder} 文件夹中\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for x in x_list:\n",
    "        for y in y_list:\n",
    "            for filter_suffix in filter_suffix_list:\n",
    "                print(f\"Processing for x={x}, y={y}, filter_suffix={filter_suffix}...\")\n",
    "                output_folder = create_output_folder(x, y, filter_suffix)\n",
    "                file_path = read_excel_file(x, y, filter_suffix)\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "                    create_empty_file(file_path)\n",
    "                    df = pd.DataFrame()\n",
    "\n",
    "                if not df.empty:\n",
    "                    filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "                    df = df[df['最终分层'].isin(filter_values)]\n",
    "                \n",
    "                plot_heatmaps(df, output_folder, filter_suffix)\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本大小的总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for x=9783, y=txt. Skipping...\n",
      "No data found for x=9783, y=price. Skipping...\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height18到29',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height29到38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height大于38',).png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps\\heatmap_wordsize_('Height小于18',).png\n",
      "No data found for x=12066, y=txt. Skipping...\n",
      "No data found for x=12066, y=price. Skipping...\n",
      "All heatmaps have been generated.\n",
      "2024-10-28 09:56:08\n",
      "2024-10-28 09:56:08\n",
      "2024-10-28 09:56:08\n"
     ]
    }
   ],
   "source": [
    "# 新流程, 可以通过list方式, 来合并读取\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']\n",
    "# # x_list = ['9736','9735','12004']  # 添加所有需要的 Subfolder 值\n",
    "# y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# z = '男士春夏上装_from_0501'\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "for item_x in x_list:\n",
    "    for item_y in y_list:\n",
    "        # 读取Excel文件\n",
    "        file_path = f'D://code//data//Lv2期结论//{z}//{item_x}//grounding_output//{item_y}//50%_txt_info_with_classification-{item_x}_{item_y}.xlsx'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"No data found for x={item_x}, y={item_y}. Skipping...\")\n",
    "            continue\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # 应用 filter_by_rectangle 函数来过滤数据\n",
    "        df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "        # 替换 Height_Category 列的值\n",
    "        df['Height_Category'] = df['Height_Category'].replace({\n",
    "            'Height_>38': 'Height大于38',\n",
    "            'Height_18-29': 'Height18到29',\n",
    "            'Height_29-38': 'Height29到38',\n",
    "            'Height_<18': 'Height小于18'\n",
    "        })\n",
    "\n",
    "        # 删除 structure 为空值的行\n",
    "        # df = df.dropna(subset=['structure'])\n",
    "        df = df.dropna(subset=['Height_Category'])\n",
    "\n",
    "        # 确保必要的列存在\n",
    "        required_columns = ['Height_Category', 'txt_x1', 'txt_y1', 'txt_x2', 'txt_y2']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"DataFrame must contain all of these columns: {required_columns}\")\n",
    "\n",
    "        # 创建输出目录\n",
    "        output_dir = f\"D://code//data//Lv2期结论//{z}//{item_x}//grounding_output//{item_y}//50%_txt_wordsize_heatmaps\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 按 structure 和 box_no 分组\n",
    "        grouped = df.groupby(['Height_Category'])\n",
    "\n",
    "        # 遍历每个分组\n",
    "        for (box_no), group in grouped:\n",
    "            # 创建一个空的 2D numpy 数组来存储热力图数据，大小为 616x616\n",
    "            heatmap_data = np.zeros((616, 616))\n",
    "\n",
    "            # 对每个矩形框增加热度值\n",
    "            for _, row in group.iterrows():\n",
    "                x1, y1, x2, y2 = map(int, [row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']])\n",
    "                # 确保坐标不超出边界\n",
    "                x1, x2 = max(0, min(x1, 616)), max(0, min(x2, 616))\n",
    "                y1, y2 = max(0, min(y1, 616)), max(0, min(y2, 616))\n",
    "                heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "            # 创建图形，设置大小为正方形\n",
    "            plt.figure(figsize=(10, 10))\n",
    "\n",
    "            # 使用 seaborn 绘制热力图\n",
    "            sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Frequency'}, square=True)\n",
    "\n",
    "            # 设置标题和轴标签\n",
    "            # plt.title(f'Bounding Box Heatmap - Structure: {structure}, word size: {Height_Category}')\n",
    "            plt.xlabel('X coordinate')\n",
    "            plt.ylabel('Y coordinate')\n",
    "\n",
    "            # 调整图形以保持正方形比例\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "            # 保存图形\n",
    "            output_path = os.path.join(output_dir, f\"heatmap_wordsize_{box_no}.png\")\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"Heatmap saved to: {output_path}\")\n",
    "\n",
    "print(\"All heatmaps have been generated.\")\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 添加了品牌分类\n",
    "# # 添加了品牌分类\n",
    "# # 添加了品牌分类\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 x_list 和 y_list\n",
    "# x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']\n",
    "# # x_list = ['9736','9735','12004']  # 添加所有需要的 Subfolder 值\n",
    "# y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# z = '男士春夏上装_from_0501'\n",
    "\n",
    "# # 定义 filter_suffix_list\n",
    "# filter_suffix_list = ['filter_1.0_2.0_3.0', 'filter_4.0_5.0_6.0']  # 可以根据需要添加更多\n",
    "\n",
    "\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# def process_data(item_x, item_y, filter_suffix):\n",
    "#     # 读取Excel文件\n",
    "#     file_path = f'D://code//data//Lv2期结论//{z}//{item_x}//grounding_output//{item_y}//50%_txt_info_with_classification-{item_x}_{item_y}_{filter_suffix}.xlsx'\n",
    "#     df = pd.read_excel(file_path)\n",
    "\n",
    "#     # 应用 filter_by_rectangle 函数来过滤数据\n",
    "#     df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "\n",
    "#     # 替换 Height_Category 列的值\n",
    "#     df['Height_Category'] = df['Height_Category'].replace({\n",
    "#         'Height_>38': 'Height大于38',\n",
    "#         'Height_18-29': 'Height18到29',\n",
    "#         'Height_29-38': 'Height29到38',\n",
    "#         'Height_<18': 'Height小于18'\n",
    "#     })\n",
    "\n",
    "#     # 删除 Height_Category 为空值的行\n",
    "#     df = df.dropna(subset=['Height_Category'])\n",
    "\n",
    "#     # 确保必要的列存在\n",
    "#     required_columns = ['Height_Category', 'txt_x1', 'txt_y1', 'txt_x2', 'txt_y2', '最终分层']\n",
    "#     if not all(col in df.columns for col in required_columns):\n",
    "#         raise ValueError(f\"DataFrame must contain all of these columns: {required_columns}\")\n",
    "\n",
    "#     # 根据 filter_suffix 筛选数据\n",
    "#     filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "#     df = df[df['最终分层'].isin(filter_values)]\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def generate_heatmap(group, output_path):\n",
    "#     # 创建一个空的 2D numpy 数组来存储热力图数据，大小为 616x616\n",
    "#     heatmap_data = np.zeros((616, 616))\n",
    "\n",
    "#     # 对每个矩形框增加热度值\n",
    "#     for _, row in group.iterrows():\n",
    "#         x1, y1, x2, y2 = map(int, [row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']])\n",
    "#         # 确保坐标不超出边界\n",
    "#         x1, x2 = max(0, min(x1, 616)), max(0, min(x2, 616))\n",
    "#         y1, y2 = max(0, min(y1, 616)), max(0, min(y2, 616))\n",
    "#         heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "#     # 创建图形，设置大小为正方形\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "\n",
    "#     # 使用 seaborn 绘制热力图\n",
    "#     sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Frequency'}, square=True)\n",
    "\n",
    "#     # 设置标题和轴标签\n",
    "#     plt.xlabel('X coordinate')\n",
    "#     plt.ylabel('Y coordinate')\n",
    "\n",
    "#     # 调整图形以保持正方形比例\n",
    "#     plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "#     # 保存图形\n",
    "#     plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "#     print(f\"Heatmap saved to: {output_path}\")\n",
    "\n",
    "# def main():\n",
    "#     for item_x in x_list:\n",
    "#         for item_y in y_list:\n",
    "#             for filter_suffix in filter_suffix_list:\n",
    "#                 # 处理数据\n",
    "#                 df = process_data(item_x, item_y, filter_suffix)\n",
    "\n",
    "#                 # 创建输出目录\n",
    "#                 output_dir = f\"D://code//data//Lv2期结论//{z}//{item_x}//grounding_output//{item_y}//50%_txt_wordsize_heatmaps_{filter_suffix}\"\n",
    "#                 os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#                 # 按 Height_Category 分组\n",
    "#                 grouped = df.groupby(['Height_Category'])\n",
    "\n",
    "#                 # 遍历每个分组\n",
    "#                 for box_no, group in grouped:\n",
    "#                     output_path = os.path.join(output_dir, f\"heatmap_wordsize_{box_no}_{filter_suffix}.png\")\n",
    "#                     generate_heatmap(group, output_path)\n",
    "\n",
    "#     print(\"All heatmaps have been generated.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "#     current_time = datetime.datetime.now()\n",
    "#     formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     print(formatted_time)\n",
    "#     print(formatted_time)\n",
    "#     print(formatted_time)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 9783 - txt - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt//50%_txt_info_with_classification-9783_txt_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for 9783 - txt - filter_1.0_2.0\n",
      "Processing: 9783 - txt - filter_3.0_4.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt//50%_txt_info_with_classification-9783_txt_filter_3.0_4.0.xlsx\n",
      "Skipping empty DataFrame for 9783 - txt - filter_3.0_4.0\n",
      "Processing: 9783 - txt - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt//50%_txt_info_with_classification-9783_txt_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for 9783 - txt - filter_5.0_6.0\n",
      "Processing: 9783 - price - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price//50%_txt_info_with_classification-9783_price_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for 9783 - price - filter_1.0_2.0\n",
      "Processing: 9783 - price - filter_3.0_4.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price//50%_txt_info_with_classification-9783_price_filter_3.0_4.0.xlsx\n",
      "Skipping empty DataFrame for 9783 - price - filter_3.0_4.0\n",
      "Processing: 9783 - price - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price//50%_txt_info_with_classification-9783_price_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for 9783 - price - filter_5.0_6.0\n",
      "Processing: 6913 - txt - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6913 - txt - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6913 - txt - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height小于18',)_filter_5.0_6.0.png\n",
      "Processing: 6913 - price - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6913 - price - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6913 - price - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height小于18',)_filter_5.0_6.0.png\n",
      "Processing: 6912 - txt - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6912 - txt - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6912 - txt - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height小于18',)_filter_5.0_6.0.png\n",
      "Processing: 6912 - price - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6912 - price - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6912 - price - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_info_with_classification-6912_price_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for 6912 - price - filter_5.0_6.0\n",
      "Processing: 6911 - txt - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6911 - txt - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6911 - txt - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height小于18',)_filter_5.0_6.0.png\n",
      "Processing: 6911 - price - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Processing: 6911 - price - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6911 - price - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Processing: 6910 - txt - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_info_with_classification-6910_txt_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for 6910 - txt - filter_1.0_2.0\n",
      "Processing: 6910 - txt - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6910 - txt - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height小于18',)_filter_5.0_6.0.png\n",
      "Processing: 6910 - price - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6910 - price - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6910 - price - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height小于18',)_filter_5.0_6.0.png\n",
      "Processing: 6909 - txt - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6909 - txt - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6909 - txt - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Processing: 6909 - price - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6909 - price - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6909 - price - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_info_with_classification-6909_price_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for 6909 - price - filter_5.0_6.0\n",
      "Processing: 6908 - txt - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6908 - txt - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6908 - txt - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height小于18',)_filter_5.0_6.0.png\n",
      "Processing: 6908 - price - filter_1.0_2.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height18到29',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height29到38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height大于38',)_filter_1.0_2.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_1.0_2.0\\heatmap_wordsize_('Height小于18',)_filter_1.0_2.0.png\n",
      "Processing: 6908 - price - filter_3.0_4.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height18到29',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height29到38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height大于38',)_filter_3.0_4.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_3.0_4.0\\heatmap_wordsize_('Height小于18',)_filter_3.0_4.0.png\n",
      "Processing: 6908 - price - filter_5.0_6.0\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height18到29',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height29到38',)_filter_5.0_6.0.png\n",
      "Heatmap saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_txt_wordsize_heatmaps_filter_5.0_6.0\\heatmap_wordsize_('Height大于38',)_filter_5.0_6.0.png\n",
      "Processing: 12066 - txt - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt//50%_txt_info_with_classification-12066_txt_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for 12066 - txt - filter_1.0_2.0\n",
      "Processing: 12066 - txt - filter_3.0_4.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt//50%_txt_info_with_classification-12066_txt_filter_3.0_4.0.xlsx\n",
      "Skipping empty DataFrame for 12066 - txt - filter_3.0_4.0\n",
      "Processing: 12066 - txt - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt//50%_txt_info_with_classification-12066_txt_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for 12066 - txt - filter_5.0_6.0\n",
      "Processing: 12066 - price - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price//50%_txt_info_with_classification-12066_price_filter_1.0_2.0.xlsx\n",
      "Skipping empty DataFrame for 12066 - price - filter_1.0_2.0\n",
      "Processing: 12066 - price - filter_3.0_4.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price//50%_txt_info_with_classification-12066_price_filter_3.0_4.0.xlsx\n",
      "Skipping empty DataFrame for 12066 - price - filter_3.0_4.0\n",
      "Processing: 12066 - price - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price//50%_txt_info_with_classification-12066_price_filter_5.0_6.0.xlsx\n",
      "Skipping empty DataFrame for 12066 - price - filter_5.0_6.0\n",
      "All heatmaps have been generated.\n",
      "2024-10-28 09:59:57\n",
      "2024-10-28 09:59:57\n",
      "2024-10-28 09:59:57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# 定义 x_list 和 y_list\n",
    "# x_list = ['12005']\n",
    "# y_list = ['txt', 'price']\n",
    "\n",
    "# z = '男士春夏上装_from_0501'\n",
    "\n",
    "# filter_suffix_list = ['filter_1.0','filter_2.0','filter_3.0','filter_4.0','filter_5.0','filter_6.0']\n",
    "filter_suffix_list = ['filter_1.0_2.0','filter_3.0_4.0','filter_5.0_6.0']\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def create_empty_file(file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    pd.DataFrame().to_excel(file_path, index=False)\n",
    "    print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "def process_data(item_x, item_y, filter_suffix):\n",
    "    file_path = f'D://code//data//Lv2期结论//{z}//{item_x}//grounding_output//{item_y}//50%_txt_info_with_classification-{item_x}_{item_y}_{filter_suffix}.xlsx'\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        create_empty_file(file_path)\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"Empty DataFrame for {file_path}\")\n",
    "        return df\n",
    "\n",
    "    required_columns = ['Height_Category', 'txt_x1', 'txt_y1', 'txt_x2', 'txt_y2', '最终分层']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(f\"Missing required columns in {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "    df['Height_Category'] = df['Height_Category'].replace({\n",
    "        'Height_>38': 'Height大于38',\n",
    "        'Height_18-29': 'Height18到29',\n",
    "        'Height_29-38': 'Height29到38',\n",
    "        'Height_<18': 'Height小于18'\n",
    "    })\n",
    "    df = df.dropna(subset=['Height_Category'])\n",
    "\n",
    "    filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "    df = df[df['最终分层'].isin(filter_values)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_heatmap(group, output_path):\n",
    "    if group.empty:\n",
    "        print(f\"Skipping empty group for {output_path}\")\n",
    "        return\n",
    "\n",
    "    heatmap_data = np.zeros((616, 616))\n",
    "    for _, row in group.iterrows():\n",
    "        x1, y1, x2, y2 = map(int, [row['txt_x1'], row['txt_y1'], row['txt_x2'], row['txt_y2']])\n",
    "        x1, x2 = max(0, min(x1, 616)), max(0, min(x2, 616))\n",
    "        y1, y2 = max(0, min(y1, 616)), max(0, min(y2, 616))\n",
    "        heatmap_data[y1:y2, x1:x2] += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Frequency'}, square=True)\n",
    "    plt.xlabel('X coordinate')\n",
    "    plt.ylabel('Y coordinate')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Heatmap saved to: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    for item_x in x_list:\n",
    "        for item_y in y_list:\n",
    "            for filter_suffix in filter_suffix_list:\n",
    "                print(f\"Processing: {item_x} - {item_y} - {filter_suffix}\")\n",
    "                df = process_data(item_x, item_y, filter_suffix)\n",
    "                \n",
    "                if df.empty:\n",
    "                    print(f\"Skipping empty DataFrame for {item_x} - {item_y} - {filter_suffix}\")\n",
    "                    continue\n",
    "\n",
    "                output_dir = f\"D://code//data//Lv2期结论//{z}//{item_x}//grounding_output//{item_y}//50%_txt_wordsize_heatmaps_{filter_suffix}\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                grouped = df.groupby(['Height_Category'])\n",
    "                for box_no, group in grouped:\n",
    "                    output_path = os.path.join(output_dir, f\"heatmap_wordsize_{box_no}_{filter_suffix}.png\")\n",
    "                    generate_heatmap(group, output_path)\n",
    "\n",
    "    print(\"All heatmaps have been generated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "    print(formatted_time)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 9783 - txt\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt//50%_txt_info_with_classification-9783_txt.xlsx\n",
      "Processing: 9783 - price\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price//50%_txt_info_with_classification-9783_price.xlsx\n",
      "Processing: 6913 - txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_6913_txt_文本分类总结.xlsx\n",
      "Processing: 6913 - price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:32<00:00,  8.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_6913_price_文本分类总结.xlsx\n",
      "Processing: 6912 - txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:39<00:00,  9.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_6912_txt_文本分类总结.xlsx\n",
      "Processing: 6912 - price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_6912_price_文本分类总结.xlsx\n",
      "Processing: 6911 - txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:41<00:00, 10.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_6911_txt_文本分类总结.xlsx\n",
      "Processing: 6911 - price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:32<00:00,  8.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_6911_price_文本分类总结.xlsx\n",
      "Processing: 6910 - txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:32<00:00,  8.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_6910_txt_文本分类总结.xlsx\n",
      "Processing: 6910 - price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:28<00:00,  7.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_6910_price_文本分类总结.xlsx\n",
      "Processing: 6909 - txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_6909_txt_文本分类总结.xlsx\n",
      "Processing: 6909 - price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:40<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_6909_price_文本分类总结.xlsx\n",
      "Processing: 6908 - txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:36<00:00,  9.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_6908_txt_文本分类总结.xlsx\n",
      "Processing: 6908 - price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:39<00:00,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_6908_price_文本分类总结.xlsx\n",
      "Processing: 12066 - txt\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt//50%_txt_info_with_classification-12066_txt.xlsx\n",
      "Processing: 12066 - price\n",
      "File not found: D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price//50%_txt_info_with_classification-12066_price.xlsx\n",
      "All processing completed.\n",
      "2024-10-28 10:08:10\n",
      "2024-10-28 10:08:10\n",
      "2024-10-28 10:08:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里是用旧版prompt,通过list读取,针对整体\n",
    "\n",
    "'''\n",
    "这里是通过读取list形式, 来简化输入的\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']  # 添加所有需要的 Subfolder 值\n",
    "# y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# z = '男士春夏上装_from_0501'\n",
    "\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "def summarize_with_gpt4(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "                # Role \n",
    "                    角色: 电商数据分析师。\n",
    "                # Profile \n",
    "                    简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "                ## Background \n",
    "                    背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "                ## Goals \n",
    "                    目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "                ## Constrains \n",
    "                    约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "                ## Tone \n",
    "                    语气风格: 正式的，客观的，科学的。\n",
    "                ## Skills \n",
    "                    技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "                ## OutputFormat \n",
    "                    输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# 遍历 x 和 y 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        file_path = f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_excel(file_path)\n",
    "        # df = df.dropna(subset=['structure'])\n",
    "\n",
    "        # 确保 'text' 列中的所有值都是字符串\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        \n",
    "        df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "        df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "        # 遍历每个分组，合并文本并进行总结\n",
    "        summaries = []\n",
    "        \n",
    "        # 遍历每个分组\n",
    "        for (height_category), group in tqdm(df_grouped):\n",
    "            # 合并该组的所有文本\n",
    "            all_text = \" \".join(group['text'].dropna())\n",
    "            # print(f\"Structure: {structure}\")\n",
    "            # print(f\"Height Category: {height_category}\")\n",
    "            # print(f\"Text: {all_text[:100]}...\")  # 只打印前100个字符\n",
    "            \n",
    "            # 使用 GPT-4 进行总结\n",
    "            try:\n",
    "                summary = summarize_with_gpt4(all_text)\n",
    "                # print(f\"Summary: {summary[:100]}...\")  # 只打印前100个字符\n",
    "            except Exception as e:\n",
    "                # print(f\"Error in summarization: {str(e)}\")\n",
    "                summary = \"Error in summarization\"\n",
    "            \n",
    "            # 将结果添加到列表中\n",
    "            summaries.append({\n",
    "                # 'structure': structure,\n",
    "                'Height_Category': height_category,\n",
    "                'text': all_text,\n",
    "                'summary': summary\n",
    "            })\n",
    "        \n",
    "        # 创建一个新的DataFrame来存储结果\n",
    "        result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "        # 保存结果到Excel文件\n",
    "        output_file = f\"D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_{x}_{y}_文本分类总结.xlsx\"\n",
    "        result_df.to_excel(output_file, index=False)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "print(\"All processing completed.\")\n",
    "\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 这里是通过读取list形式, 来简化输入的\n",
    "# 添加了针对品牌维度的分类\n",
    "# '''\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "# import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# # x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']  # 添加所有需要的 Subfolder 值\n",
    "# x_list = ['9736','9735','12004']  # 添加所有需要的 Subfolder 值\n",
    "# y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# z = '男士春夏下装_from_0501'\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 filter_suffix_list\n",
    "# filter_suffix_list = ['filter_1.0_2.0_3.0', 'filter_4.0_5.0_6.0']  # 可以根据需要添加更多\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "# def summarize_with_gpt4(text):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "#             {\"role\": \"user\", \"content\": \"\"\"\n",
    "#                 # Role \n",
    "#                     角色: 电商数据分析师。\n",
    "#                 # Profile \n",
    "#                     简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "#                 ## Background \n",
    "#                     背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "#                 ## Goals \n",
    "#                     目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "#                 ## Constrains \n",
    "#                     约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "#                 ## Tone \n",
    "#                     语气风格: 正式的，客观的，科学的。\n",
    "#                 ## Skills \n",
    "#                     技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "#                 ## OutputFormat \n",
    "#                     输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "# # 遍历 x, y 和 filter_suffix 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         for filter_suffix in filter_suffix_list:\n",
    "#             print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "            \n",
    "#             # 读取Excel文件\n",
    "#             file_path = f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}_{filter_suffix}.xlsx'\n",
    "#             if not os.path.exists(file_path):\n",
    "#                 print(f\"File not found: {file_path}\")\n",
    "#                 continue\n",
    "            \n",
    "#             df = pd.read_excel(file_path)\n",
    "\n",
    "#             # 确保 'text' 列中的所有值都是字符串\n",
    "#             df['text'] = df['text'].astype(str)\n",
    "            \n",
    "#             # 根据 filter_suffix 筛选数据\n",
    "#             filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "#             df = df[df['最终分层'].isin(filter_values)]\n",
    "            \n",
    "#             df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "#             df_grouped = df.groupby(['Height_Category'])\n",
    "            \n",
    "#             # 遍历每个分组，合并文本并进行总结\n",
    "#             summaries = []\n",
    "            \n",
    "#             # 遍历每个分组\n",
    "#             for (height_category), group in tqdm(df_grouped):\n",
    "#                 # 合并该组的所有文本\n",
    "#                 all_text = \" \".join(group['text'].dropna())\n",
    "                \n",
    "#                 # 使用 GPT-4 进行总结\n",
    "#                 try:\n",
    "#                     summary = summarize_with_gpt4(all_text)\n",
    "#                 except Exception as e:\n",
    "#                     summary = f\"Error in summarization: {str(e)}\"\n",
    "                \n",
    "#                 # 将结果添加到列表中\n",
    "#                 summaries.append({\n",
    "#                     'Height_Category': height_category,\n",
    "#                     'text': all_text,\n",
    "#                     'summary': summary\n",
    "#                 })\n",
    "            \n",
    "#             # 创建一个新的DataFrame来存储结果\n",
    "#             result_df = pd.DataFrame(summaries)\n",
    "            \n",
    "#             # 保存结果到Excel文件，包含 filter_suffix 在文件名中\n",
    "#             output_file = f\"D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_{x}_{y}_{filter_suffix}_文本分类总结.xlsx\"\n",
    "#             result_df.to_excel(output_file, index=False)\n",
    "#             print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# print(\"All processing completed.\")\n",
    "\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "# print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 9783 - txt - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt//50%_txt_info_with_classification-9783_txt_filter_1.0_2.0.xlsx\n",
      "Processing: 9783 - txt - filter_3.0_4.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt//50%_txt_info_with_classification-9783_txt_filter_3.0_4.0.xlsx\n",
      "Processing: 9783 - txt - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//txt//50%_txt_info_with_classification-9783_txt_filter_5.0_6.0.xlsx\n",
      "Processing: 9783 - price - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price//50%_txt_info_with_classification-9783_price_filter_1.0_2.0.xlsx\n",
      "Processing: 9783 - price - filter_3.0_4.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price//50%_txt_info_with_classification-9783_price_filter_3.0_4.0.xlsx\n",
      "Processing: 9783 - price - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//9783//grounding_output//price//50%_txt_info_with_classification-9783_price_filter_5.0_6.0.xlsx\n",
      "Processing: 6913 - txt - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_6913_txt_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6913 - txt - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:31<00:00,  7.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_6913_txt_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6913 - txt - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:37<00:00,  9.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//txt//50%_6913_txt_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 6913 - price - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_6913_price_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6913 - price - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:30<00:00,  7.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_6913_price_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6913 - price - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6913//grounding_output//price//50%_6913_price_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 6912 - txt - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:26<00:00,  6.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_6912_txt_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6912 - txt - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_6912_txt_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6912 - txt - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:24<00:00,  6.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//txt//50%_6912_txt_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 6912 - price - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:48<00:00, 12.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_6912_price_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6912 - price - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:34<00:00,  8.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_6912_price_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6912 - price - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//6912//grounding_output//price//50%_txt_info_with_classification-6912_price_filter_5.0_6.0.xlsx\n",
      "Processing: 6911 - txt - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:19<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_6911_txt_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6911 - txt - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_6911_txt_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6911 - txt - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:30<00:00,  7.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//txt//50%_6911_txt_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 6911 - price - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:21<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_6911_price_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6911 - price - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:36<00:00,  9.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_6911_price_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6911 - price - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:16<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6911//grounding_output//price//50%_6911_price_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 6910 - txt - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_txt_info_with_classification-6910_txt_filter_1.0_2.0.xlsx\n",
      "Processing: 6910 - txt - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:23<00:00,  5.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_6910_txt_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6910 - txt - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//txt//50%_6910_txt_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 6910 - price - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:21<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_6910_price_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6910 - price - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_6910_price_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6910 - price - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:24<00:00,  6.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6910//grounding_output//price//50%_6910_price_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 6909 - txt - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:25<00:00,  6.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_6909_txt_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6909 - txt - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:27<00:00,  6.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_6909_txt_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6909 - txt - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:23<00:00,  7.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//txt//50%_6909_txt_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 6909 - price - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:32<00:00,  8.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_6909_price_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6909 - price - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:31<00:00,  7.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_6909_price_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6909 - price - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//6909//grounding_output//price//50%_txt_info_with_classification-6909_price_filter_5.0_6.0.xlsx\n",
      "Processing: 6908 - txt - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_6908_txt_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6908 - txt - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:32<00:00,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_6908_txt_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6908 - txt - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:25<00:00,  6.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//txt//50%_6908_txt_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 6908 - price - filter_1.0_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_6908_price_filter_1.0_2.0_文本分类总结.xlsx\n",
      "Processing: 6908 - price - filter_3.0_4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:27<00:00,  6.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_6908_price_filter_3.0_4.0_文本分类总结.xlsx\n",
      "Processing: 6908 - price - filter_5.0_6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:22<00:00,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男鞋_from_0501//6908//grounding_output//price//50%_6908_price_filter_5.0_6.0_文本分类总结.xlsx\n",
      "Processing: 12066 - txt - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt//50%_txt_info_with_classification-12066_txt_filter_1.0_2.0.xlsx\n",
      "Processing: 12066 - txt - filter_3.0_4.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt//50%_txt_info_with_classification-12066_txt_filter_3.0_4.0.xlsx\n",
      "Processing: 12066 - txt - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//txt//50%_txt_info_with_classification-12066_txt_filter_5.0_6.0.xlsx\n",
      "Processing: 12066 - price - filter_1.0_2.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price//50%_txt_info_with_classification-12066_price_filter_1.0_2.0.xlsx\n",
      "Processing: 12066 - price - filter_3.0_4.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price//50%_txt_info_with_classification-12066_price_filter_3.0_4.0.xlsx\n",
      "Processing: 12066 - price - filter_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男鞋_from_0501//12066//grounding_output//price//50%_txt_info_with_classification-12066_price_filter_5.0_6.0.xlsx\n",
      "All processing completed.\n",
      "2024-10-28 10:29:50\n",
      "2024-10-28 10:29:50\n",
      "2024-10-28 10:29:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里是用旧版prompt,通过list读取,针对的是品牌维度的分类\n",
    "\n",
    "\n",
    "'''\n",
    "这里是通过读取list形式, 来简化输入的\n",
    "添加了针对品牌维度的分类\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']   # 添加所有需要的 Subfolder 值\n",
    "# y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# z = '男士春夏上装_from_0501'\n",
    "\n",
    "# 定义 filter_suffix_list\n",
    "# filter_suffix_list = ['filter_1.0','filter_2.0','filter_3.0','filter_4.0','filter_5.0','filter_6.0']\n",
    "filter_suffix_list = ['filter_1.0_2.0','filter_3.0_4.0','filter_5.0_6.0']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "def create_empty_file(file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    pd.DataFrame().to_excel(file_path, index=False)\n",
    "    print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "def summarize_with_gpt4(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "                {\"role\": \"user\", \"content\": \"\"\"\n",
    "                    # Role \n",
    "                        角色: 电商数据分析师。\n",
    "                    # Profile \n",
    "                        简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "                    ## Background \n",
    "                        背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，找出这些描述信息都是从哪些维度切入的， 并且在这个维度上统计出下钻的一个细分维度。\n",
    "                    ## Goals \n",
    "                        目标: 基于我给到的商品描述信息数据集，归纳总结出描述的方向维度，需要特别关注与细化商品本身的特性，并统计这些维度出现的频率。\n",
    "                    ## Constrains \n",
    "                        约束条件: 1、时刻保持自己是电商数据分析师的角色, 2、可以进行适当的联想和猜测, 3、举例的时候禁止出现\"\", 4、统计频率的时候请仔细仔细再仔细\n",
    "                    ## Tone \n",
    "                        语气风格: 正式的，客观的，科学的。\n",
    "                    ## Skills \n",
    "                        技能: 1、你有出色的文本理解能力,能够理解输入数据的含义 2、你有出色的归纳总结能力,能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力,能够精确的统计出各个维度出现的频次。\n",
    "                    ## OutputFormat \n",
    "                        输出格式:以文字方式输出，按照维度，细分维度，细分维度下具体内容举例，细分维度出现频次呈现\"\"\"}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarization: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# 遍历 x, y 和 filter_suffix 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        for filter_suffix in filter_suffix_list:\n",
    "            print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "            \n",
    "            # 读取Excel文件\n",
    "            file_path = f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}_{filter_suffix}.xlsx'\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                create_empty_file(file_path)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_excel(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "                create_empty_file(file_path)\n",
    "                continue\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Empty DataFrame for {file_path}\")\n",
    "                continue\n",
    "\n",
    "            # 确保 'text' 列中的所有值都是字符串\n",
    "            df['text'] = df['text'].astype(str)\n",
    "            \n",
    "            # 根据 filter_suffix 筛选数据\n",
    "            filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "            df = df[df['最终分层'].isin(filter_values)]\n",
    "            \n",
    "            df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "            df_grouped = df.groupby(['Height_Category'])\n",
    "            \n",
    "            # 遍历每个分组，合并文本并进行总结\n",
    "            summaries = []\n",
    "            \n",
    "            # 遍历每个分组\n",
    "            for (height_category), group in tqdm(df_grouped):\n",
    "                # 合并该组的所有文本\n",
    "                all_text = \" \".join(group['text'].dropna())\n",
    "                \n",
    "                # 使用 GPT-4 进行总结\n",
    "                summary = summarize_with_gpt4(all_text)\n",
    "                \n",
    "                # 将结果添加到列表中\n",
    "                summaries.append({\n",
    "                    'Height_Category': height_category,\n",
    "                    'text': all_text,\n",
    "                    'summary': summary\n",
    "                })\n",
    "            \n",
    "            # 创建一个新的DataFrame来存储结果\n",
    "            result_df = pd.DataFrame(summaries)\n",
    "            \n",
    "            # 保存结果到Excel文件，包含 filter_suffix 在文件名中\n",
    "            output_file = f\"D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_{x}_{y}_{filter_suffix}_文本分类总结.xlsx\"\n",
    "            result_df.to_excel(output_file, index=False)\n",
    "            print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "print(\"All processing completed.\")\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下使用了新的prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里尝试了新的prompt,但结果上看,它的内容比较死板,总结多过分析,所以还是使用上面的版本\n",
    "\n",
    "\n",
    "# '''\n",
    "# 这里是通过读取list形式, 来简化输入的\n",
    "# '''\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # 定义 x 和 y 列表\n",
    "# x_list = ['1657']  # 示例值，请根据实际需求修改\n",
    "# y_list = ['txt']  # 示例值，请根据实际需求修改\n",
    "\n",
    "# # 设置API密钥和基础URL\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "#     base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "# )\n",
    "\n",
    "# def filter_by_rectangle(row):\n",
    "#     right, bottom = 616 * 0.3, 616 * 0.2\n",
    "#     if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "# def summarize_with_gpt4(text):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "#             {\"role\": \"user\", \"content\": \"\"\"\n",
    "#                 # Role \n",
    "#                 角色: 电商数据分析师。\n",
    "#                 # Profile \n",
    "#                 简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "#                 ## Background \n",
    "#                 背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，并基于一些前置的定义，找出这些描述信息都是从哪些维度切入的。\n",
    "#                 ## Goals \n",
    "#                 目标: 基于我给到的商品描述信息数据集和前置的维度定义，归纳总结出描述的方向维度，需要特别关注与细化商品本身的卖点特性，并统计这些维度出现的频率。\n",
    "#                 ## Definitions\n",
    "#                 定义：\n",
    "#                 1. 直接展示价格：直接展示价格信息，到手价，预估到手价，会员价等，通常包含上述前缀，¥+具体的价格数字或者具体的价格数字+元。\n",
    "#                 2. 折扣信息：描述商品的折扣，通常包含具体的折扣数字+折。\n",
    "#                 3. 直降信息：描述商品相较原价进行了大幅降价，通常包含直降、立减。\n",
    "#                 4. 满减信息：描述若购买到一定金额，可以在此基础上进行金额优惠，通常包含满+具体的金额+减+具体的金额\n",
    "#                 5. 赠品信息：描述若购买商品则会赠送服务或商品，通常包含赠、送\n",
    "#                 6. 限时：描述商品促销的时间，通常包含活动时间段、活动开始时间、活动结束时间\n",
    "#                 7. 品牌名称：描述商品的品牌名称\n",
    "#                 8. 代言人信息：描述商品的代言人信息\n",
    "#                 9. 价保：价格保护，通常包含价保\n",
    "#                 10. 店铺背书：描述店铺的信息，通常包含旗舰店、自营\n",
    "#                 11. 物流服务：描述商品所包含的物流服务，通常包含物流时效、运费险、物流名称、仓库名称、包邮\n",
    "#                 12. 直接展示价格属于价格信息一级维度，折扣信息、直降信息、满减信息、赠品信息、限时属于价促活动一级维度，品牌名称、代言人信息属于品牌信息一级维度，价保、店铺背书、物流服务属于服务保障一级维度\n",
    "#                 ## Constrains \n",
    "#                 约束条件: 1、时刻保持自己是电商数据分析师的角色，2、可以进行适当的联想和猜测，3、举例的时候禁止出现\"\"，4、统计频率的时候请仔细仔细再仔细，5、若识别到的内容不在上述定义的维度中，可自行命名并统计，请不要忽视未被定义的维度，特别是关于商品本身的卖点信息描述\n",
    "#                 ## Tone \n",
    "#                 语气风格: 正式的，客观的，科学的。\n",
    "#                 ## Skills \n",
    "#                 技能: 1、你有出色的文本理解能力，能够理解输入数据的含义 2、你有出色的归纳总结能力，能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力，能够精确的统计出各个维度出现的频次。\n",
    "#                 ## OutputFormat \n",
    "#                 输出格式:以文字方式输出，一级维度，一级维度下具体内容和举例和频次，输出顺序按照价格信息、价促活动、品牌信息、服务保障、商品卖点进行输出，商品卖点为未定义维度，请你依照自己的知识库信息进行汇总输出，需要特别注意，是关于商品本身的描述，输出格式为1.价格信息 总频次 直接展示价格 频次 举例 以此类推,注意输出要精简，减少不必要的换行\n",
    "#                     \"\"\"}\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 遍历 x 和 y 的所有组合\n",
    "# for x in x_list:\n",
    "#     for y in y_list:\n",
    "#         print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "#         # 读取Excel文件\n",
    "#         file_path = f'D://code//data//Lv2期结论//京喜//筛选//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'\n",
    "#         if not os.path.exists(file_path):\n",
    "#             print(f\"File not found: {file_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         df = pd.read_excel(file_path)\n",
    "#         # df = df.dropna(subset=['structure'])\n",
    "\n",
    "#         # 确保 'text' 列中的所有值都是字符串\n",
    "#         df['text'] = df['text'].astype(str)\n",
    "        \n",
    "#         df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "#         df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "#         # 遍历每个分组，合并文本并进行总结\n",
    "#         summaries = []\n",
    "        \n",
    "#         # 遍历每个分组\n",
    "#         for (height_category), group in tqdm(df_grouped):\n",
    "#             # 合并该组的所有文本\n",
    "#             all_text = \" \".join(group['text'].dropna())\n",
    "#             # print(f\"Structure: {structure}\")\n",
    "#             print(f\"Height Category: {height_category}\")\n",
    "#             print(f\"Text: {all_text[:100]}...\")  # 只打印前100个字符\n",
    "            \n",
    "#             # 使用 GPT-4 进行总结\n",
    "#             try:\n",
    "#                 summary = summarize_with_gpt4(all_text)\n",
    "#                 print(f\"Summary: {summary[:100]}...\")  # 只打印前100个字符\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error in summarization: {str(e)}\")\n",
    "#                 summary = \"Error in summarization\"\n",
    "            \n",
    "#             # 将结果添加到列表中\n",
    "#             summaries.append({\n",
    "#                 # 'structure': structure,\n",
    "#                 'Height_Category': height_category,\n",
    "#                 'text': all_text,\n",
    "#                 'summary': summary\n",
    "#             })\n",
    "        \n",
    "#         # 创建一个新的DataFrame来存储结果\n",
    "#         result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "#         # 保存结果到Excel文件\n",
    "#         output_file = f\"D://code//data//Lv2期结论//京喜//筛选//{x}//grounding_output//{y}//{x}_{y}_文本分类总结.xlsx\"\n",
    "#         result_df.to_excel(output_file, index=False)\n",
    "#         print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# print(\"All processing completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 9736 - price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:33<00:00,  8.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏下装_from_0501//9736//grounding_output//price//50%_9736_price_文本分类总结-new.xlsx\n",
      "Processing: 9735 - price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:01<00:00, 15.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏下装_from_0501//9735//grounding_output//price//50%_9735_price_文本分类总结-new.xlsx\n",
      "Processing: 12004 - price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:41<00:00, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏下装_from_0501//12004//grounding_output//price//50%_12004_price_文本分类总结-new.xlsx\n",
      "All processing completed.\n",
      "2024-10-29 17:52:52\n",
      "2024-10-29 17:52:52\n",
      "2024-10-29 17:52:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里是用新版prompt,通过list读取,针对整体\n",
    "\n",
    "'''\n",
    "这里是通过读取list形式, 来简化输入的\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']  # \n",
    "# x_list = ['6917','9777','9776','9775','6918','6916','6914']  # 女鞋\n",
    "x_list = ['9736', '9735', '12004']  # 男士春夏下\n",
    "# x_list = ['1348', '1349', '1350', '9733', '9734', '12005', '21444', '35434']  # 男士春夏上装\n",
    "\n",
    "y_list = ['price']    # 添加所有需要的 Style 值\n",
    "\n",
    "z = '男士春夏下装_from_0501'\n",
    "\n",
    "\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "def summarize_with_gpt4(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "                # Role \n",
    "                角色: 电商数据分析师。\n",
    "                # Profile \n",
    "                简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "                ## Background \n",
    "                背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，并基于一些前置的定义，找出这些描述信息都是从哪些维度切入的。\n",
    "                ## Goals \n",
    "                目标: 基于我给到的商品描述信息数据集和前置的维度定义，归纳总结出描述的方向维度，需要特别关注与细化商品本身的卖点特性，并统计这些维度出现的频率。\n",
    "                ## Definitions\n",
    "                定义：\n",
    "                1. 直接展示价格：直接展示价格信息，到手价，预估到手价，会员价等，通常包含上述前缀，¥+具体的价格数字或者具体的价格数字+元。\n",
    "                2. 折扣信息：描述商品的折扣，通常包含具体的折扣数字+折。\n",
    "                3. 直降信息：描述商品相较原价进行了大幅降价，通常包含直降、立减。\n",
    "                4. 满减信息：描述若购买到一定金额，可以在此基础上进行金额优惠，通常包含满+具体的金额+减+具体的金额\n",
    "                5. 赠品信息：描述若购买商品则会赠送服务或商品，通常包含赠、送\n",
    "                6. 限时：描述商品促销的时间，通常包含活动时间段、活动开始时间、活动结束时间\n",
    "                7. 品牌名称：描述商品的品牌名称\n",
    "                8. 代言人信息：描述商品的代言人信息\n",
    "                9. 价保：价格保护，通常包含价保\n",
    "                10. 店铺背书：描述店铺的信息，通常包含旗舰店、自营\n",
    "                11. 物流服务：描述商品所包含的物流服务，通常包含物流时效、运费险、物流名称、仓库名称、包邮\n",
    "                12. 直接展示价格属于价格信息一级维度，折扣信息、直降信息、满减信息、赠品信息、限时属于价促活动一级维度，品牌名称、代言人信息属于品牌信息一级维度，价保、店铺背书、物流服务属于服务保障一级维度\n",
    "                ## Constrains \n",
    "                约束条件: 1、时刻保持自己是电商数据分析师的角色，2、可以进行适当的联想和猜测，3、举例的时候禁止出现\"\"，4、统计频率的时候请仔细仔细再仔细，5、若识别到的内容不在上述定义的维度中，可自行命名并统计，请不要忽视未被定义的维度，特别是关于商品本身的卖点信息描述\n",
    "                ## Tone \n",
    "                语气风格: 正式的，客观的，科学的。\n",
    "                ## Skills \n",
    "                技能: 1、你有出色的文本理解能力，能够理解输入数据的含义 2、你有出色的归纳总结能力，能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力，能够精确的统计出各个维度出现的频次。\n",
    "                ## OutputFormat \n",
    "                输出格式:以文字方式输出，一级维度，一级维度下具体内容和举例和频次，输出顺序按照价格信息、价促活动、品牌信息、服务保障、商品卖点进行输出，商品卖点为未定义维度，请你依照自己的知识库信息进行汇总输出，需要特别注意，是关于商品本身的描述，输出格式为1.价格信息 总频次 直接展示价格 频次 举例 以此类推,注意输出要精简，减少不必要的换行\n",
    "                    \"\"\"}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# 遍历 x 和 y 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        print(f\"Processing: {x} - {y}\")\n",
    "        \n",
    "        # 读取Excel文件\n",
    "        file_path = f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}.xlsx'\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_excel(file_path)\n",
    "        # df = df.dropna(subset=['structure'])\n",
    "\n",
    "        # 确保 'text' 列中的所有值都是字符串\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        \n",
    "        df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "        df_grouped = df.groupby(['Height_Category'])\n",
    "        \n",
    "        # 遍历每个分组，合并文本并进行总结\n",
    "        summaries = []\n",
    "        \n",
    "        # 遍历每个分组\n",
    "        for (height_category), group in tqdm(df_grouped):\n",
    "            # 合并该组的所有文本\n",
    "            all_text = \" \".join(group['text'].dropna())\n",
    "            # print(f\"Structure: {structure}\")\n",
    "            # print(f\"Height Category: {height_category}\")\n",
    "            # print(f\"Text: {all_text[:100]}...\")  # 只打印前100个字符\n",
    "            \n",
    "            # 使用 GPT-4 进行总结\n",
    "            try:\n",
    "                summary = summarize_with_gpt4(all_text)\n",
    "                # print(f\"Summary: {summary[:100]}...\")  # 只打印前100个字符\n",
    "            except Exception as e:\n",
    "                # print(f\"Error in summarization: {str(e)}\")\n",
    "                summary = \"Error in summarization\"\n",
    "            \n",
    "            # 将结果添加到列表中\n",
    "            summaries.append({\n",
    "                # 'structure': structure,\n",
    "                'Height_Category': height_category,\n",
    "                'text': all_text,\n",
    "                'summary': summary\n",
    "            })\n",
    "        \n",
    "        # 创建一个新的DataFrame来存储结果\n",
    "        result_df = pd.DataFrame(summaries)\n",
    "        \n",
    "        # 保存结果到Excel文件\n",
    "        output_file = f\"D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_{x}_{y}_文本分类总结-new.xlsx\"\n",
    "        result_df.to_excel(output_file, index=False)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "print(\"All processing completed.\")\n",
    "\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1348 - price - filter_1.0_2.0_3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:52<00:00, 13.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏上装_from_0501//1348//grounding_output//price//50%_1348_price_filter_1.0_2.0_3.0_文本分类总结-new.xlsx\n",
      "Processing: 1348 - price - filter4.0_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男士春夏上装_from_0501//1348//grounding_output//price//50%_txt_info_with_classification-1348_price_filter4.0_5.0_6.0.xlsx\n",
      "Processing: 1349 - price - filter_1.0_2.0_3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:53<00:00, 13.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏上装_from_0501//1349//grounding_output//price//50%_1349_price_filter_1.0_2.0_3.0_文本分类总结-new.xlsx\n",
      "Processing: 1349 - price - filter4.0_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男士春夏上装_from_0501//1349//grounding_output//price//50%_txt_info_with_classification-1349_price_filter4.0_5.0_6.0.xlsx\n",
      "Processing: 1350 - price - filter_1.0_2.0_3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:35<00:00,  8.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏上装_from_0501//1350//grounding_output//price//50%_1350_price_filter_1.0_2.0_3.0_文本分类总结-new.xlsx\n",
      "Processing: 1350 - price - filter4.0_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男士春夏上装_from_0501//1350//grounding_output//price//50%_txt_info_with_classification-1350_price_filter4.0_5.0_6.0.xlsx\n",
      "Processing: 9733 - price - filter_1.0_2.0_3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:07<00:00, 16.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏上装_from_0501//9733//grounding_output//price//50%_9733_price_filter_1.0_2.0_3.0_文本分类总结-new.xlsx\n",
      "Processing: 9733 - price - filter4.0_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男士春夏上装_from_0501//9733//grounding_output//price//50%_txt_info_with_classification-9733_price_filter4.0_5.0_6.0.xlsx\n",
      "Processing: 9734 - price - filter_1.0_2.0_3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏上装_from_0501//9734//grounding_output//price//50%_9734_price_filter_1.0_2.0_3.0_文本分类总结-new.xlsx\n",
      "Processing: 9734 - price - filter4.0_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男士春夏上装_from_0501//9734//grounding_output//price//50%_txt_info_with_classification-9734_price_filter4.0_5.0_6.0.xlsx\n",
      "Processing: 12005 - price - filter_1.0_2.0_3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:50<00:00, 12.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏上装_from_0501//12005//grounding_output//price//50%_12005_price_filter_1.0_2.0_3.0_文本分类总结-new.xlsx\n",
      "Processing: 12005 - price - filter4.0_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男士春夏上装_from_0501//12005//grounding_output//price//50%_txt_info_with_classification-12005_price_filter4.0_5.0_6.0.xlsx\n",
      "Processing: 21444 - price - filter_1.0_2.0_3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:08<00:00, 17.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏上装_from_0501//21444//grounding_output//price//50%_21444_price_filter_1.0_2.0_3.0_文本分类总结-new.xlsx\n",
      "Processing: 21444 - price - filter4.0_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男士春夏上装_from_0501//21444//grounding_output//price//50%_txt_info_with_classification-21444_price_filter4.0_5.0_6.0.xlsx\n",
      "Processing: 35434 - price - filter_1.0_2.0_3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:56<00:00, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: D://code//data//Lv2期结论//男士春夏上装_from_0501//35434//grounding_output//price//50%_35434_price_filter_1.0_2.0_3.0_文本分类总结-new.xlsx\n",
      "Processing: 35434 - price - filter4.0_5.0_6.0\n",
      "Empty DataFrame for D://code//data//Lv2期结论//男士春夏上装_from_0501//35434//grounding_output//price//50%_txt_info_with_classification-35434_price_filter4.0_5.0_6.0.xlsx\n",
      "All processing completed.\n",
      "2024-10-29 17:47:07\n",
      "2024-10-29 17:47:07\n",
      "2024-10-29 17:47:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里是用旧版prompt,通过list读取,针对的是品牌维度的分类\n",
    "\n",
    "\n",
    "'''\n",
    "这里是通过读取list形式, 来简化输入的\n",
    "添加了针对品牌维度的分类\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "# # 定义 x_list 和 y_list（只需要一次）\n",
    "# x_list = ['9734', '9733', '35434','21444', '1350','1349','1348','12005']   # 添加所有需要的 Subfolder 值\n",
    "# y_list = ['txt', 'price']    # 添加所有需要的 Style 值\n",
    "\n",
    "# z = '男士春夏上装_from_0501'\n",
    "\n",
    "# 定义 filter_suffix_list\n",
    "# filter_suffix_list = ['filter_1.0','filter_2.0','filter_3.0','filter_4.0','filter_5.0','filter_6.0']  # 女鞋\n",
    "# filter_suffix_list = ['filter_1.0_2.0','filter_3.0_4.0','filter_5.0_6.0']\n",
    "filter_suffix_list = ['filter_1.0_2.0_3.0','filter4.0_5.0_6.0']  # 男士春夏上/下装\n",
    "\n",
    "\n",
    "\n",
    "# 设置API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    ")\n",
    "\n",
    "def create_empty_file(file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    pd.DataFrame().to_excel(file_path, index=False)\n",
    "    print(f\"Created empty file: {file_path}\")\n",
    "\n",
    "def filter_by_rectangle(row):\n",
    "    right, bottom = 616 * 0.3, 616 * 0.2\n",
    "    if row['txt_x2'] < right and row['txt_y2'] < bottom:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 定义一个函数，使用GPT-4模型对文本进行自动摘要\n",
    "def summarize_with_gpt4(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # 使用正确的模型名称，如果需要请更改\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"你是一个非常优秀的电商分析师,现在有这样一个数据集data:\\n\\n{text}\"},\n",
    "                {\"role\": \"user\", \"content\": \"\"\"\n",
    "                    # Role \n",
    "                    角色: 电商数据分析师。\n",
    "                    # Profile \n",
    "                    简介: 我是一名电商数据分析师，我的工作是通过收集和整理各种信息，找出影响商品销量的因素，并形成报告。\n",
    "                    ## Background \n",
    "                    背景: 现在有一些关于同一类商品的描述信息，我需要从中进行归纳总结，并基于一些前置的定义，找出这些描述信息都是从哪些维度切入的。\n",
    "                    ## Goals \n",
    "                    目标: 基于我给到的商品描述信息数据集和前置的维度定义，归纳总结出描述的方向维度，需要特别关注与细化商品本身的卖点特性，并统计这些维度出现的频率。\n",
    "                    ## Definitions\n",
    "                    定义：\n",
    "                    1. 直接展示价格：直接展示价格信息，到手价，预估到手价，会员价等，通常包含上述前缀，¥+具体的价格数字或者具体的价格数字+元。\n",
    "                    2. 折扣信息：描述商品的折扣，通常包含具体的折扣数字+折。\n",
    "                    3. 直降信息：描述商品相较原价进行了大幅降价，通常包含直降、立减。\n",
    "                    4. 满减信息：描述若购买到一定金额，可以在此基础上进行金额优惠，通常包含满+具体的金额+减+具体的金额\n",
    "                    5. 赠品信息：描述若购买商品则会赠送服务或商品，通常包含赠、送\n",
    "                    6. 限时：描述商品促销的时间，通常包含活动时间段、活动开始时间、活动结束时间\n",
    "                    7. 品牌名称：描述商品的品牌名称\n",
    "                    8. 代言人信息：描述商品的代言人信息\n",
    "                    9. 价保：价格保护，通常包含价保\n",
    "                    10. 店铺背书：描述店铺的信息，通常包含旗舰店、自营\n",
    "                    11. 物流服务：描述商品所包含的物流服务，通常包含物流时效、运费险、物流名称、仓库名称、包邮\n",
    "                    12. 直接展示价格属于价格信息一级维度，折扣信息、直降信息、满减信息、赠品信息、限时属于价促活动一级维度，品牌名称、代言人信息属于品牌信息一级维度，价保、店铺背书、物流服务属于服务保障一级维度\n",
    "                    ## Constrains \n",
    "                    约束条件: 1、时刻保持自己是电商数据分析师的角色，2、可以进行适当的联想和猜测，3、举例的时候禁止出现\"\"，4、统计频率的时候请仔细仔细再仔细，5、若识别到的内容不在上述定义的维度中，可自行命名并统计，请不要忽视未被定义的维度，特别是关于商品本身的卖点信息描述\n",
    "                    ## Tone \n",
    "                    语气风格: 正式的，客观的，科学的。\n",
    "                    ## Skills \n",
    "                    技能: 1、你有出色的文本理解能力，能够理解输入数据的含义 2、你有出色的归纳总结能力，能够归纳总结出数据的描述维度 3、你也有出色的数据统计能力，能够精确的统计出各个维度出现的频次。\n",
    "                    ## OutputFormat \n",
    "                    输出格式:以文字方式输出，一级维度，一级维度下具体内容和举例和频次，输出顺序按照价格信息、价促活动、品牌信息、服务保障、商品卖点进行输出，商品卖点为未定义维度，请你依照自己的知识库信息进行汇总输出，需要特别注意，是关于商品本身的描述，输出格式为1.价格信息 总频次 直接展示价格 频次 举例 以此类推,注意输出要精简，减少不必要的换行\n",
    "                        \"\"\"}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarization: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# 遍历 x, y 和 filter_suffix 的所有组合\n",
    "for x in x_list:\n",
    "    for y in y_list:\n",
    "        for filter_suffix in filter_suffix_list:\n",
    "            print(f\"Processing: {x} - {y} - {filter_suffix}\")\n",
    "            \n",
    "            # 读取Excel文件\n",
    "            file_path = f'D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_txt_info_with_classification-{x}_{y}_{filter_suffix}.xlsx'\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                create_empty_file(file_path)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_excel(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "                create_empty_file(file_path)\n",
    "                continue\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Empty DataFrame for {file_path}\")\n",
    "                continue\n",
    "\n",
    "            # 确保 'text' 列中的所有值都是字符串\n",
    "            df['text'] = df['text'].astype(str)\n",
    "            \n",
    "            # 根据 filter_suffix 筛选数据\n",
    "            filter_values = [float(val) for val in filter_suffix.split('_')[1:]]\n",
    "            df = df[df['最终分层'].isin(filter_values)]\n",
    "            \n",
    "            df = df[df.apply(filter_by_rectangle, axis=1)]\n",
    "            df_grouped = df.groupby(['Height_Category'])\n",
    "            \n",
    "            # 遍历每个分组，合并文本并进行总结\n",
    "            summaries = []\n",
    "            \n",
    "            # 遍历每个分组\n",
    "            for (height_category), group in tqdm(df_grouped):\n",
    "                # 合并该组的所有文本\n",
    "                all_text = \" \".join(group['text'].dropna())\n",
    "                \n",
    "                # 使用 GPT-4 进行总结\n",
    "                summary = summarize_with_gpt4(all_text)\n",
    "                \n",
    "                # 将结果添加到列表中\n",
    "                summaries.append({\n",
    "                    'Height_Category': height_category,\n",
    "                    'text': all_text,\n",
    "                    'summary': summary\n",
    "                })\n",
    "            \n",
    "            # 创建一个新的DataFrame来存储结果\n",
    "            result_df = pd.DataFrame(summaries)\n",
    "            \n",
    "            # 保存结果到Excel文件，包含 filter_suffix 在文件名中\n",
    "            output_file = f\"D://code//data//Lv2期结论//{z}//{x}//grounding_output//{y}//50%_{x}_{y}_{filter_suffix}_文本分类总结-new.xlsx\"\n",
    "            result_df.to_excel(output_file, index=False)\n",
    "            print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "print(\"All processing completed.\")\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "print(formatted_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里是基于claude的api接口调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='msg_bdrk_01GYwM9oPBaRBGDfjziSv6nX', choices=[Choice(finish_reason='max_tokens', index=0, logprobs=None, message=ChatCompletionMessage(content=[{'text': '我是一个由Anthropic公司开发的人工智能助手,名叫Claude。我的目标是以友好和有帮助的方式与人交流,并尽', 'type': 'text'}], role='assistant', function_call=None, tool_calls=[]))], created='2024-10-24 12:05:53', model='anthropic.claude-3-5-sonnet-20240620-v1:0', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=49, prompt_tokens=21, total_tokens=70))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import ujson as json\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"35f54cc4-be7a-4414-808e-f5f9f0194d4f\"\n",
    "# url无需添加具体接口的后缀，openai的sdk会自动补全\n",
    "# 若是通过http方式调用则需要完整的接口url\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "\n",
    "def open_ai_sdk():\n",
    "    client = OpenAI(\n",
    "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "    )\n",
    "    # 此处传入headers中的Authorization 与在client传api_key是一样的\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"\n",
    "    }\n",
    "\n",
    "    # 本示例为请求聊天完成接口，如果需要请求别的接口请修改\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"you are chatgpt3.5\"}, {\"content\": \"你是谁？\", \"role\": \"user\"}],\n",
    "        temperature=0.5,\n",
    "        max_tokens=50,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream=False,\n",
    "        # 入参时erp改为不必填 但如果输入了erp会校验erp是否真实存在，erp仅用于观测实际调用人\n",
    "        # extra_body={\"erp\": \"python\"}\n",
    "        # 请求头\n",
    "        # extra_headers=headers\n",
    "\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(open_ai_sdk())\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
